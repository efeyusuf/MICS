{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Datasets/winequality-white.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0         1            7.0              0.27         0.36            20.7   \n",
       "1         1            6.3              0.30         0.34             1.6   \n",
       "2         1            8.1              0.28         0.40             6.9   \n",
       "3         1            7.2              0.23         0.32             8.5   \n",
       "4         1            7.2              0.23         0.32             8.5   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "4893      1            6.2              0.21         0.29             1.6   \n",
       "4894      0            6.6              0.32         0.36             8.0   \n",
       "4895      1            6.5              0.24         0.19             1.2   \n",
       "4896      1            5.5              0.29         0.30             1.1   \n",
       "4897      1            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0         0.045                 45.0                 170.0  1.00100  3.00   \n",
       "1         0.049                 14.0                 132.0  0.99400  3.30   \n",
       "2         0.050                 30.0                  97.0  0.99510  3.26   \n",
       "3         0.058                 47.0                 186.0  0.99560  3.19   \n",
       "4         0.058                 47.0                 186.0  0.99560  3.19   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "4893      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "4894      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "4895      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "4896      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "4897      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulphates  alcohol  \n",
       "0          0.45      8.8  \n",
       "1          0.49      9.5  \n",
       "2          0.44     10.1  \n",
       "3          0.40      9.9  \n",
       "4          0.40      9.9  \n",
       "...         ...      ...  \n",
       "4893       0.50     11.2  \n",
       "4894       0.46      9.6  \n",
       "4895       0.46      9.4  \n",
       "4896       0.38     12.8  \n",
       "4897       0.32     11.8  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, sep = \";\", index_col=None)\n",
    "df = df.fillna(df.mean())\n",
    "df['quality'] = (df['quality'] >= 6).astype(int)\n",
    "label_column = df.pop('quality')\n",
    "label_column = np.asarray(label_column)\n",
    "df.insert(0, 'label', label_column)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./Datasets/white_wine_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 12 and number of rows is: 4898\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 3, hidden_size=256):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "        h = keras.layers.BatchNormalization()(h)\n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 3, hidden_size=256):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    h = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    h = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h)    \n",
    "    h = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h) \n",
    "    h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h) \n",
    "    \n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3258\n",
       "0    1640\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.9397 - accuracy: 0.7070 - val_loss: 1.3242 - val_accuracy: 0.7102\n",
      "Epoch 2/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.0498 - accuracy: 0.7356 - val_loss: 0.8814 - val_accuracy: 0.7398\n",
      "Epoch 3/300\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.9284 - accuracy: 0.7399 - val_loss: 0.9427 - val_accuracy: 0.6786\n",
      "Epoch 4/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8877 - accuracy: 0.7210 - val_loss: 0.8989 - val_accuracy: 0.7265\n",
      "Epoch 5/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.8762 - accuracy: 0.7340 - val_loss: 0.8130 - val_accuracy: 0.7561\n",
      "Epoch 6/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8547 - accuracy: 0.7305 - val_loss: 0.8808 - val_accuracy: 0.7480\n",
      "Epoch 7/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8480 - accuracy: 0.7437 - val_loss: 0.8303 - val_accuracy: 0.7786\n",
      "Epoch 8/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8703 - accuracy: 0.7430 - val_loss: 0.8362 - val_accuracy: 0.7704\n",
      "Epoch 9/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8605 - accuracy: 0.7412 - val_loss: 0.7727 - val_accuracy: 0.7653\n",
      "Epoch 10/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8666 - accuracy: 0.7430 - val_loss: 0.8138 - val_accuracy: 0.7051\n",
      "Epoch 11/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8221 - accuracy: 0.7417 - val_loss: 0.8434 - val_accuracy: 0.7296\n",
      "Epoch 12/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8210 - accuracy: 0.7392 - val_loss: 0.7620 - val_accuracy: 0.7602\n",
      "Epoch 13/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8159 - accuracy: 0.7300 - val_loss: 0.8530 - val_accuracy: 0.7500\n",
      "Epoch 14/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7919 - accuracy: 0.7343 - val_loss: 0.8411 - val_accuracy: 0.7673\n",
      "Epoch 15/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8131 - accuracy: 0.7417 - val_loss: 0.7671 - val_accuracy: 0.7816\n",
      "Epoch 16/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7730 - accuracy: 0.7425 - val_loss: 0.7885 - val_accuracy: 0.7673\n",
      "Epoch 17/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7810 - accuracy: 0.7397 - val_loss: 0.8140 - val_accuracy: 0.7388\n",
      "Epoch 18/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7691 - accuracy: 0.7292 - val_loss: 0.7036 - val_accuracy: 0.7561\n",
      "Epoch 19/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7556 - accuracy: 0.7392 - val_loss: 0.7473 - val_accuracy: 0.7561\n",
      "Epoch 20/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7628 - accuracy: 0.7333 - val_loss: 0.7928 - val_accuracy: 0.7673\n",
      "Epoch 21/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7687 - accuracy: 0.7369 - val_loss: 0.6829 - val_accuracy: 0.7694\n",
      "Epoch 22/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7609 - accuracy: 0.7343 - val_loss: 0.6908 - val_accuracy: 0.7643\n",
      "Epoch 23/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7540 - accuracy: 0.7427 - val_loss: 0.6781 - val_accuracy: 0.7755\n",
      "Epoch 24/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7390 - accuracy: 0.7376 - val_loss: 0.7383 - val_accuracy: 0.7612\n",
      "Epoch 25/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7263 - accuracy: 0.7371 - val_loss: 0.6721 - val_accuracy: 0.7694\n",
      "Epoch 26/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7165 - accuracy: 0.7399 - val_loss: 0.7223 - val_accuracy: 0.7714\n",
      "Epoch 27/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7034 - accuracy: 0.7315 - val_loss: 0.6868 - val_accuracy: 0.7592\n",
      "Epoch 28/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.7394 - val_loss: 0.6880 - val_accuracy: 0.7714\n",
      "Epoch 29/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.7389 - val_loss: 0.6983 - val_accuracy: 0.7327\n",
      "Epoch 30/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6908 - accuracy: 0.7379 - val_loss: 0.6606 - val_accuracy: 0.7633\n",
      "Epoch 31/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.7325 - val_loss: 0.6361 - val_accuracy: 0.7684\n",
      "Epoch 32/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.7389 - val_loss: 0.6794 - val_accuracy: 0.7327\n",
      "Epoch 33/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6876 - accuracy: 0.7340 - val_loss: 0.6373 - val_accuracy: 0.7592\n",
      "Epoch 34/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6721 - accuracy: 0.7374 - val_loss: 0.6504 - val_accuracy: 0.7541\n",
      "Epoch 35/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6744 - accuracy: 0.7363 - val_loss: 0.7061 - val_accuracy: 0.7153\n",
      "Epoch 36/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6861 - accuracy: 0.7310 - val_loss: 0.6491 - val_accuracy: 0.7296\n",
      "Epoch 37/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.7371 - val_loss: 0.6547 - val_accuracy: 0.7663\n",
      "Epoch 38/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6693 - accuracy: 0.7471 - val_loss: 0.6652 - val_accuracy: 0.7276\n",
      "Epoch 39/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.7307 - val_loss: 0.6333 - val_accuracy: 0.7806\n",
      "Epoch 40/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.7343 - val_loss: 0.6244 - val_accuracy: 0.7653\n",
      "Epoch 41/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.7386 - val_loss: 0.6131 - val_accuracy: 0.7765\n",
      "Epoch 42/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6569 - accuracy: 0.7409 - val_loss: 0.6402 - val_accuracy: 0.7449\n",
      "Epoch 43/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6597 - accuracy: 0.7379 - val_loss: 0.6528 - val_accuracy: 0.7765\n",
      "Epoch 44/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6649 - accuracy: 0.7450 - val_loss: 0.6313 - val_accuracy: 0.7735\n",
      "Epoch 45/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.7333 - val_loss: 0.6666 - val_accuracy: 0.7571\n",
      "Epoch 46/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6593 - accuracy: 0.7371 - val_loss: 0.6207 - val_accuracy: 0.7602\n",
      "Epoch 47/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.7315 - val_loss: 0.6446 - val_accuracy: 0.7551\n",
      "Epoch 48/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6489 - accuracy: 0.7363 - val_loss: 0.6273 - val_accuracy: 0.7735\n",
      "Epoch 49/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6630 - accuracy: 0.7404 - val_loss: 0.6556 - val_accuracy: 0.7622\n",
      "Epoch 50/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.7397 - val_loss: 0.6359 - val_accuracy: 0.7776\n",
      "Epoch 51/300\n",
      "115/123 [===========================>..] - ETA: 0s - loss: 0.6611 - accuracy: 0.7459\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.7443 - val_loss: 0.6783 - val_accuracy: 0.7500\n",
      "Epoch 52/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.7414 - val_loss: 0.5898 - val_accuracy: 0.7755\n",
      "Epoch 53/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7542 - val_loss: 0.5686 - val_accuracy: 0.7633\n",
      "Epoch 54/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5690 - accuracy: 0.7550 - val_loss: 0.5607 - val_accuracy: 0.7653\n",
      "Epoch 55/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.7560 - val_loss: 0.5499 - val_accuracy: 0.7582\n",
      "Epoch 56/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5479 - accuracy: 0.7557 - val_loss: 0.5494 - val_accuracy: 0.7663\n",
      "Epoch 57/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5471 - accuracy: 0.7545 - val_loss: 0.5370 - val_accuracy: 0.7684\n",
      "Epoch 58/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5374 - accuracy: 0.7603 - val_loss: 0.5371 - val_accuracy: 0.7724\n",
      "Epoch 59/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7606 - val_loss: 0.5382 - val_accuracy: 0.7745\n",
      "Epoch 60/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5369 - accuracy: 0.7534 - val_loss: 0.5302 - val_accuracy: 0.7724\n",
      "Epoch 61/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7583 - val_loss: 0.5359 - val_accuracy: 0.7806\n",
      "Epoch 62/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5344 - accuracy: 0.7555 - val_loss: 0.5243 - val_accuracy: 0.7796\n",
      "Epoch 63/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5321 - accuracy: 0.7588 - val_loss: 0.5257 - val_accuracy: 0.7857\n",
      "Epoch 64/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5345 - accuracy: 0.7660 - val_loss: 0.5239 - val_accuracy: 0.7786\n",
      "Epoch 65/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5195 - accuracy: 0.7695 - val_loss: 0.5247 - val_accuracy: 0.7806\n",
      "Epoch 66/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5220 - accuracy: 0.7667 - val_loss: 0.5341 - val_accuracy: 0.7837\n",
      "Epoch 67/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5255 - accuracy: 0.7675 - val_loss: 0.5200 - val_accuracy: 0.7878\n",
      "Epoch 68/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5286 - accuracy: 0.7654 - val_loss: 0.5229 - val_accuracy: 0.7837\n",
      "Epoch 69/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5201 - accuracy: 0.7726 - val_loss: 0.5190 - val_accuracy: 0.7816\n",
      "Epoch 70/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5230 - accuracy: 0.7591 - val_loss: 0.5225 - val_accuracy: 0.7929\n",
      "Epoch 71/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5357 - accuracy: 0.7688 - val_loss: 0.5228 - val_accuracy: 0.7765\n",
      "Epoch 72/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5237 - accuracy: 0.7690 - val_loss: 0.5258 - val_accuracy: 0.7878\n",
      "Epoch 73/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5240 - accuracy: 0.7657 - val_loss: 0.5199 - val_accuracy: 0.7827\n",
      "Epoch 74/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5245 - accuracy: 0.7695 - val_loss: 0.5161 - val_accuracy: 0.7837\n",
      "Epoch 75/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5230 - accuracy: 0.7614 - val_loss: 0.5125 - val_accuracy: 0.7857\n",
      "Epoch 76/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5204 - accuracy: 0.7586 - val_loss: 0.5160 - val_accuracy: 0.7908\n",
      "Epoch 77/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5144 - accuracy: 0.7703 - val_loss: 0.5230 - val_accuracy: 0.7867\n",
      "Epoch 78/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5246 - accuracy: 0.7703 - val_loss: 0.5137 - val_accuracy: 0.7888\n",
      "Epoch 79/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5170 - accuracy: 0.7624 - val_loss: 0.5204 - val_accuracy: 0.7908\n",
      "Epoch 80/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5123 - accuracy: 0.7705 - val_loss: 0.5140 - val_accuracy: 0.7908\n",
      "Epoch 81/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5142 - accuracy: 0.7705 - val_loss: 0.5098 - val_accuracy: 0.7878\n",
      "Epoch 82/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5094 - accuracy: 0.7723 - val_loss: 0.5140 - val_accuracy: 0.7929\n",
      "Epoch 83/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5081 - accuracy: 0.7774 - val_loss: 0.5077 - val_accuracy: 0.7908\n",
      "Epoch 84/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5109 - accuracy: 0.7688 - val_loss: 0.5151 - val_accuracy: 0.7959\n",
      "Epoch 85/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5180 - accuracy: 0.7734 - val_loss: 0.5073 - val_accuracy: 0.7929\n",
      "Epoch 86/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5140 - accuracy: 0.7690 - val_loss: 0.5153 - val_accuracy: 0.7857\n",
      "Epoch 87/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5213 - accuracy: 0.7703 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
      "Epoch 88/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5089 - accuracy: 0.7652 - val_loss: 0.5137 - val_accuracy: 0.7939\n",
      "Epoch 89/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5124 - accuracy: 0.7700 - val_loss: 0.5040 - val_accuracy: 0.7969\n",
      "Epoch 90/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5162 - accuracy: 0.7703 - val_loss: 0.5043 - val_accuracy: 0.7867\n",
      "Epoch 91/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5107 - accuracy: 0.7713 - val_loss: 0.5087 - val_accuracy: 0.7878\n",
      "Epoch 92/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5173 - accuracy: 0.7657 - val_loss: 0.5046 - val_accuracy: 0.7878\n",
      "Epoch 93/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5185 - accuracy: 0.7695 - val_loss: 0.5174 - val_accuracy: 0.7704\n",
      "Epoch 94/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5080 - accuracy: 0.7751 - val_loss: 0.5045 - val_accuracy: 0.7908\n",
      "Epoch 95/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5114 - accuracy: 0.7675 - val_loss: 0.5080 - val_accuracy: 0.7888\n",
      "Epoch 96/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5147 - accuracy: 0.7670 - val_loss: 0.5062 - val_accuracy: 0.7878\n",
      "Epoch 97/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5095 - accuracy: 0.7675 - val_loss: 0.5149 - val_accuracy: 0.7888\n",
      "Epoch 98/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5177 - accuracy: 0.7751 - val_loss: 0.5064 - val_accuracy: 0.7929\n",
      "Epoch 99/300\n",
      "119/123 [============================>.] - ETA: 0s - loss: 0.5120 - accuracy: 0.7718\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5116 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7908\n",
      "Epoch 100/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5054 - accuracy: 0.7741 - val_loss: 0.5076 - val_accuracy: 0.7939\n",
      "Epoch 101/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4965 - accuracy: 0.7751 - val_loss: 0.5058 - val_accuracy: 0.7949\n",
      "Epoch 102/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4898 - accuracy: 0.7810 - val_loss: 0.5045 - val_accuracy: 0.7969\n",
      "Epoch 103/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.7795 - val_loss: 0.5025 - val_accuracy: 0.7918\n",
      "Epoch 104/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4951 - accuracy: 0.7790 - val_loss: 0.5027 - val_accuracy: 0.7949\n",
      "Epoch 105/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4935 - accuracy: 0.7731 - val_loss: 0.5017 - val_accuracy: 0.7929\n",
      "Epoch 106/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.7708 - val_loss: 0.4997 - val_accuracy: 0.7990\n",
      "Epoch 107/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4993 - accuracy: 0.7810 - val_loss: 0.4974 - val_accuracy: 0.7939\n",
      "Epoch 108/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5046 - accuracy: 0.7657 - val_loss: 0.4994 - val_accuracy: 0.7949\n",
      "Epoch 109/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4996 - accuracy: 0.7723 - val_loss: 0.4972 - val_accuracy: 0.7939\n",
      "Epoch 110/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4932 - accuracy: 0.7836 - val_loss: 0.4966 - val_accuracy: 0.7949\n",
      "Epoch 111/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4897 - accuracy: 0.7774 - val_loss: 0.4962 - val_accuracy: 0.7949\n",
      "Epoch 112/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4940 - accuracy: 0.7825 - val_loss: 0.4957 - val_accuracy: 0.7980\n",
      "Epoch 113/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4851 - accuracy: 0.7797 - val_loss: 0.4963 - val_accuracy: 0.7908\n",
      "Epoch 114/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4866 - accuracy: 0.7785 - val_loss: 0.4965 - val_accuracy: 0.7990\n",
      "Epoch 115/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4905 - accuracy: 0.7774 - val_loss: 0.4956 - val_accuracy: 0.7969\n",
      "Epoch 116/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4892 - accuracy: 0.7728 - val_loss: 0.4952 - val_accuracy: 0.7918\n",
      "Epoch 117/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4908 - accuracy: 0.7833 - val_loss: 0.4931 - val_accuracy: 0.7959\n",
      "Epoch 118/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7744 - val_loss: 0.4946 - val_accuracy: 0.7959\n",
      "Epoch 119/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4835 - accuracy: 0.7808 - val_loss: 0.4934 - val_accuracy: 0.7959\n",
      "Epoch 120/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.7884 - val_loss: 0.4910 - val_accuracy: 0.7959\n",
      "Epoch 121/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4884 - accuracy: 0.7797 - val_loss: 0.4926 - val_accuracy: 0.7929\n",
      "Epoch 122/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4829 - accuracy: 0.7769 - val_loss: 0.4924 - val_accuracy: 0.7918\n",
      "Epoch 123/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4868 - accuracy: 0.7825 - val_loss: 0.4926 - val_accuracy: 0.7949\n",
      "Epoch 124/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4912 - accuracy: 0.7815 - val_loss: 0.4914 - val_accuracy: 0.7949\n",
      "Epoch 125/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4880 - accuracy: 0.7782 - val_loss: 0.4933 - val_accuracy: 0.7990\n",
      "Epoch 126/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4828 - accuracy: 0.7820 - val_loss: 0.4929 - val_accuracy: 0.7990\n",
      "Epoch 127/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4883 - accuracy: 0.7828 - val_loss: 0.4919 - val_accuracy: 0.8020\n",
      "Epoch 128/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4835 - accuracy: 0.7813 - val_loss: 0.4922 - val_accuracy: 0.7949\n",
      "Epoch 129/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4826 - accuracy: 0.7772 - val_loss: 0.4918 - val_accuracy: 0.7969\n",
      "Epoch 130/300\n",
      "119/123 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.7742\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.7751 - val_loss: 0.4913 - val_accuracy: 0.7980\n",
      "Epoch 131/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4831 - accuracy: 0.7831 - val_loss: 0.4919 - val_accuracy: 0.7969\n",
      "Epoch 132/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4826 - accuracy: 0.7818 - val_loss: 0.4913 - val_accuracy: 0.7990\n",
      "Epoch 133/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4849 - accuracy: 0.7841 - val_loss: 0.4923 - val_accuracy: 0.7990\n",
      "Epoch 134/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4763 - accuracy: 0.7813 - val_loss: 0.4914 - val_accuracy: 0.7990\n",
      "Epoch 135/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4787 - accuracy: 0.7871 - val_loss: 0.4909 - val_accuracy: 0.7990\n",
      "Epoch 136/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4850 - accuracy: 0.7805 - val_loss: 0.4910 - val_accuracy: 0.7990\n",
      "Epoch 137/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4815 - accuracy: 0.7802 - val_loss: 0.4908 - val_accuracy: 0.7990\n",
      "Epoch 138/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4806 - accuracy: 0.7774 - val_loss: 0.4909 - val_accuracy: 0.7990\n",
      "Epoch 139/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4837 - accuracy: 0.7795 - val_loss: 0.4914 - val_accuracy: 0.7990\n",
      "Epoch 140/300\n",
      "123/123 [==============================] - 1s 9ms/step - loss: 0.4851 - accuracy: 0.7782 - val_loss: 0.4911 - val_accuracy: 0.8000\n",
      "Epoch 141/300\n",
      "123/123 [==============================] - 1s 9ms/step - loss: 0.4877 - accuracy: 0.7797 - val_loss: 0.4910 - val_accuracy: 0.7969\n",
      "Epoch 142/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4749 - accuracy: 0.7910 - val_loss: 0.4906 - val_accuracy: 0.7990\n",
      "Epoch 143/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4829 - accuracy: 0.7831 - val_loss: 0.4903 - val_accuracy: 0.7969\n",
      "Epoch 144/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4763 - accuracy: 0.7861 - val_loss: 0.4906 - val_accuracy: 0.7969\n",
      "Epoch 145/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4842 - accuracy: 0.7833 - val_loss: 0.4902 - val_accuracy: 0.7980\n",
      "Epoch 146/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4801 - accuracy: 0.7790 - val_loss: 0.4899 - val_accuracy: 0.7990\n",
      "Epoch 147/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4828 - accuracy: 0.7823 - val_loss: 0.4900 - val_accuracy: 0.8010\n",
      "Epoch 148/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4759 - accuracy: 0.7820 - val_loss: 0.4907 - val_accuracy: 0.8010\n",
      "Epoch 149/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4849 - accuracy: 0.7782 - val_loss: 0.4901 - val_accuracy: 0.7990\n",
      "Epoch 150/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4818 - accuracy: 0.7846 - val_loss: 0.4912 - val_accuracy: 0.7980\n",
      "Epoch 151/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4823 - accuracy: 0.7818 - val_loss: 0.4913 - val_accuracy: 0.8000\n",
      "Epoch 152/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4902 - accuracy: 0.7754 - val_loss: 0.4914 - val_accuracy: 0.8000\n",
      "Epoch 153/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4844 - accuracy: 0.7810 - val_loss: 0.4911 - val_accuracy: 0.7990\n",
      "Epoch 154/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4843 - accuracy: 0.7805 - val_loss: 0.4903 - val_accuracy: 0.7980\n",
      "Epoch 155/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4828 - accuracy: 0.7856 - val_loss: 0.4912 - val_accuracy: 0.7980\n",
      "Epoch 156/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4882 - accuracy: 0.7805 - val_loss: 0.4904 - val_accuracy: 0.7990\n",
      "Epoch 157/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4833 - accuracy: 0.7797 - val_loss: 0.4904 - val_accuracy: 0.7990\n",
      "Epoch 158/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4826 - accuracy: 0.7825 - val_loss: 0.4905 - val_accuracy: 0.7990\n",
      "Epoch 159/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4827 - accuracy: 0.7787 - val_loss: 0.4905 - val_accuracy: 0.7990\n",
      "Epoch 160/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4750 - accuracy: 0.7864 - val_loss: 0.4908 - val_accuracy: 0.8010\n",
      "Epoch 161/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4807 - accuracy: 0.7841 - val_loss: 0.4908 - val_accuracy: 0.8010\n",
      "Epoch 162/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4790 - accuracy: 0.7846 - val_loss: 0.4903 - val_accuracy: 0.8010\n",
      "Epoch 163/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4866 - accuracy: 0.7810 - val_loss: 0.4900 - val_accuracy: 0.8020\n",
      "Epoch 164/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4846 - accuracy: 0.7728 - val_loss: 0.4899 - val_accuracy: 0.7990\n",
      "Epoch 165/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4796 - accuracy: 0.7915 - val_loss: 0.4905 - val_accuracy: 0.7990\n",
      "Epoch 166/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4817 - accuracy: 0.7831 - val_loss: 0.4910 - val_accuracy: 0.8000\n",
      "Epoch 167/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4846 - accuracy: 0.7790 - val_loss: 0.4906 - val_accuracy: 0.8000\n",
      "Epoch 168/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4826 - accuracy: 0.7823 - val_loss: 0.4903 - val_accuracy: 0.7990\n",
      "Epoch 169/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.7815 - val_loss: 0.4909 - val_accuracy: 0.8000\n",
      "Epoch 170/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4760 - accuracy: 0.7861 - val_loss: 0.4905 - val_accuracy: 0.8000\n",
      "Epoch 171/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4876 - accuracy: 0.7741 - val_loss: 0.4909 - val_accuracy: 0.8000\n",
      "Epoch 172/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4834 - accuracy: 0.7797 - val_loss: 0.4910 - val_accuracy: 0.8000\n",
      "Epoch 173/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4827 - accuracy: 0.7774 - val_loss: 0.4902 - val_accuracy: 0.8020\n",
      "Epoch 174/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4835 - accuracy: 0.7790 - val_loss: 0.4899 - val_accuracy: 0.8031\n",
      "Epoch 175/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4859 - accuracy: 0.7802 - val_loss: 0.4903 - val_accuracy: 0.7990\n",
      "Epoch 176/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4854 - accuracy: 0.7772 - val_loss: 0.4901 - val_accuracy: 0.8000\n",
      "Epoch 177/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4764 - accuracy: 0.7887 - val_loss: 0.4898 - val_accuracy: 0.8020\n",
      "Epoch 178/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4816 - accuracy: 0.7823 - val_loss: 0.4898 - val_accuracy: 0.7980\n",
      "Epoch 179/300\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4767 - accuracy: 0.7838 - val_loss: 0.4904 - val_accuracy: 0.8000\n",
      "Epoch 180/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4883 - accuracy: 0.7820 - val_loss: 0.4913 - val_accuracy: 0.7980\n",
      "Epoch 181/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4833 - accuracy: 0.7879 - val_loss: 0.4907 - val_accuracy: 0.7990\n",
      "Epoch 182/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4852 - accuracy: 0.7841 - val_loss: 0.4902 - val_accuracy: 0.7990\n",
      "Epoch 183/300\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.4877 - accuracy: 0.7767 - val_loss: 0.4900 - val_accuracy: 0.7990\n",
      "Epoch 184/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.4820 - accuracy: 0.7769 - val_loss: 0.4897 - val_accuracy: 0.8010\n",
      "Epoch 185/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.7818 - val_loss: 0.4909 - val_accuracy: 0.8020\n",
      "Epoch 186/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4830 - accuracy: 0.7856 - val_loss: 0.4897 - val_accuracy: 0.8020\n",
      "Epoch 187/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4871 - accuracy: 0.7810 - val_loss: 0.4906 - val_accuracy: 0.8020\n",
      "Epoch 188/300\n",
      "121/123 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7807"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-86d44627e3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           min_lr = 1e-5)]\n\u001b[1;32m      7\u001b[0m \u001b[0mMICS_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m history = MICS_model.fit(x = [trainx_scaled], y = trainy.values,  \n\u001b[0m\u001b[1;32m      9\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtextx_scaled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                          epochs=300, batch_size = 32, callbacks=callback)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.1, patience=10,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-5)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics = [\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx_scaled], y = trainy.values,  \n",
    "                         validation_data = ([textx_scaled], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy\n",
    "#min_losses3[c].append(best_val_loss)\n",
    "#i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
