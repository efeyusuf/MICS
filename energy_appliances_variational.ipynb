{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 502694250212599870\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16118960290105895081\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n",
      "2.3.1\n",
      "Num GPUs Available:  0\n",
      "WARNING:tensorflow:From <ipython-input-2-b5bb3d29fd0a>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_gpu_available()\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./Datasets/energydata_complete.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.500000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.600000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.700000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:30:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.800000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:40:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:50:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.026667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>6.016667</td>\n",
       "      <td>734.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>43.833333</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>44.919484</td>\n",
       "      <td>44.919484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>45.766667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>734.100000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>47.233763</td>\n",
       "      <td>47.233763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.863333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>734.166667</td>\n",
       "      <td>91.833333</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>33.039890</td>\n",
       "      <td>33.039890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:20:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.597500</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.433333</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>734.233333</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>31.455702</td>\n",
       "      <td>31.455702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:30:00</th>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>46.090000</td>\n",
       "      <td>19.23</td>\n",
       "      <td>44.400000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.863333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>46.096667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>734.300000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>3.089314</td>\n",
       "      <td>3.089314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1     T2  \\\n",
       "date                                                                   \n",
       "2016-01-11 17:00:00          60      30  19.890000  47.596667  19.20   \n",
       "2016-01-11 17:10:00          60      30  19.890000  46.693333  19.20   \n",
       "2016-01-11 17:20:00          50      30  19.890000  46.300000  19.20   \n",
       "2016-01-11 17:30:00          50      40  19.890000  46.066667  19.20   \n",
       "2016-01-11 17:40:00          60      40  19.890000  46.333333  19.20   \n",
       "2016-01-11 17:50:00          50      40  19.890000  46.026667  19.20   \n",
       "2016-01-11 18:00:00          60      50  19.890000  45.766667  19.20   \n",
       "2016-01-11 18:10:00          60      50  19.856667  45.560000  19.20   \n",
       "2016-01-11 18:20:00          60      40  19.790000  45.597500  19.20   \n",
       "2016-01-11 18:30:00          70      40  19.856667  46.090000  19.23   \n",
       "\n",
       "                          RH_2     T3       RH_3         T4       RH_4  ...  \\\n",
       "date                                                                    ...   \n",
       "2016-01-11 17:00:00  44.790000  19.79  44.730000  19.000000  45.566667  ...   \n",
       "2016-01-11 17:10:00  44.722500  19.79  44.790000  19.000000  45.992500  ...   \n",
       "2016-01-11 17:20:00  44.626667  19.79  44.933333  18.926667  45.890000  ...   \n",
       "2016-01-11 17:30:00  44.590000  19.79  45.000000  18.890000  45.723333  ...   \n",
       "2016-01-11 17:40:00  44.530000  19.79  45.000000  18.890000  45.530000  ...   \n",
       "2016-01-11 17:50:00  44.500000  19.79  44.933333  18.890000  45.730000  ...   \n",
       "2016-01-11 18:00:00  44.500000  19.79  44.900000  18.890000  45.790000  ...   \n",
       "2016-01-11 18:10:00  44.500000  19.73  44.900000  18.890000  45.863333  ...   \n",
       "2016-01-11 18:20:00  44.433333  19.73  44.790000  18.890000  45.790000  ...   \n",
       "2016-01-11 18:30:00  44.400000  19.79  44.863333  18.890000  46.096667  ...   \n",
       "\n",
       "                            T9   RH_9     T_out  Press_mm_hg     RH_out  \\\n",
       "date                                                                      \n",
       "2016-01-11 17:00:00  17.033333  45.53  6.600000   733.500000  92.000000   \n",
       "2016-01-11 17:10:00  17.066667  45.56  6.483333   733.600000  92.000000   \n",
       "2016-01-11 17:20:00  17.000000  45.50  6.366667   733.700000  92.000000   \n",
       "2016-01-11 17:30:00  17.000000  45.40  6.250000   733.800000  92.000000   \n",
       "2016-01-11 17:40:00  17.000000  45.40  6.133333   733.900000  92.000000   \n",
       "2016-01-11 17:50:00  17.000000  45.29  6.016667   734.000000  92.000000   \n",
       "2016-01-11 18:00:00  17.000000  45.29  5.900000   734.100000  92.000000   \n",
       "2016-01-11 18:10:00  17.000000  45.29  5.916667   734.166667  91.833333   \n",
       "2016-01-11 18:20:00  17.000000  45.29  5.933333   734.233333  91.666667   \n",
       "2016-01-11 18:30:00  17.000000  45.29  5.950000   734.300000  91.500000   \n",
       "\n",
       "                     Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
       "date                                                                         \n",
       "2016-01-11 17:00:00   7.000000   63.000000   5.300000  13.275433  13.275433  \n",
       "2016-01-11 17:10:00   6.666667   59.166667   5.200000  18.606195  18.606195  \n",
       "2016-01-11 17:20:00   6.333333   55.333333   5.100000  28.642668  28.642668  \n",
       "2016-01-11 17:30:00   6.000000   51.500000   5.000000  45.410389  45.410389  \n",
       "2016-01-11 17:40:00   5.666667   47.666667   4.900000  10.084097  10.084097  \n",
       "2016-01-11 17:50:00   5.333333   43.833333   4.800000  44.919484  44.919484  \n",
       "2016-01-11 18:00:00   5.000000   40.000000   4.700000  47.233763  47.233763  \n",
       "2016-01-11 18:10:00   5.166667   40.000000   4.683333  33.039890  33.039890  \n",
       "2016-01-11 18:20:00   5.333333   40.000000   4.666667  31.455702  31.455702  \n",
       "2016-01-11 18:30:00   5.500000   40.000000   4.650000   3.089314   3.089314  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=0)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'Appliances'}>]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(column=\"Appliances\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 28 and number of rows is: 19735\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num-2)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num-2)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>T8</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-03-03 21:50:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>0.458472</td>\n",
       "      <td>-0.841548</td>\n",
       "      <td>0.301055</td>\n",
       "      <td>-0.732773</td>\n",
       "      <td>-0.458634</td>\n",
       "      <td>-0.539270</td>\n",
       "      <td>-0.340717</td>\n",
       "      <td>-1.027087</td>\n",
       "      <td>0.432952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038344</td>\n",
       "      <td>-0.657327</td>\n",
       "      <td>-0.536487</td>\n",
       "      <td>-1.165534</td>\n",
       "      <td>-1.143814</td>\n",
       "      <td>-0.825338</td>\n",
       "      <td>0.810910</td>\n",
       "      <td>-0.488701</td>\n",
       "      <td>-1.232979</td>\n",
       "      <td>-0.869501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-24 05:20:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>-0.805305</td>\n",
       "      <td>-0.178343</td>\n",
       "      <td>-0.976536</td>\n",
       "      <td>0.139469</td>\n",
       "      <td>-0.531993</td>\n",
       "      <td>0.412904</td>\n",
       "      <td>-0.474212</td>\n",
       "      <td>-0.147929</td>\n",
       "      <td>-0.593967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887525</td>\n",
       "      <td>0.543066</td>\n",
       "      <td>-0.593044</td>\n",
       "      <td>0.616681</td>\n",
       "      <td>-1.657647</td>\n",
       "      <td>0.600426</td>\n",
       "      <td>1.135113</td>\n",
       "      <td>-1.101903</td>\n",
       "      <td>-0.341855</td>\n",
       "      <td>-1.350103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-04 08:40:00</th>\n",
       "      <td>2.03889</td>\n",
       "      <td>-0.556460</td>\n",
       "      <td>0.855829</td>\n",
       "      <td>-0.155227</td>\n",
       "      <td>0.541591</td>\n",
       "      <td>-0.294446</td>\n",
       "      <td>1.102247</td>\n",
       "      <td>-0.371335</td>\n",
       "      <td>0.805068</td>\n",
       "      <td>-0.485298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473877</td>\n",
       "      <td>0.837005</td>\n",
       "      <td>-0.543323</td>\n",
       "      <td>1.453444</td>\n",
       "      <td>-0.642513</td>\n",
       "      <td>0.988450</td>\n",
       "      <td>0.956243</td>\n",
       "      <td>0.805836</td>\n",
       "      <td>1.440392</td>\n",
       "      <td>-0.150584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-17 15:30:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>-1.436824</td>\n",
       "      <td>-0.063971</td>\n",
       "      <td>-1.589821</td>\n",
       "      <td>-0.631703</td>\n",
       "      <td>-0.878232</td>\n",
       "      <td>0.167546</td>\n",
       "      <td>-0.817104</td>\n",
       "      <td>-0.829416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>-0.699379</td>\n",
       "      <td>-0.543323</td>\n",
       "      <td>-0.588263</td>\n",
       "      <td>-0.849300</td>\n",
       "      <td>1.297516</td>\n",
       "      <td>-1.357897</td>\n",
       "      <td>-1.033769</td>\n",
       "      <td>0.139068</td>\n",
       "      <td>-1.922059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-02 11:20:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>-0.494249</td>\n",
       "      <td>-0.218668</td>\n",
       "      <td>-0.543067</td>\n",
       "      <td>0.421365</td>\n",
       "      <td>-0.237845</td>\n",
       "      <td>-0.603981</td>\n",
       "      <td>-0.523201</td>\n",
       "      <td>-0.173312</td>\n",
       "      <td>-0.164726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473877</td>\n",
       "      <td>-0.642035</td>\n",
       "      <td>0.058298</td>\n",
       "      <td>-0.958971</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.329711</td>\n",
       "      <td>-0.139341</td>\n",
       "      <td>-0.829369</td>\n",
       "      <td>1.242365</td>\n",
       "      <td>0.206888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 11:10:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>1.544207</td>\n",
       "      <td>2.064744</td>\n",
       "      <td>2.988559</td>\n",
       "      <td>-0.214000</td>\n",
       "      <td>2.708614</td>\n",
       "      <td>1.083637</td>\n",
       "      <td>1.784188</td>\n",
       "      <td>1.259645</td>\n",
       "      <td>1.597518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899840</td>\n",
       "      <td>1.020929</td>\n",
       "      <td>1.748805</td>\n",
       "      <td>1.109273</td>\n",
       "      <td>2.083308</td>\n",
       "      <td>0.083812</td>\n",
       "      <td>-0.631235</td>\n",
       "      <td>-0.897502</td>\n",
       "      <td>2.119344</td>\n",
       "      <td>2.188875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-18 05:30:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>-0.203929</td>\n",
       "      <td>-0.470701</td>\n",
       "      <td>-1.204677</td>\n",
       "      <td>0.820615</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>-0.684099</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>-0.724804</td>\n",
       "      <td>0.378618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>-0.907728</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.476488</td>\n",
       "      <td>-1.300470</td>\n",
       "      <td>0.701944</td>\n",
       "      <td>1.123934</td>\n",
       "      <td>-1.033769</td>\n",
       "      <td>0.223937</td>\n",
       "      <td>-0.897305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-01 13:30:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>-0.269075</td>\n",
       "      <td>1.213620</td>\n",
       "      <td>-0.968419</td>\n",
       "      <td>-0.003524</td>\n",
       "      <td>-0.395468</td>\n",
       "      <td>0.196939</td>\n",
       "      <td>-0.247921</td>\n",
       "      <td>0.058045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496406</td>\n",
       "      <td>-0.570037</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.541809</td>\n",
       "      <td>0.715480</td>\n",
       "      <td>-0.854823</td>\n",
       "      <td>-1.033769</td>\n",
       "      <td>-0.964228</td>\n",
       "      <td>0.115534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-25 20:40:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>1.191676</td>\n",
       "      <td>0.150979</td>\n",
       "      <td>0.346683</td>\n",
       "      <td>0.580983</td>\n",
       "      <td>1.146475</td>\n",
       "      <td>-0.385196</td>\n",
       "      <td>-0.126389</td>\n",
       "      <td>0.343568</td>\n",
       "      <td>0.058045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751744</td>\n",
       "      <td>-0.678353</td>\n",
       "      <td>0.108019</td>\n",
       "      <td>-0.652594</td>\n",
       "      <td>-0.260272</td>\n",
       "      <td>0.451533</td>\n",
       "      <td>0.061889</td>\n",
       "      <td>-1.238170</td>\n",
       "      <td>-0.483304</td>\n",
       "      <td>-0.214135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19 04:40:00</th>\n",
       "      <td>-0.48443</td>\n",
       "      <td>-1.489629</td>\n",
       "      <td>-0.311920</td>\n",
       "      <td>-1.163612</td>\n",
       "      <td>-0.238033</td>\n",
       "      <td>-1.279825</td>\n",
       "      <td>0.435502</td>\n",
       "      <td>-1.073513</td>\n",
       "      <td>-0.202540</td>\n",
       "      <td>-1.354647</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.262587</td>\n",
       "      <td>0.402574</td>\n",
       "      <td>-1.338856</td>\n",
       "      <td>-0.073614</td>\n",
       "      <td>-2.165213</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.620860</td>\n",
       "      <td>-1.238170</td>\n",
       "      <td>2.175923</td>\n",
       "      <td>-2.255700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15788 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      lights        T1      RH_1        T2      RH_2  \\\n",
       "date                                                                   \n",
       "2016-03-03 21:50:00 -0.48443  0.458472 -0.841548  0.301055 -0.732773   \n",
       "2016-02-24 05:20:00 -0.48443 -0.805305 -0.178343 -0.976536  0.139469   \n",
       "2016-02-04 08:40:00  2.03889 -0.556460  0.855829 -0.155227  0.541591   \n",
       "2016-01-17 15:30:00 -0.48443  0.009663 -1.436824 -0.063971 -1.589821   \n",
       "2016-04-02 11:20:00 -0.48443 -0.494249 -0.218668 -0.543067  0.421365   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "2016-05-27 11:10:00 -0.48443  1.544207  2.064744  2.988559 -0.214000   \n",
       "2016-04-18 05:30:00 -0.48443 -0.203929 -0.470701 -1.204677  0.820615   \n",
       "2016-04-01 13:30:00 -0.48443  0.376709 -0.269075  1.213620 -0.968419   \n",
       "2016-03-25 20:40:00 -0.48443  1.191676  0.150979  0.346683  0.580983   \n",
       "2016-01-19 04:40:00 -0.48443 -1.489629 -0.311920 -1.163612 -0.238033   \n",
       "\n",
       "                           T3      RH_3        T4      RH_4        T5  ...  \\\n",
       "date                                                                   ...   \n",
       "2016-03-03 21:50:00 -0.458634 -0.539270 -0.340717 -1.027087  0.432952  ...   \n",
       "2016-02-24 05:20:00 -0.531993  0.412904 -0.474212 -0.147929 -0.593967  ...   \n",
       "2016-02-04 08:40:00 -0.294446  1.102247 -0.371335  0.805068 -0.485298  ...   \n",
       "2016-01-17 15:30:00 -0.631703 -0.878232  0.167546 -0.817104 -0.829416  ...   \n",
       "2016-04-02 11:20:00 -0.237845 -0.603981 -0.523201 -0.173312 -0.164726  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "2016-05-27 11:10:00  2.708614  1.083637  1.784188  1.259645  1.597518  ...   \n",
       "2016-04-18 05:30:00  0.028051 -0.684099  0.505571 -0.724804  0.378618  ...   \n",
       "2016-04-01 13:30:00 -0.003524 -0.395468  0.196939 -0.247921  0.058045  ...   \n",
       "2016-03-25 20:40:00  1.146475 -0.385196 -0.126389  0.343568  0.058045  ...   \n",
       "2016-01-19 04:40:00 -1.279825  0.435502 -1.073513 -0.202540 -1.354647  ...   \n",
       "\n",
       "                           T8      RH_8        T9      RH_9     T_out  \\\n",
       "date                                                                    \n",
       "2016-03-03 21:50:00 -0.038344 -0.657327 -0.536487 -1.165534 -1.143814   \n",
       "2016-02-24 05:20:00 -0.887525  0.543066 -0.593044  0.616681 -1.657647   \n",
       "2016-02-04 08:40:00 -0.473877  0.837005 -0.543323  1.453444 -0.642513   \n",
       "2016-01-17 15:30:00  0.049565 -0.699379 -0.543323 -0.588263 -0.849300   \n",
       "2016-04-02 11:20:00 -0.473877 -0.642035  0.058298 -0.958971  0.172100   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2016-05-27 11:10:00  0.899840  1.020929  1.748805  1.109273  2.083308   \n",
       "2016-04-18 05:30:00  0.036798 -0.907728  0.505785 -0.476488 -1.300470   \n",
       "2016-04-01 13:30:00  0.496406 -0.570037  0.505785 -0.001242  0.541809   \n",
       "2016-03-25 20:40:00  0.751744 -0.678353  0.108019 -0.652594 -0.260272   \n",
       "2016-01-19 04:40:00 -1.262587  0.402574 -1.338856 -0.073614 -2.165213   \n",
       "\n",
       "                     Press_mm_hg    RH_out  Windspeed  Visibility  Tdewpoint  \n",
       "date                                                                          \n",
       "2016-03-03 21:50:00    -0.825338  0.810910  -0.488701   -1.232979  -0.869501  \n",
       "2016-02-24 05:20:00     0.600426  1.135113  -1.101903   -0.341855  -1.350103  \n",
       "2016-02-04 08:40:00     0.988450  0.956243   0.805836    1.440392  -0.150584  \n",
       "2016-01-17 15:30:00     1.297516 -1.357897  -1.033769    0.139068  -1.922059  \n",
       "2016-04-02 11:20:00     0.329711 -0.139341  -0.829369    1.242365   0.206888  \n",
       "...                          ...       ...        ...         ...        ...  \n",
       "2016-05-27 11:10:00     0.083812 -0.631235  -0.897502    2.119344   2.188875  \n",
       "2016-04-18 05:30:00     0.701944  1.123934  -1.033769    0.223937  -0.897305  \n",
       "2016-04-01 13:30:00     0.715480 -0.854823  -1.033769   -0.964228   0.115534  \n",
       "2016-03-25 20:40:00     0.451533  0.061889  -1.238170   -0.483304  -0.214135  \n",
       "2016-01-19 04:40:00     0.095092  0.620860  -1.238170    2.175923  -2.255700  \n",
       "\n",
       "[15788 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_A_train_x_op = trainx_scaled.iloc[:,[0,19,20,21,22,23,24]] \n",
    "group_B_train_x_op = trainx_scaled.iloc[:,[1,2,3,4,5,6,7,8,9,10]]\n",
    "group_C_train_x_op = trainx_scaled.iloc[:,[11,12,13,14,15,16,17,18]]\n",
    "\n",
    "group_A_test_x_op = textx_scaled.iloc[:,[0,19,20,21,22,23,24]]\n",
    "group_B_test_x_op = textx_scaled.iloc[:,[1,2,3,4,5,6,7,8,9,10]]\n",
    "group_C_test_x_op = textx_scaled.iloc[:,[11,12,13,14,15,16,17,18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_inp_a = len(group_A_train_x_op.columns)\n",
    "size_inp_b = len(group_B_train_x_op.columns)\n",
    "size_inp_c = len(group_C_train_x_op.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_sizeA, inp_sizeB, inp_sizeC, use_encoders, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs_A = keras.layers.Input(shape=(inp_sizeA), name=\"input_A\")\n",
    "    inputs_B = keras.layers.Input(shape=(inp_sizeB), name=\"input_B\")\n",
    "    inputs_C = keras.layers.Input(shape=(inp_sizeC), name=\"input_C\")\n",
    "    \n",
    "    #If encoders are not to be used, inputs will be directly given to global model\n",
    "    \n",
    "    if use_encoders == True:\n",
    "        encoder_A = get_encoder_model(inp_sizeA)\n",
    "        encoder_B = get_encoder_model(inp_sizeB)\n",
    "        encoder_C = get_encoder_model(inp_sizeC)\n",
    "    \n",
    "        global_inp_A = encoder_A(inputs_A)\n",
    "        global_inp_B = encoder_B(inputs_B)\n",
    "        global_inp_C = encoder_C(inputs_C)\n",
    "\n",
    "        global_inp = keras.layers.concatenate([global_inp_A, global_inp_B, global_inp_C])\n",
    "    else:\n",
    "        global_inp = keras.layers.concatenate([inputs_A, inputs_A, inputs_A])\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_inp)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"relu\")(h)    \n",
    "    return keras.Model(inputs=[inputs_A, inputs_B, inputs_C], outputs = outputs)\n",
    "\n",
    "def get_encoder_model(inp_size):\n",
    "    inputs = layers.Input(shape=(inp_size))\n",
    "    h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "    z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "    outputs = Sampling()([z_mean, z_log_var])\n",
    "    return keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 12474.0723 - val_loss: 10291.2393\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 10257.7080 - val_loss: 8385.8340\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9834.1475 - val_loss: 7832.1416\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9559.2285 - val_loss: 8141.3750\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9418.1543 - val_loss: 8472.8896\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9230.9795 - val_loss: 7756.6436\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9126.7217 - val_loss: 7874.2368\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9089.3125 - val_loss: 7905.1099\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9039.7734 - val_loss: 7806.7847\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9013.9648 - val_loss: 7701.9097\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8866.0420 - val_loss: 8111.9307\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8780.0283 - val_loss: 7801.7344\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8859.3438 - val_loss: 7767.5337\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8625.7529 - val_loss: 8021.9927\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8534.0322 - val_loss: 7630.6069\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8793.8135 - val_loss: 7930.4771\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8585.3340 - val_loss: 7395.4478\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8528.4785 - val_loss: 7412.5098\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8403.7861 - val_loss: 7565.4717\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8386.2881 - val_loss: 7256.4746\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8466.8525 - val_loss: 8459.5752\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8601.4375 - val_loss: 7440.2197\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8371.0703 - val_loss: 7643.6157\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8371.7393 - val_loss: 7494.4609\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8153.0483 - val_loss: 7265.6304\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8352.4619 - val_loss: 7658.7275\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8128.2715 - val_loss: 7614.1182\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8250.5410 - val_loss: 8137.6987\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8230.1689 - val_loss: 7385.2334\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7991.2070 - val_loss: 7781.6377\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8314.9023 - val_loss: 7344.4771\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8230.7607 - val_loss: 7793.9014\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8118.5581 - val_loss: 8026.1011\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8224.2617 - val_loss: 7400.0396\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8191.5815 - val_loss: 7395.6006\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8174.4233 - val_loss: 7700.8857\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8304.3564 - val_loss: 7162.7905\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8076.3628 - val_loss: 7270.4819\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7996.0112 - val_loss: 7242.1152\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8068.9165 - val_loss: 7419.0439\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8110.8198 - val_loss: 7229.2622\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8103.4692 - val_loss: 7088.0342\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7879.9375 - val_loss: 7100.0820\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7826.1108 - val_loss: 7430.2603\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7742.2427 - val_loss: 7011.7085\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7849.0415 - val_loss: 7099.1719\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7780.5771 - val_loss: 7057.7026\n",
      "Epoch 48/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7891.1528 - val_loss: 6929.8286\n",
      "Epoch 49/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8098.6802 - val_loss: 8019.9277\n",
      "Epoch 50/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7885.5298 - val_loss: 7696.2744\n",
      "Epoch 51/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7817.5610 - val_loss: 7320.7295\n",
      "Epoch 52/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7776.1685 - val_loss: 7262.1533\n",
      "Epoch 53/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7891.6431 - val_loss: 6899.3457\n",
      "Epoch 54/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7710.7715 - val_loss: 6976.7227\n",
      "Epoch 55/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7681.7754 - val_loss: 7311.1919\n",
      "Epoch 56/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7550.8374 - val_loss: 7113.0137\n",
      "Epoch 57/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7561.0776 - val_loss: 7342.5566\n",
      "Epoch 58/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7781.9380 - val_loss: 7656.9971\n",
      "Epoch 59/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7859.3281 - val_loss: 7354.8096\n",
      "Epoch 60/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7723.1587 - val_loss: 7028.5811\n",
      "Epoch 61/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7735.0703 - val_loss: 7307.9521\n",
      "Epoch 62/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7925.7852 - val_loss: 7006.3589\n",
      "Epoch 63/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7561.5298 - val_loss: 7074.6621\n",
      "Epoch 64/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7523.6816 - val_loss: 7056.1821\n",
      "Epoch 65/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7645.9658 - val_loss: 7083.1265\n",
      "Epoch 66/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7589.2539 - val_loss: 7185.8979\n",
      "Epoch 67/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7632.1616 - val_loss: 7109.0659\n",
      "Epoch 68/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7586.5591 - val_loss: 7089.5664\n",
      "Epoch 69/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7695.1572 - val_loss: 7224.5547\n",
      "Epoch 70/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7656.2549 - val_loss: 7184.4287\n",
      "Epoch 71/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7470.3677 - val_loss: 7084.9180\n",
      "Epoch 72/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7559.4985 - val_loss: 7181.2246\n",
      "Epoch 73/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7393.0420 - val_loss: 6970.1782\n",
      "Epoch 74/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7349.5947 - val_loss: 7033.3906\n",
      "Epoch 75/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7454.8359 - val_loss: 7035.4077\n",
      "Epoch 76/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7396.4941 - val_loss: 6896.3452\n",
      "Epoch 77/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7549.2681 - val_loss: 7192.4419\n",
      "Epoch 78/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7359.0791 - val_loss: 7212.3989\n",
      "Epoch 79/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7492.0015 - val_loss: 7066.2861\n",
      "Epoch 80/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7275.2441 - val_loss: 7646.5557\n",
      "Epoch 81/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7372.7280 - val_loss: 7071.8267\n",
      "Epoch 82/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7468.3662 - val_loss: 7040.3413\n",
      "Epoch 83/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7179.6240 - val_loss: 7437.9546\n",
      "Epoch 84/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7416.1128 - val_loss: 7021.6797\n",
      "Epoch 85/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7415.1841 - val_loss: 7019.6211\n",
      "Epoch 86/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7469.9771 - val_loss: 6860.8701\n",
      "Epoch 87/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7230.2964 - val_loss: 6845.7725\n",
      "Epoch 88/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7269.7598 - val_loss: 7618.3813\n",
      "Epoch 89/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7331.5439 - val_loss: 7120.8364\n",
      "Epoch 90/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7228.8486 - val_loss: 7162.1001\n",
      "Epoch 91/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7173.9072 - val_loss: 7016.4771\n",
      "Epoch 92/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7285.4819 - val_loss: 6930.5205\n",
      "Epoch 93/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7438.6357 - val_loss: 6866.5630\n",
      "Epoch 94/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7345.6519 - val_loss: 7344.2295\n",
      "Epoch 95/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7306.0698 - val_loss: 6877.9878\n",
      "Epoch 96/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7350.4380 - val_loss: 7149.8828\n",
      "Epoch 97/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7429.1250 - val_loss: 7138.3682\n",
      "Epoch 98/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7393.9058 - val_loss: 7383.2407\n",
      "Epoch 99/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7182.2041 - val_loss: 7410.1753\n",
      "Epoch 100/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7164.5986 - val_loss: 7114.6826\n",
      "Epoch 101/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6970.5103 - val_loss: 7023.4395\n",
      "Epoch 102/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7285.4229 - val_loss: 7390.4004\n",
      "Epoch 103/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7412.0137 - val_loss: 7015.7183\n",
      "Epoch 104/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7353.5435 - val_loss: 7503.9995\n",
      "Epoch 105/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7335.7866 - val_loss: 6912.7671\n",
      "Epoch 106/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7203.0986 - val_loss: 7150.7085\n",
      "Epoch 107/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7145.0693 - val_loss: 6997.1265\n",
      "Epoch 108/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7113.8447 - val_loss: 6748.9697\n",
      "Epoch 109/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7342.9824 - val_loss: 6943.7217\n",
      "Epoch 110/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7158.3770 - val_loss: 6838.0894\n",
      "Epoch 111/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7291.3135 - val_loss: 6740.0610\n",
      "Epoch 112/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7054.3950 - val_loss: 6899.8682\n",
      "Epoch 113/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7285.2915 - val_loss: 6855.4370\n",
      "Epoch 114/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7011.7217 - val_loss: 7269.0088\n",
      "Epoch 115/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7015.3740 - val_loss: 6893.1660\n",
      "Epoch 116/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7107.0371 - val_loss: 7432.1372\n",
      "Epoch 117/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7152.3203 - val_loss: 6827.1914\n",
      "Epoch 118/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7232.7554 - val_loss: 6892.7397\n",
      "Epoch 119/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7304.0327 - val_loss: 7449.1978\n",
      "Epoch 120/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7182.4658 - val_loss: 7160.7363\n",
      "Epoch 121/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7019.8433 - val_loss: 6900.2734\n",
      "Epoch 122/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7042.0161 - val_loss: 7139.4121\n",
      "Epoch 123/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6993.9839 - val_loss: 6953.3286\n",
      "Epoch 124/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7149.6357 - val_loss: 6954.0117\n",
      "Epoch 125/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7177.6616 - val_loss: 7447.2495\n",
      "Epoch 126/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7131.2402 - val_loss: 6803.7896\n",
      "Epoch 127/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7119.6772 - val_loss: 7129.2202\n",
      "Epoch 128/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6892.3721 - val_loss: 6904.0732\n",
      "Epoch 129/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7126.0249 - val_loss: 7084.3311\n",
      "Epoch 130/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7117.2124 - val_loss: 7079.7671\n",
      "Epoch 131/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7113.9141 - val_loss: 6795.5889\n",
      "Epoch 132/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6939.4663 - val_loss: 7366.8408\n",
      "Epoch 133/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6994.7085 - val_loss: 6798.3267\n",
      "Epoch 134/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6970.7275 - val_loss: 6703.1465\n",
      "Epoch 135/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6921.0771 - val_loss: 7052.8457\n",
      "Epoch 136/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7084.8623 - val_loss: 7170.4521\n",
      "Epoch 137/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6895.2139 - val_loss: 7351.1172\n",
      "Epoch 138/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7092.1201 - val_loss: 7007.9956\n",
      "Epoch 139/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7124.7788 - val_loss: 7322.1001\n",
      "Epoch 140/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7214.4839 - val_loss: 6971.5020\n",
      "Epoch 141/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7013.6187 - val_loss: 6692.2378\n",
      "Epoch 142/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7046.4390 - val_loss: 7217.5850\n",
      "Epoch 143/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6930.0356 - val_loss: 6753.8613\n",
      "Epoch 144/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6877.0815 - val_loss: 6820.1758\n",
      "Epoch 145/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7003.8740 - val_loss: 6785.8467\n",
      "Epoch 146/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7099.5986 - val_loss: 6902.6675\n",
      "Epoch 147/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6937.3999 - val_loss: 6823.9058\n",
      "Epoch 148/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7068.8896 - val_loss: 7040.0410\n",
      "Epoch 149/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7225.5044 - val_loss: 7198.0723\n",
      "Epoch 150/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6768.4614 - val_loss: 6866.9814\n",
      "Epoch 151/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7057.8584 - val_loss: 7407.4346\n",
      "Epoch 152/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7072.2476 - val_loss: 6666.7642\n",
      "Epoch 153/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7010.4985 - val_loss: 6818.2383\n",
      "Epoch 154/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6804.5225 - val_loss: 7140.2490\n",
      "Epoch 155/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6819.1748 - val_loss: 6787.2847\n",
      "Epoch 156/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6889.5786 - val_loss: 7376.3574\n",
      "Epoch 157/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6815.5195 - val_loss: 6681.8364\n",
      "Epoch 158/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6868.4185 - val_loss: 6850.3613\n",
      "Epoch 159/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6913.9219 - val_loss: 7034.8145\n",
      "Epoch 160/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7183.7637 - val_loss: 7783.1001\n",
      "Epoch 161/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7109.8853 - val_loss: 7044.7666\n",
      "Epoch 162/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6772.1152 - val_loss: 7162.0894\n",
      "Epoch 163/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6746.3540 - val_loss: 7074.6890\n",
      "Epoch 164/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6745.0459 - val_loss: 7048.7324\n",
      "Epoch 165/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6980.8696 - val_loss: 7287.8491\n",
      "Epoch 166/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6878.9800 - val_loss: 6854.8032\n",
      "Epoch 167/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6659.5405 - val_loss: 7059.4741\n",
      "Epoch 168/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6717.8101 - val_loss: 7157.9492\n",
      "Epoch 169/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6822.1655 - val_loss: 6743.7734\n",
      "Epoch 170/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6722.5474 - val_loss: 6823.0713\n",
      "Epoch 171/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6681.2759 - val_loss: 7142.2134\n",
      "Epoch 172/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6685.6733 - val_loss: 6900.4878\n",
      "Epoch 173/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6701.7637 - val_loss: 7064.2988\n",
      "Epoch 174/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7089.3911 - val_loss: 6754.6265\n",
      "Epoch 175/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6853.8115 - val_loss: 7305.0112\n",
      "Epoch 176/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6758.2544 - val_loss: 7117.2842\n",
      "Epoch 177/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6942.1548 - val_loss: 7105.1084\n",
      "Epoch 178/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6859.5127 - val_loss: 7162.1069\n",
      "Epoch 179/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6832.7603 - val_loss: 6988.6899\n",
      "Epoch 180/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7271.9214 - val_loss: 7507.7461\n",
      "Epoch 181/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6815.8584 - val_loss: 6822.9233\n",
      "Epoch 182/300\n",
      "73/79 [==========================>...] - ETA: 0s - loss: 6848.5610\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6851.3579 - val_loss: 7129.8765\n",
      "Epoch 183/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6836.4102 - val_loss: 7149.5762\n",
      "Epoch 184/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6639.2314 - val_loss: 7304.2729\n",
      "Epoch 185/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6747.7661 - val_loss: 6982.4062\n",
      "Epoch 186/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6618.2324 - val_loss: 6700.3491\n",
      "Epoch 187/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6703.0430 - val_loss: 6819.9443\n",
      "Epoch 188/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6725.7090 - val_loss: 6898.5249\n",
      "Epoch 189/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6603.8213 - val_loss: 6696.3384\n",
      "Epoch 190/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6631.8770 - val_loss: 6928.8110\n",
      "Epoch 191/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6650.7236 - val_loss: 7160.7202\n",
      "Epoch 192/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6618.1689 - val_loss: 6726.0352\n",
      "Epoch 193/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6535.1680 - val_loss: 6721.7095\n",
      "Epoch 194/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6642.7188 - val_loss: 7002.8662\n",
      "Epoch 195/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6437.4546 - val_loss: 7089.3726\n",
      "Epoch 196/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6741.6108 - val_loss: 6838.1064\n",
      "Epoch 197/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6818.2944 - val_loss: 7312.8384\n",
      "Epoch 198/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6673.5186 - val_loss: 7282.6426\n",
      "Epoch 199/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6685.5942 - val_loss: 7205.7583\n",
      "Epoch 200/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6851.4985 - val_loss: 7118.7441\n",
      "Epoch 201/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6470.6748 - val_loss: 7071.3921\n",
      "Epoch 202/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6402.8135 - val_loss: 6813.4424\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 12281.8340 - val_loss: 9318.8291\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 10569.4404 - val_loss: 8547.5850\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9935.7061 - val_loss: 8536.9150\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9580.5957 - val_loss: 7928.6152\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9340.0479 - val_loss: 7804.3413\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9262.1436 - val_loss: 7896.7632\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9188.5000 - val_loss: 8085.6924\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9085.5889 - val_loss: 7958.2700\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9072.5498 - val_loss: 8511.5547\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9008.6084 - val_loss: 7669.6260\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8873.8379 - val_loss: 7841.4766\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8816.4932 - val_loss: 7793.4819\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8604.5498 - val_loss: 7942.9634\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8597.1846 - val_loss: 7912.9502\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8658.0420 - val_loss: 7580.7070\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8576.3545 - val_loss: 7680.7935\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8636.4365 - val_loss: 7536.8247\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8613.5986 - val_loss: 8007.4473\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8427.7295 - val_loss: 7576.4619\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8342.6436 - val_loss: 7676.5635\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8352.9258 - val_loss: 7637.0054\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8336.9248 - val_loss: 7595.6436\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8467.6846 - val_loss: 8039.1904\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8569.3438 - val_loss: 7459.9292\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8226.7520 - val_loss: 7858.2295\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8441.2910 - val_loss: 7802.6924\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8324.4590 - val_loss: 7429.4390\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8280.5137 - val_loss: 7599.6460\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8442.8428 - val_loss: 7709.9414\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8199.3018 - val_loss: 7958.5479\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8064.1689 - val_loss: 7479.6572\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8321.1104 - val_loss: 7524.1138\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8364.7334 - val_loss: 7509.5493\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8417.8555 - val_loss: 7543.6094\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8119.3667 - val_loss: 7406.4648\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8117.1636 - val_loss: 7463.1719\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8115.6592 - val_loss: 7314.4941\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7930.2041 - val_loss: 7337.0752\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7935.3721 - val_loss: 7300.3018\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7882.6924 - val_loss: 7428.0825\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7904.3755 - val_loss: 7651.6841\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7875.7397 - val_loss: 7368.8428\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7936.8418 - val_loss: 7301.7734\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7985.1616 - val_loss: 7408.2725\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7906.3364 - val_loss: 7165.7769\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8109.7422 - val_loss: 7185.2808\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7835.3530 - val_loss: 7520.0322\n",
      "Epoch 48/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8058.5693 - val_loss: 7347.3706\n",
      "Epoch 49/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7795.8530 - val_loss: 7377.8584\n",
      "Epoch 50/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7954.9937 - val_loss: 7463.0630\n",
      "Epoch 51/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7927.5215 - val_loss: 7234.2412\n",
      "Epoch 52/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7763.6738 - val_loss: 7246.6030\n",
      "Epoch 53/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7835.2451 - val_loss: 7837.5630\n",
      "Epoch 54/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7931.1802 - val_loss: 7522.1650\n",
      "Epoch 55/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7820.4556 - val_loss: 7348.9780\n",
      "Epoch 56/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7876.5146 - val_loss: 7692.7769\n",
      "Epoch 57/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8179.3682 - val_loss: 7456.4741\n",
      "Epoch 58/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7771.2021 - val_loss: 7371.6611\n",
      "Epoch 59/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7877.9858 - val_loss: 6890.6055\n",
      "Epoch 60/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7700.7793 - val_loss: 7337.2891\n",
      "Epoch 61/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7670.1836 - val_loss: 7189.7109\n",
      "Epoch 62/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7673.6592 - val_loss: 7258.6792\n",
      "Epoch 63/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7823.3120 - val_loss: 7057.5283\n",
      "Epoch 64/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7800.1816 - val_loss: 7141.3755\n",
      "Epoch 65/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7714.1963 - val_loss: 7230.7188\n",
      "Epoch 66/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7804.9858 - val_loss: 7123.8564\n",
      "Epoch 67/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7753.0698 - val_loss: 7163.0278\n",
      "Epoch 68/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7877.0532 - val_loss: 7167.0264\n",
      "Epoch 69/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7804.3750 - val_loss: 7117.1782\n",
      "Epoch 70/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7855.9639 - val_loss: 7197.5498\n",
      "Epoch 71/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7773.3037 - val_loss: 6991.3135\n",
      "Epoch 72/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7756.8833 - val_loss: 7030.7456\n",
      "Epoch 73/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7759.4307 - val_loss: 7776.7104\n",
      "Epoch 74/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7800.4141 - val_loss: 7546.8081\n",
      "Epoch 75/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7774.9277 - val_loss: 7164.1118\n",
      "Epoch 76/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7650.2935 - val_loss: 7276.4614\n",
      "Epoch 77/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7654.3916 - val_loss: 7079.8501\n",
      "Epoch 78/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7761.7598 - val_loss: 7148.9053\n",
      "Epoch 79/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7673.6626 - val_loss: 7079.8662\n",
      "Epoch 80/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7825.1304 - val_loss: 7373.8677\n",
      "Epoch 81/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7594.4858 - val_loss: 7538.5850\n",
      "Epoch 82/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7775.6665 - val_loss: 7096.2192\n",
      "Epoch 83/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7723.4741 - val_loss: 7083.2505\n",
      "Epoch 84/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7625.4092 - val_loss: 7224.9873\n",
      "Epoch 85/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7606.2583 - val_loss: 7000.7661\n",
      "Epoch 86/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7665.6606 - val_loss: 7560.7212\n",
      "Epoch 87/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7780.0078 - val_loss: 7662.0815\n",
      "Epoch 88/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7747.7407 - val_loss: 7318.4058\n",
      "Epoch 89/300\n",
      "73/79 [==========================>...] - ETA: 0s - loss: 7880.2407\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7837.2803 - val_loss: 7398.5996\n",
      "Epoch 90/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7550.2271 - val_loss: 7323.7393\n",
      "Epoch 91/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7444.2036 - val_loss: 7155.2500\n",
      "Epoch 92/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7385.2114 - val_loss: 7421.4536\n",
      "Epoch 93/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7404.4927 - val_loss: 7272.4458\n",
      "Epoch 94/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7409.6436 - val_loss: 7310.8311\n",
      "Epoch 95/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7687.3945 - val_loss: 7058.4429\n",
      "Epoch 96/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8063.3091 - val_loss: 7432.2646\n",
      "Epoch 97/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7626.4844 - val_loss: 7362.9146\n",
      "Epoch 98/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7652.2607 - val_loss: 7022.9995\n",
      "Epoch 99/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7403.8501 - val_loss: 7016.1992\n",
      "Epoch 100/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7605.7505 - val_loss: 7073.2051\n",
      "Epoch 101/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7384.5557 - val_loss: 7144.8896\n",
      "Epoch 102/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7561.3813 - val_loss: 7344.1597\n",
      "Epoch 103/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7321.4888 - val_loss: 7108.6230\n",
      "Epoch 104/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7501.9678 - val_loss: 7053.1470\n",
      "Epoch 105/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7342.1724 - val_loss: 7200.0244\n",
      "Epoch 106/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7461.3291 - val_loss: 7256.4775\n",
      "Epoch 107/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7582.8027 - val_loss: 6935.9590\n",
      "Epoch 108/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7184.7881 - val_loss: 7151.0708\n",
      "Epoch 109/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7212.1738 - val_loss: 7219.5591\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 12343.0566 - val_loss: 8924.9590\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 10276.0498 - val_loss: 8306.7744\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9841.3252 - val_loss: 8804.8574\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9517.8711 - val_loss: 8304.5049\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9423.8887 - val_loss: 8199.9727\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9374.9590 - val_loss: 8389.9912\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9197.6396 - val_loss: 8326.3906\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8926.7598 - val_loss: 7826.7056\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8851.5908 - val_loss: 7736.1660\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8816.4521 - val_loss: 7775.4326\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8726.7061 - val_loss: 7769.6533\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8572.6387 - val_loss: 7449.7134\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8545.5312 - val_loss: 7835.7388\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8533.3975 - val_loss: 7801.2803\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8453.0215 - val_loss: 7461.7168\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8407.3867 - val_loss: 7935.9927\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8646.9160 - val_loss: 7603.9644\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8348.4629 - val_loss: 7990.9736\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8316.2793 - val_loss: 7400.5190\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8291.9688 - val_loss: 7346.6060\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8198.5088 - val_loss: 7316.6978\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8359.5059 - val_loss: 7698.2388\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8282.3604 - val_loss: 7556.6211\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8194.9658 - val_loss: 7423.4106\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8253.0625 - val_loss: 7460.9165\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8243.5322 - val_loss: 7488.2271\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8154.3379 - val_loss: 7359.5679\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8034.0601 - val_loss: 7387.6680\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8183.7065 - val_loss: 7417.3989\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8015.0786 - val_loss: 7344.6440\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7903.7227 - val_loss: 7420.7578\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8039.9204 - val_loss: 7596.4312\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7930.2144 - val_loss: 7389.9863\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8107.6997 - val_loss: 7287.0698\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8019.6382 - val_loss: 7471.4414\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7749.5566 - val_loss: 7207.5742\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7791.0454 - val_loss: 7388.3516\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7783.9854 - val_loss: 7228.6426\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7750.0146 - val_loss: 7380.7510\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8000.8018 - val_loss: 7194.6675\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7902.9595 - val_loss: 7452.6372\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7773.2041 - val_loss: 7161.5122\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7700.0513 - val_loss: 7372.1436\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7767.6504 - val_loss: 7158.8252\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7617.2935 - val_loss: 7193.7866\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7691.1401 - val_loss: 7472.5620\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7659.2695 - val_loss: 7458.6338\n",
      "Epoch 48/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7777.9409 - val_loss: 7078.2100\n",
      "Epoch 49/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7617.0923 - val_loss: 7407.6152\n",
      "Epoch 50/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7684.1694 - val_loss: 7682.1304\n",
      "Epoch 51/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7616.0288 - val_loss: 7311.4062\n",
      "Epoch 52/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7564.3936 - val_loss: 7119.5298\n",
      "Epoch 53/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7713.7251 - val_loss: 7466.0107\n",
      "Epoch 54/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7602.4702 - val_loss: 7057.3066\n",
      "Epoch 55/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7509.3594 - val_loss: 7070.1201\n",
      "Epoch 56/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7541.5420 - val_loss: 7134.3950\n",
      "Epoch 57/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7634.8452 - val_loss: 7055.3687\n",
      "Epoch 58/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7564.3687 - val_loss: 7437.0005\n",
      "Epoch 59/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7358.8706 - val_loss: 6848.8779\n",
      "Epoch 60/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7464.1870 - val_loss: 7081.0171\n",
      "Epoch 61/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7415.7397 - val_loss: 6955.5713\n",
      "Epoch 62/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7632.8091 - val_loss: 7278.4697\n",
      "Epoch 63/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7344.7866 - val_loss: 7327.0610\n",
      "Epoch 64/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7605.2446 - val_loss: 7361.2153\n",
      "Epoch 65/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7354.0059 - val_loss: 7114.7280\n",
      "Epoch 66/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7397.1592 - val_loss: 6988.7422\n",
      "Epoch 67/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7212.9160 - val_loss: 7080.0054\n",
      "Epoch 68/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7328.3853 - val_loss: 6921.8306\n",
      "Epoch 69/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7355.8135 - val_loss: 6999.2681\n",
      "Epoch 70/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7172.8198 - val_loss: 6837.9155\n",
      "Epoch 71/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7411.6533 - val_loss: 7094.0293\n",
      "Epoch 72/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7194.1836 - val_loss: 7047.5630\n",
      "Epoch 73/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7210.8955 - val_loss: 6924.9077\n",
      "Epoch 74/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7278.6025 - val_loss: 7155.0732\n",
      "Epoch 75/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7356.3325 - val_loss: 7205.1621\n",
      "Epoch 76/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7685.1260 - val_loss: 7248.4751\n",
      "Epoch 77/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7600.3506 - val_loss: 7280.1479\n",
      "Epoch 78/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7313.8794 - val_loss: 6808.2441\n",
      "Epoch 79/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7230.5298 - val_loss: 7056.4375\n",
      "Epoch 80/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7231.4395 - val_loss: 6945.0786\n",
      "Epoch 81/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7323.9771 - val_loss: 6848.1440\n",
      "Epoch 82/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7328.4346 - val_loss: 7715.2056\n",
      "Epoch 83/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7775.7261 - val_loss: 7128.8013\n",
      "Epoch 84/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7534.4507 - val_loss: 7049.3589\n",
      "Epoch 85/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7420.6743 - val_loss: 7023.7026\n",
      "Epoch 86/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7358.6274 - val_loss: 7039.2036\n",
      "Epoch 87/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7230.9531 - val_loss: 6825.2129\n",
      "Epoch 88/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7081.9087 - val_loss: 7064.1973\n",
      "Epoch 89/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7065.2583 - val_loss: 6999.1846\n",
      "Epoch 90/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7283.4561 - val_loss: 7341.1348\n",
      "Epoch 91/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7205.4795 - val_loss: 6936.1689\n",
      "Epoch 92/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7126.8062 - val_loss: 6923.2163\n",
      "Epoch 93/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7438.9292 - val_loss: 7111.1543\n",
      "Epoch 94/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7380.4043 - val_loss: 6953.1084\n",
      "Epoch 95/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7192.4019 - val_loss: 6724.9922\n",
      "Epoch 96/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7198.0381 - val_loss: 6979.1528\n",
      "Epoch 97/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 7232.5376 - val_loss: 7317.7988\n",
      "Epoch 98/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 7513.4761 - val_loss: 6802.8564\n",
      "Epoch 99/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 7251.4897 - val_loss: 6824.1602\n",
      "Epoch 100/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7199.6885 - val_loss: 6914.0840\n",
      "Epoch 101/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 7220.7817 - val_loss: 6750.6299\n",
      "Epoch 102/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 7120.01 - 0s 3ms/step - loss: 7126.1548 - val_loss: 6835.0254\n",
      "Epoch 103/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7225.3804 - val_loss: 6828.0425\n",
      "Epoch 104/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7437.7949 - val_loss: 7332.8325\n",
      "Epoch 105/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7329.0474 - val_loss: 7276.9028\n",
      "Epoch 106/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7462.9507 - val_loss: 6971.8447\n",
      "Epoch 107/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7216.8838 - val_loss: 7127.8042\n",
      "Epoch 108/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6992.8555 - val_loss: 6860.1567\n",
      "Epoch 109/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7135.5654 - val_loss: 6835.2983\n",
      "Epoch 110/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7149.9180 - val_loss: 6917.4409\n",
      "Epoch 111/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7254.8008 - val_loss: 6941.5522\n",
      "Epoch 112/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7418.9897 - val_loss: 7140.0156\n",
      "Epoch 113/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7520.3115 - val_loss: 7016.6147\n",
      "Epoch 114/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7197.6660 - val_loss: 6975.1113\n",
      "Epoch 115/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7127.0688 - val_loss: 7186.6299\n",
      "Epoch 116/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7138.7173 - val_loss: 6909.8584\n",
      "Epoch 117/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7060.5601 - val_loss: 6838.4395\n",
      "Epoch 118/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7214.8213 - val_loss: 6913.5229\n",
      "Epoch 119/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7152.5024 - val_loss: 6837.2314\n",
      "Epoch 120/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7193.0034 - val_loss: 7027.8242\n",
      "Epoch 121/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7054.6909 - val_loss: 6825.1538\n",
      "Epoch 122/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7074.2637 - val_loss: 6914.1880\n",
      "Epoch 123/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6977.7539 - val_loss: 6866.7017\n",
      "Epoch 124/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7168.7832 - val_loss: 6829.2935\n",
      "Epoch 125/300\n",
      "59/79 [=====================>........] - ETA: 0s - loss: 6994.1245\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7152.8921 - val_loss: 7322.5259\n",
      "Epoch 126/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7233.2734 - val_loss: 6775.0967\n",
      "Epoch 127/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6981.4863 - val_loss: 6740.6577\n",
      "Epoch 128/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6965.2861 - val_loss: 7240.4648\n",
      "Epoch 129/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6950.5044 - val_loss: 6775.4663\n",
      "Epoch 130/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6819.0728 - val_loss: 6967.3364\n",
      "Epoch 131/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6925.7627 - val_loss: 6793.0679\n",
      "Epoch 132/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6957.1768 - val_loss: 6992.3999\n",
      "Epoch 133/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6828.8784 - val_loss: 6961.6147\n",
      "Epoch 134/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6860.2705 - val_loss: 6769.8013\n",
      "Epoch 135/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6716.8906 - val_loss: 6720.9751\n",
      "Epoch 136/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6753.9087 - val_loss: 6955.1836\n",
      "Epoch 137/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6683.1733 - val_loss: 6709.7539\n",
      "Epoch 138/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6962.8057 - val_loss: 7050.7842\n",
      "Epoch 139/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6995.5156 - val_loss: 7103.2578\n",
      "Epoch 140/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6935.6904 - val_loss: 6745.5938\n",
      "Epoch 141/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6906.6821 - val_loss: 6897.9048\n",
      "Epoch 142/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6940.1782 - val_loss: 6848.5830\n",
      "Epoch 143/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6736.8462 - val_loss: 6946.9590\n",
      "Epoch 144/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6878.9746 - val_loss: 6744.7407\n",
      "Epoch 145/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6887.2578 - val_loss: 6913.1543\n",
      "Epoch 146/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 6809.51 - 0s 2ms/step - loss: 6851.0532 - val_loss: 6844.0664\n",
      "Epoch 147/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6962.7354 - val_loss: 7011.5000\n",
      "Epoch 148/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6891.2236 - val_loss: 6710.1709\n",
      "Epoch 149/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6985.4321 - val_loss: 6986.4302\n",
      "Epoch 150/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7073.4365 - val_loss: 6918.0483\n",
      "Epoch 151/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6945.5571 - val_loss: 6685.8755\n",
      "Epoch 152/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6715.1348 - val_loss: 6771.2183\n",
      "Epoch 153/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6719.6113 - val_loss: 6830.4595\n",
      "Epoch 154/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6909.1274 - val_loss: 6733.0698\n",
      "Epoch 155/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6722.8032 - val_loss: 6858.5513\n",
      "Epoch 156/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6703.0742 - val_loss: 6957.3003\n",
      "Epoch 157/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6856.2798 - val_loss: 6869.0957\n",
      "Epoch 158/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6945.7456 - val_loss: 6798.0854\n",
      "Epoch 159/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6884.8999 - val_loss: 7117.9004\n",
      "Epoch 160/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7089.3691 - val_loss: 6682.6675\n",
      "Epoch 161/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6855.6313 - val_loss: 6882.0181\n",
      "Epoch 162/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6919.6919 - val_loss: 6939.0713\n",
      "Epoch 163/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6949.7803 - val_loss: 6677.8960\n",
      "Epoch 164/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6940.2026 - val_loss: 6914.2334\n",
      "Epoch 165/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6990.2856 - val_loss: 6829.1411\n",
      "Epoch 166/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6974.2720 - val_loss: 6869.5669\n",
      "Epoch 167/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6755.7075 - val_loss: 6845.7217\n",
      "Epoch 168/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6822.4785 - val_loss: 7151.4575\n",
      "Epoch 169/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7134.6055 - val_loss: 6889.2100\n",
      "Epoch 170/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7120.8481 - val_loss: 6860.7949\n",
      "Epoch 171/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6699.8438 - val_loss: 6759.6045\n",
      "Epoch 172/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6823.6982 - val_loss: 6956.1299\n",
      "Epoch 173/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6954.4272 - val_loss: 6722.3569\n",
      "Epoch 174/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6810.1216 - val_loss: 6791.2075\n",
      "Epoch 175/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6839.7134 - val_loss: 6761.1406\n",
      "Epoch 176/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7157.8599 - val_loss: 6740.0791\n",
      "Epoch 177/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6564.3525 - val_loss: 6689.7080\n",
      "Epoch 178/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6847.3550 - val_loss: 6992.2769\n",
      "Epoch 179/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6829.9438 - val_loss: 6756.1743\n",
      "Epoch 180/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6636.3481 - val_loss: 6906.1270\n",
      "Epoch 181/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6705.4658 - val_loss: 6796.8906\n",
      "Epoch 182/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6684.8320 - val_loss: 6765.8818\n",
      "Epoch 183/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6710.7373 - val_loss: 6742.1235\n",
      "Epoch 184/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6844.7007 - val_loss: 6874.0762\n",
      "Epoch 185/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6805.1636 - val_loss: 6943.6089\n",
      "Epoch 186/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6703.5376 - val_loss: 6857.6533\n",
      "Epoch 187/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7163.1064 - val_loss: 6841.4238\n",
      "Epoch 188/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7035.7739 - val_loss: 6787.6172\n",
      "Epoch 189/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6773.6348 - val_loss: 6742.9165\n",
      "Epoch 190/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6801.6631 - val_loss: 6833.9771\n",
      "Epoch 191/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6765.8833 - val_loss: 6910.8691\n",
      "Epoch 192/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6864.2915 - val_loss: 6617.7969\n",
      "Epoch 193/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6639.0322 - val_loss: 6674.8369\n",
      "Epoch 194/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6665.3936 - val_loss: 6822.2778\n",
      "Epoch 195/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6657.4546 - val_loss: 6836.2393\n",
      "Epoch 196/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6651.1670 - val_loss: 6692.6606\n",
      "Epoch 197/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6577.1660 - val_loss: 6651.8472\n",
      "Epoch 198/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6901.7583 - val_loss: 6875.4653\n",
      "Epoch 199/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6674.2026 - val_loss: 6645.3999\n",
      "Epoch 200/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6766.2148 - val_loss: 6481.7583\n",
      "Epoch 201/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6628.0205 - val_loss: 6876.6445\n",
      "Epoch 202/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6914.5254 - val_loss: 6739.0469\n",
      "Epoch 203/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6795.4316 - val_loss: 6653.1030\n",
      "Epoch 204/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6888.5991 - val_loss: 6636.7754\n",
      "Epoch 205/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6681.5425 - val_loss: 6720.0088\n",
      "Epoch 206/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6712.1250 - val_loss: 6603.0435\n",
      "Epoch 207/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6469.7554 - val_loss: 6634.6587\n",
      "Epoch 208/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6746.4844 - val_loss: 6594.9551\n",
      "Epoch 209/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6432.9531 - val_loss: 6572.9136\n",
      "Epoch 210/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6594.3945 - val_loss: 6782.4863\n",
      "Epoch 211/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6590.9839 - val_loss: 6498.0107\n",
      "Epoch 212/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6781.7705 - val_loss: 7019.4858\n",
      "Epoch 213/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7312.7368 - val_loss: 6693.1606\n",
      "Epoch 214/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6771.2812 - val_loss: 6537.6733\n",
      "Epoch 215/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6632.6895 - val_loss: 6539.2290\n",
      "Epoch 216/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6957.4146 - val_loss: 6570.3335\n",
      "Epoch 217/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6592.0654 - val_loss: 6713.9160\n",
      "Epoch 218/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6708.7598 - val_loss: 6589.9209\n",
      "Epoch 219/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6502.4473 - val_loss: 6491.5806\n",
      "Epoch 220/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6479.0439 - val_loss: 6504.0942\n",
      "Epoch 221/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6607.8765 - val_loss: 6615.0503\n",
      "Epoch 222/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6462.9707 - val_loss: 6493.8657\n",
      "Epoch 223/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6547.7993 - val_loss: 6760.4390\n",
      "Epoch 224/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6566.8384 - val_loss: 6574.6152\n",
      "Epoch 225/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6605.5220 - val_loss: 6485.7070\n",
      "Epoch 226/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6575.2505 - val_loss: 6727.3979\n",
      "Epoch 227/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6683.9736 - val_loss: 6583.9434\n",
      "Epoch 228/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7155.4551 - val_loss: 6653.1108\n",
      "Epoch 229/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6943.5879 - val_loss: 6691.3921\n",
      "Epoch 230/300\n",
      "77/79 [============================>.] - ETA: 0s - loss: 6805.9458\n",
      "Epoch 00230: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6845.9385 - val_loss: 6880.4424\n",
      "Epoch 231/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6915.0093 - val_loss: 6926.9067\n",
      "Epoch 232/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6717.5791 - val_loss: 6752.7812\n",
      "Epoch 233/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6497.6279 - val_loss: 6704.7495\n",
      "Epoch 234/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6592.0439 - val_loss: 6548.0942\n",
      "Epoch 235/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6624.0713 - val_loss: 6622.4688\n",
      "Epoch 236/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6487.8022 - val_loss: 6683.7578\n",
      "Epoch 237/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6595.8145 - val_loss: 6526.8955\n",
      "Epoch 238/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6661.6509 - val_loss: 6721.3555\n",
      "Epoch 239/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6445.7163 - val_loss: 6564.0078\n",
      "Epoch 240/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6544.0635 - val_loss: 6628.4165\n",
      "Epoch 241/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6601.9429 - val_loss: 6654.0894\n",
      "Epoch 242/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6377.5088 - val_loss: 6663.2285\n",
      "Epoch 243/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6632.5283 - val_loss: 6794.6938\n",
      "Epoch 244/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6519.4814 - val_loss: 6545.4507\n",
      "Epoch 245/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6339.6689 - val_loss: 6606.1406\n",
      "Epoch 246/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6551.4639 - val_loss: 6519.5229\n",
      "Epoch 247/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6290.2173 - val_loss: 6739.9810\n",
      "Epoch 248/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6297.2500 - val_loss: 6564.1055\n",
      "Epoch 249/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6373.1455 - val_loss: 6659.5498\n",
      "Epoch 250/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7294.0859 - val_loss: 7091.3540\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 12038.9385 - val_loss: 9293.4648\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 10490.4248 - val_loss: 8475.7188\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9719.0693 - val_loss: 8397.7773\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9523.9404 - val_loss: 7872.4932\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9204.6328 - val_loss: 8433.7148\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9277.6719 - val_loss: 8000.1172\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9137.9863 - val_loss: 7990.8848\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 9093.3984 - val_loss: 7736.5381\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8931.0166 - val_loss: 7857.0215\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8857.9355 - val_loss: 7754.7974\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8841.3135 - val_loss: 7758.7002\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8648.9404 - val_loss: 7463.7676\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8649.1318 - val_loss: 7602.0703\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8615.8955 - val_loss: 8061.0488\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8853.6445 - val_loss: 7717.6372\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8453.6855 - val_loss: 7881.4888\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8532.1172 - val_loss: 7864.3940\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8560.6982 - val_loss: 7374.2397\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 8478.80 - 0s 3ms/step - loss: 8491.7578 - val_loss: 7470.3511\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8536.9697 - val_loss: 7405.5581\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8482.6875 - val_loss: 8274.1670\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8419.9287 - val_loss: 7591.8760\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8336.7490 - val_loss: 7852.2192\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8312.3779 - val_loss: 7615.9780\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8432.7725 - val_loss: 7473.2119\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8539.3467 - val_loss: 7582.7085\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8369.6680 - val_loss: 7330.8882\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8218.2402 - val_loss: 7411.5806\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8296.2705 - val_loss: 7386.8037\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8280.6904 - val_loss: 7638.4106\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8215.4902 - val_loss: 7250.4316\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8378.3564 - val_loss: 7553.0703\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8183.2798 - val_loss: 7305.1909\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8106.2129 - val_loss: 7282.1162\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8213.6064 - val_loss: 7429.5508\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8092.2017 - val_loss: 7456.6089\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8141.1479 - val_loss: 7551.2905\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7999.9692 - val_loss: 7320.8452\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8172.0483 - val_loss: 7231.2026\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8061.3457 - val_loss: 7291.3667\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8264.0010 - val_loss: 8149.4199\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8090.6255 - val_loss: 7202.2490\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8062.8936 - val_loss: 7441.1060\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8159.9912 - val_loss: 7763.1699\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8009.3511 - val_loss: 7352.6353\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7935.0312 - val_loss: 7412.7607\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7991.6562 - val_loss: 7478.4165\n",
      "Epoch 48/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8042.7598 - val_loss: 7323.7896\n",
      "Epoch 49/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8000.5288 - val_loss: 7211.0010\n",
      "Epoch 50/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8020.6147 - val_loss: 7344.4692\n",
      "Epoch 51/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7871.5327 - val_loss: 7028.5278\n",
      "Epoch 52/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7947.4214 - val_loss: 7298.2949\n",
      "Epoch 53/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8088.3257 - val_loss: 7302.4536\n",
      "Epoch 54/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7881.1304 - val_loss: 7058.0264\n",
      "Epoch 55/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7884.7700 - val_loss: 7253.3804\n",
      "Epoch 56/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7999.0083 - val_loss: 7694.7227\n",
      "Epoch 57/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8030.4683 - val_loss: 7323.5562\n",
      "Epoch 58/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7850.7441 - val_loss: 7309.6460\n",
      "Epoch 59/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7792.2734 - val_loss: 6986.9849\n",
      "Epoch 60/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7859.1641 - val_loss: 7058.5879\n",
      "Epoch 61/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7733.1060 - val_loss: 6949.6675\n",
      "Epoch 62/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7708.4917 - val_loss: 7008.9951\n",
      "Epoch 63/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7928.6553 - val_loss: 7126.8877\n",
      "Epoch 64/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7900.7295 - val_loss: 7447.6099\n",
      "Epoch 65/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7606.9443 - val_loss: 7105.5020\n",
      "Epoch 66/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7751.4355 - val_loss: 7506.8696\n",
      "Epoch 67/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7983.1631 - val_loss: 7237.4307\n",
      "Epoch 68/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7861.5273 - val_loss: 7233.0186\n",
      "Epoch 69/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7942.4019 - val_loss: 7296.2305\n",
      "Epoch 70/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7918.3452 - val_loss: 7124.6543\n",
      "Epoch 71/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7785.1182 - val_loss: 7229.6890\n",
      "Epoch 72/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7723.3457 - val_loss: 7367.7871\n",
      "Epoch 73/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7886.6475 - val_loss: 7395.1108\n",
      "Epoch 74/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8049.5020 - val_loss: 7250.1787\n",
      "Epoch 75/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7857.3042 - val_loss: 7402.1147\n",
      "Epoch 76/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7825.3276 - val_loss: 7183.9590\n",
      "Epoch 77/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8025.9736 - val_loss: 7187.4458\n",
      "Epoch 78/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7884.8599 - val_loss: 7389.2471\n",
      "Epoch 79/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7760.0996 - val_loss: 7530.9590\n",
      "Epoch 80/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7682.8188 - val_loss: 7163.6821\n",
      "Epoch 81/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7736.8647 - val_loss: 7484.2393\n",
      "Epoch 82/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7807.7798 - val_loss: 7711.5723\n",
      "Epoch 83/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7812.1743 - val_loss: 7001.4575\n",
      "Epoch 84/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7716.9629 - val_loss: 7176.6499\n",
      "Epoch 85/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7785.7290 - val_loss: 7420.1094\n",
      "Epoch 86/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7878.4053 - val_loss: 7212.8755\n",
      "Epoch 87/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7623.7354 - val_loss: 7213.4741\n",
      "Epoch 88/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7525.8330 - val_loss: 7223.5356\n",
      "Epoch 89/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7785.7949 - val_loss: 7051.0215\n",
      "Epoch 90/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7677.3936 - val_loss: 7043.1318\n",
      "Epoch 91/300\n",
      "75/79 [===========================>..] - ETA: 0s - loss: 7591.7036\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7664.2515 - val_loss: 7051.4688\n",
      "Epoch 92/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7435.7476 - val_loss: 7369.1274\n",
      "Epoch 93/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7679.6763 - val_loss: 7011.9907\n",
      "Epoch 94/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7429.8252 - val_loss: 6971.0693\n",
      "Epoch 95/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7433.4004 - val_loss: 7194.7656\n",
      "Epoch 96/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7503.1274 - val_loss: 6976.1978\n",
      "Epoch 97/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 7433.90 - 0s 3ms/step - loss: 7398.4268 - val_loss: 7002.4678\n",
      "Epoch 98/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7343.5381 - val_loss: 7053.6558\n",
      "Epoch 99/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7373.9067 - val_loss: 7121.3848\n",
      "Epoch 100/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7230.2690 - val_loss: 7182.4546\n",
      "Epoch 101/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7316.0669 - val_loss: 6883.9761\n",
      "Epoch 102/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7335.0303 - val_loss: 7145.1099\n",
      "Epoch 103/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7490.9907 - val_loss: 6993.0972\n",
      "Epoch 104/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7353.3682 - val_loss: 6863.1104\n",
      "Epoch 105/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7225.0054 - val_loss: 6996.5015\n",
      "Epoch 106/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7362.5791 - val_loss: 7028.4155\n",
      "Epoch 107/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7280.0864 - val_loss: 6927.3848\n",
      "Epoch 108/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7204.6675 - val_loss: 6978.2036\n",
      "Epoch 109/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7342.2285 - val_loss: 6903.6948\n",
      "Epoch 110/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7290.2036 - val_loss: 6913.7070\n",
      "Epoch 111/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7225.9380 - val_loss: 6896.1851\n",
      "Epoch 112/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7320.3350 - val_loss: 6953.3652\n",
      "Epoch 113/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7336.5010 - val_loss: 7293.2168\n",
      "Epoch 114/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7521.1699 - val_loss: 6854.7593\n",
      "Epoch 115/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7342.1504 - val_loss: 7051.4580\n",
      "Epoch 116/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7172.1055 - val_loss: 7129.9414\n",
      "Epoch 117/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7280.4717 - val_loss: 7101.6875\n",
      "Epoch 118/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7390.7983 - val_loss: 7023.3979\n",
      "Epoch 119/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7289.3091 - val_loss: 7067.4326\n",
      "Epoch 120/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7358.5059 - val_loss: 6940.2168\n",
      "Epoch 121/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7370.7773 - val_loss: 7197.5933\n",
      "Epoch 122/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7307.1948 - val_loss: 6825.5444\n",
      "Epoch 123/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7208.8696 - val_loss: 6995.9883\n",
      "Epoch 124/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7219.1919 - val_loss: 6816.3833\n",
      "Epoch 125/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7364.5269 - val_loss: 6896.8501\n",
      "Epoch 126/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7148.6196 - val_loss: 6778.4307\n",
      "Epoch 127/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7074.5371 - val_loss: 7108.9175\n",
      "Epoch 128/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7479.9272 - val_loss: 6969.9971\n",
      "Epoch 129/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7216.5679 - val_loss: 6931.1675\n",
      "Epoch 130/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7335.8574 - val_loss: 7307.4819\n",
      "Epoch 131/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7298.8354 - val_loss: 6782.7227\n",
      "Epoch 132/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7401.7769 - val_loss: 7222.0791\n",
      "Epoch 133/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7217.3848 - val_loss: 6994.0479\n",
      "Epoch 134/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7328.0103 - val_loss: 6960.9058\n",
      "Epoch 135/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6983.8882 - val_loss: 6730.5566\n",
      "Epoch 136/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7379.1841 - val_loss: 6876.8354\n",
      "Epoch 137/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7145.5420 - val_loss: 7143.5679\n",
      "Epoch 138/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7158.1885 - val_loss: 6862.6909\n",
      "Epoch 139/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7263.5698 - val_loss: 7455.0215\n",
      "Epoch 140/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7371.0776 - val_loss: 6901.7095\n",
      "Epoch 141/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7423.4751 - val_loss: 7349.7148\n",
      "Epoch 142/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7081.0464 - val_loss: 7066.3115\n",
      "Epoch 143/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7308.2310 - val_loss: 6862.7231\n",
      "Epoch 144/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7146.9741 - val_loss: 7034.2402\n",
      "Epoch 145/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7498.0586 - val_loss: 6920.5312\n",
      "Epoch 146/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7142.4761 - val_loss: 7089.2334\n",
      "Epoch 147/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7327.4365 - val_loss: 6946.5029\n",
      "Epoch 148/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7167.1665 - val_loss: 7090.6494\n",
      "Epoch 149/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7177.9653 - val_loss: 7263.1279\n",
      "Epoch 150/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7015.0439 - val_loss: 6777.3594\n",
      "Epoch 151/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7144.3433 - val_loss: 6850.8828\n",
      "Epoch 152/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7143.2969 - val_loss: 7042.9629\n",
      "Epoch 153/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7463.8311 - val_loss: 7008.6860\n",
      "Epoch 154/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7121.1504 - val_loss: 6997.6558\n",
      "Epoch 155/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7235.3936 - val_loss: 7812.9321\n",
      "Epoch 156/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7214.1665 - val_loss: 6991.2388\n",
      "Epoch 157/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7172.5918 - val_loss: 6952.7861\n",
      "Epoch 158/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6982.7480 - val_loss: 6981.2803\n",
      "Epoch 159/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6928.2446 - val_loss: 6976.1323\n",
      "Epoch 160/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7227.6738 - val_loss: 6810.3306\n",
      "Epoch 161/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6895.3726 - val_loss: 6856.1426\n",
      "Epoch 162/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6952.3164 - val_loss: 6967.8765\n",
      "Epoch 163/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6989.4268 - val_loss: 6987.0850\n",
      "Epoch 164/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7264.1978 - val_loss: 6897.7144\n",
      "Epoch 165/300\n",
      "77/79 [============================>.] - ETA: 0s - loss: 7213.2871\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7181.8130 - val_loss: 6879.7510\n",
      "Epoch 166/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6842.5117 - val_loss: 6702.7676\n",
      "Epoch 167/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6874.0723 - val_loss: 6654.9263\n",
      "Epoch 168/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6801.8013 - val_loss: 7065.5747\n",
      "Epoch 169/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6791.2729 - val_loss: 6913.0039\n",
      "Epoch 170/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6787.9883 - val_loss: 6852.5078\n",
      "Epoch 171/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6868.8999 - val_loss: 6858.3271\n",
      "Epoch 172/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6833.8691 - val_loss: 6917.9194\n",
      "Epoch 173/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6745.6743 - val_loss: 6748.5186\n",
      "Epoch 174/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6807.5088 - val_loss: 6720.8892\n",
      "Epoch 175/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6795.7822 - val_loss: 7000.3774\n",
      "Epoch 176/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6860.7202 - val_loss: 6998.0376\n",
      "Epoch 177/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6772.2578 - val_loss: 6752.1494\n",
      "Epoch 178/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6691.1807 - val_loss: 6830.1270\n",
      "Epoch 179/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6679.0376 - val_loss: 6733.1069\n",
      "Epoch 180/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6584.1523 - val_loss: 6833.4805\n",
      "Epoch 181/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6564.4453 - val_loss: 6898.5200\n",
      "Epoch 182/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7036.9424 - val_loss: 6794.3491\n",
      "Epoch 183/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6741.6279 - val_loss: 6772.3506\n",
      "Epoch 184/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6786.5312 - val_loss: 6743.8135\n",
      "Epoch 185/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6824.2441 - val_loss: 6937.9824\n",
      "Epoch 186/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6817.0752 - val_loss: 6635.5244\n",
      "Epoch 187/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6776.8271 - val_loss: 6752.3799\n",
      "Epoch 188/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6748.2402 - val_loss: 6742.2261\n",
      "Epoch 189/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6984.1479 - val_loss: 7129.7583\n",
      "Epoch 190/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6762.0933 - val_loss: 6569.8428\n",
      "Epoch 191/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6634.3901 - val_loss: 6772.4331\n",
      "Epoch 192/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6890.7437 - val_loss: 6728.2559\n",
      "Epoch 193/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6977.3286 - val_loss: 7042.8345\n",
      "Epoch 194/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6986.8301 - val_loss: 6940.7056\n",
      "Epoch 195/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6936.1987 - val_loss: 6720.8848\n",
      "Epoch 196/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7019.9766 - val_loss: 6756.0356\n",
      "Epoch 197/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6790.7363 - val_loss: 6702.7847\n",
      "Epoch 198/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6651.1753 - val_loss: 6742.0513\n",
      "Epoch 199/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6707.9956 - val_loss: 6748.0957\n",
      "Epoch 200/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6804.4741 - val_loss: 6712.6182\n",
      "Epoch 201/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6657.5703 - val_loss: 6793.0479\n",
      "Epoch 202/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6621.6582 - val_loss: 6825.0913\n",
      "Epoch 203/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6630.9150 - val_loss: 6726.2422\n",
      "Epoch 204/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6671.7173 - val_loss: 6680.6826\n",
      "Epoch 205/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6771.2705 - val_loss: 6711.8076\n",
      "Epoch 206/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6766.7769 - val_loss: 6729.1323\n",
      "Epoch 207/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6711.6929 - val_loss: 6723.8789\n",
      "Epoch 208/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6880.2266 - val_loss: 6828.8003\n",
      "Epoch 209/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6903.2798 - val_loss: 6943.9390\n",
      "Epoch 210/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6699.6685 - val_loss: 6709.2524\n",
      "Epoch 211/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6623.9619 - val_loss: 6634.1836\n",
      "Epoch 212/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6662.8311 - val_loss: 6727.5811\n",
      "Epoch 213/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6640.2280 - val_loss: 6669.5933\n",
      "Epoch 214/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6670.0381 - val_loss: 6934.7065\n",
      "Epoch 215/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6538.7856 - val_loss: 6655.2959\n",
      "Epoch 216/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6571.0464 - val_loss: 6744.4077\n",
      "Epoch 217/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6756.7144 - val_loss: 6679.0640\n",
      "Epoch 218/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6661.7295 - val_loss: 6545.9658\n",
      "Epoch 219/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6664.5171 - val_loss: 6736.9639\n",
      "Epoch 220/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6664.4463 - val_loss: 6825.8179\n",
      "Epoch 221/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6844.4082 - val_loss: 6796.9355\n",
      "Epoch 222/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6783.2910 - val_loss: 6802.2773\n",
      "Epoch 223/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6770.8584 - val_loss: 6869.4888\n",
      "Epoch 224/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6874.5693 - val_loss: 6894.9824\n",
      "Epoch 225/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6777.6255 - val_loss: 6734.9683\n",
      "Epoch 226/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6766.9473 - val_loss: 6606.6709\n",
      "Epoch 227/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6597.8335 - val_loss: 6688.4585\n",
      "Epoch 228/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 6602.9541 - val_loss: 6821.9521\n",
      "Epoch 229/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 6789.0571 - val_loss: 6761.2866\n",
      "Epoch 230/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6872.6943 - val_loss: 6762.1182\n",
      "Epoch 231/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6779.0928 - val_loss: 6646.8984\n",
      "Epoch 232/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6791.4121 - val_loss: 6947.0396\n",
      "Epoch 233/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6844.2148 - val_loss: 7273.2407\n",
      "Epoch 234/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6685.7349 - val_loss: 6782.6958\n",
      "Epoch 235/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6725.8257 - val_loss: 6827.5244\n",
      "Epoch 236/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6713.7871 - val_loss: 6905.2632\n",
      "Epoch 237/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6548.8140 - val_loss: 6717.6689\n",
      "Epoch 238/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6647.0259 - val_loss: 6825.1797\n",
      "Epoch 239/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6657.3677 - val_loss: 6838.3804\n",
      "Epoch 240/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6711.5825 - val_loss: 6884.6333\n",
      "Epoch 241/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6760.7368 - val_loss: 6868.0674\n",
      "Epoch 242/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6583.7363 - val_loss: 6577.9854\n",
      "Epoch 243/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6705.6270 - val_loss: 7054.5732\n",
      "Epoch 244/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6649.9365 - val_loss: 6766.8369\n",
      "Epoch 245/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6666.6865 - val_loss: 6646.9160\n",
      "Epoch 246/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6568.6743 - val_loss: 6489.6216\n",
      "Epoch 247/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6495.8716 - val_loss: 6705.0566\n",
      "Epoch 248/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6514.8530 - val_loss: 6548.2202\n",
      "Epoch 249/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6609.4355 - val_loss: 6754.8472\n",
      "Epoch 250/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6578.6089 - val_loss: 6706.0142\n",
      "Epoch 251/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6511.5557 - val_loss: 6646.9570\n",
      "Epoch 252/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6554.0117 - val_loss: 6832.1733\n",
      "Epoch 253/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6480.1978 - val_loss: 6611.2842\n",
      "Epoch 254/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6498.4756 - val_loss: 6713.4658\n",
      "Epoch 255/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6471.5957 - val_loss: 6872.2993\n",
      "Epoch 256/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6563.4902 - val_loss: 6692.4751\n",
      "Epoch 257/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6760.4556 - val_loss: 7184.7812\n",
      "Epoch 258/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6616.9956 - val_loss: 6672.1230\n",
      "Epoch 259/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6724.0762 - val_loss: 6633.5991\n",
      "Epoch 260/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6641.7686 - val_loss: 6719.9727\n",
      "Epoch 261/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6410.1782 - val_loss: 6730.3057\n",
      "Epoch 262/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6724.5171 - val_loss: 6825.3696\n",
      "Epoch 263/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6719.0156 - val_loss: 6845.9194\n",
      "Epoch 264/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6558.9224 - val_loss: 6756.1147\n",
      "Epoch 265/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6573.1440 - val_loss: 6686.9126\n",
      "Epoch 266/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6406.9888 - val_loss: 6913.5005\n",
      "Epoch 267/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6507.9292 - val_loss: 6798.9585\n",
      "Epoch 268/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6475.8257 - val_loss: 6748.2852\n",
      "Epoch 269/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6692.9858 - val_loss: 6651.9575\n",
      "Epoch 270/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6475.6479 - val_loss: 6768.7490\n",
      "Epoch 271/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6543.4829 - val_loss: 6800.3721\n",
      "Epoch 272/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6456.6387 - val_loss: 7065.9849\n",
      "Epoch 273/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6751.5347 - val_loss: 7028.2920\n",
      "Epoch 274/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6536.9146 - val_loss: 6766.8955\n",
      "Epoch 275/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6539.0200 - val_loss: 6720.8237\n",
      "Epoch 276/300\n",
      "77/79 [============================>.] - ETA: 0s - loss: 6523.7686\n",
      "Epoch 00276: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6539.9961 - val_loss: 6710.9976\n",
      "Epoch 277/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6576.1030 - val_loss: 6737.1484\n",
      "Epoch 278/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6547.9419 - val_loss: 6665.7588\n",
      "Epoch 279/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6605.1226 - val_loss: 7013.2754\n",
      "Epoch 280/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6625.7329 - val_loss: 6662.3589\n",
      "Epoch 281/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6465.4761 - val_loss: 6894.1934\n",
      "Epoch 282/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6376.7300 - val_loss: 6744.0806\n",
      "Epoch 283/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6466.1055 - val_loss: 6612.5073\n",
      "Epoch 284/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6449.6611 - val_loss: 6609.0591\n",
      "Epoch 285/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6455.3960 - val_loss: 6821.4766\n",
      "Epoch 286/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6391.9648 - val_loss: 6547.9028\n",
      "Epoch 287/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6305.2378 - val_loss: 6751.2090\n",
      "Epoch 288/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6374.3027 - val_loss: 6590.6270\n",
      "Epoch 289/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6286.2188 - val_loss: 6613.2690\n",
      "Epoch 290/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6249.9351 - val_loss: 6760.3467\n",
      "Epoch 291/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6490.1064 - val_loss: 6723.0854\n",
      "Epoch 292/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6368.9736 - val_loss: 6722.5781\n",
      "Epoch 293/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6275.2041 - val_loss: 6659.7812\n",
      "Epoch 294/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6325.8037 - val_loss: 6530.1694\n",
      "Epoch 295/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6301.0103 - val_loss: 6746.4707\n",
      "Epoch 296/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6270.9658 - val_loss: 6591.7549\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 11880.9004 - val_loss: 9033.3594\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 10362.1260 - val_loss: 8556.8848\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9718.7607 - val_loss: 8113.9751\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9517.1104 - val_loss: 8237.0400\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9322.8291 - val_loss: 8429.3389\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9222.7842 - val_loss: 9145.2471\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9085.3711 - val_loss: 7783.3008\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9209.2471 - val_loss: 8013.6494\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8915.7480 - val_loss: 7599.4199\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8895.5381 - val_loss: 8029.8662\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8770.5107 - val_loss: 7641.5493\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8808.5029 - val_loss: 7843.3120\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8729.0010 - val_loss: 8093.5425\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8731.7070 - val_loss: 7821.6548\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8544.1445 - val_loss: 7862.9268\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8653.7090 - val_loss: 7418.2432\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8582.6885 - val_loss: 7575.0205\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8370.8975 - val_loss: 7415.1411\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8327.8213 - val_loss: 7569.2290\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8328.3574 - val_loss: 7684.9565\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8481.1475 - val_loss: 7566.6768\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8426.4951 - val_loss: 7725.0508\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8265.8350 - val_loss: 7377.5068\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8236.8486 - val_loss: 7589.9258\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8100.2588 - val_loss: 7822.9448\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8275.6777 - val_loss: 7479.3384\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8228.7607 - val_loss: 7986.8506\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8082.7021 - val_loss: 7800.9136\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8032.2002 - val_loss: 7489.9902\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8153.1567 - val_loss: 7755.0986\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8082.3940 - val_loss: 7777.7104\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7860.1621 - val_loss: 7709.5952\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7989.9585 - val_loss: 7432.0195\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7960.5908 - val_loss: 7550.1782\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7955.2275 - val_loss: 7556.0376\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7897.0205 - val_loss: 7315.9170\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7911.3022 - val_loss: 7396.9678\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7940.1216 - val_loss: 7943.1108\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7833.5068 - val_loss: 7388.6616\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8070.3384 - val_loss: 7604.0591\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7974.3604 - val_loss: 7366.5698\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7855.2852 - val_loss: 7442.9995\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7815.2861 - val_loss: 7320.8169\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7654.5088 - val_loss: 7571.2876\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7868.0435 - val_loss: 7405.8130\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7775.2808 - val_loss: 7344.0781\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7698.3018 - val_loss: 7533.5698\n",
      "Epoch 48/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7804.1196 - val_loss: 7839.8057\n",
      "Epoch 49/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7808.3535 - val_loss: 7627.1260\n",
      "Epoch 50/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7882.5449 - val_loss: 7953.2539\n",
      "Epoch 51/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7853.7847 - val_loss: 7320.1421\n",
      "Epoch 52/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7734.9961 - val_loss: 7583.9380\n",
      "Epoch 53/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7830.0688 - val_loss: 7628.1294\n",
      "Epoch 54/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7690.5596 - val_loss: 7252.2427\n",
      "Epoch 55/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7832.6021 - val_loss: 7510.7275\n",
      "Epoch 56/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7728.4019 - val_loss: 7495.7500\n",
      "Epoch 57/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7633.9834 - val_loss: 7427.6191\n",
      "Epoch 58/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7648.9883 - val_loss: 7254.9229\n",
      "Epoch 59/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7782.1914 - val_loss: 7305.6216\n",
      "Epoch 60/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7772.3706 - val_loss: 7900.7871\n",
      "Epoch 61/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7928.1504 - val_loss: 7712.6704\n",
      "Epoch 62/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7707.6406 - val_loss: 7373.2183\n",
      "Epoch 63/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7468.8730 - val_loss: 7237.9878\n",
      "Epoch 64/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7554.8286 - val_loss: 7285.2695\n",
      "Epoch 65/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7541.8555 - val_loss: 7709.7671\n",
      "Epoch 66/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7660.5020 - val_loss: 7170.3540\n",
      "Epoch 67/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7591.1245 - val_loss: 7231.8511\n",
      "Epoch 68/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7753.7207 - val_loss: 7338.0942\n",
      "Epoch 69/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7693.5791 - val_loss: 7219.4014\n",
      "Epoch 70/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7618.3765 - val_loss: 7436.8228\n",
      "Epoch 71/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7557.8286 - val_loss: 7378.0312\n",
      "Epoch 72/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7390.4736 - val_loss: 7572.4009\n",
      "Epoch 73/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7643.9409 - val_loss: 7423.1060\n",
      "Epoch 74/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7504.9370 - val_loss: 7205.8833\n",
      "Epoch 75/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7545.8164 - val_loss: 7115.8789\n",
      "Epoch 76/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7676.2109 - val_loss: 7281.0283\n",
      "Epoch 77/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7586.4741 - val_loss: 7480.1929\n",
      "Epoch 78/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7548.6016 - val_loss: 6949.3755\n",
      "Epoch 79/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7507.6538 - val_loss: 7804.2480\n",
      "Epoch 80/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7612.4634 - val_loss: 7090.5444\n",
      "Epoch 81/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7499.0405 - val_loss: 7228.7383\n",
      "Epoch 82/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7289.9883 - val_loss: 7257.9971\n",
      "Epoch 83/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7548.0430 - val_loss: 7721.0620\n",
      "Epoch 84/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7648.1235 - val_loss: 7362.2554\n",
      "Epoch 85/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7637.0547 - val_loss: 7435.3159\n",
      "Epoch 86/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7551.3604 - val_loss: 7146.6733\n",
      "Epoch 87/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7425.8330 - val_loss: 7053.0923\n",
      "Epoch 88/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7352.6719 - val_loss: 7366.6685\n",
      "Epoch 89/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7687.4482 - val_loss: 7608.7739\n",
      "Epoch 90/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7815.9248 - val_loss: 7870.8271\n",
      "Epoch 91/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7624.6445 - val_loss: 7222.0156\n",
      "Epoch 92/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7567.5815 - val_loss: 7175.6758\n",
      "Epoch 93/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7545.7998 - val_loss: 7381.3765\n",
      "Epoch 94/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7669.6519 - val_loss: 7273.1426\n",
      "Epoch 95/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7646.9717 - val_loss: 7250.6167\n",
      "Epoch 96/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7525.2168 - val_loss: 7068.0674\n",
      "Epoch 97/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7465.6616 - val_loss: 7336.2002\n",
      "Epoch 98/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7766.0723 - val_loss: 7680.1099\n",
      "Epoch 99/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 8051.4302 - val_loss: 7315.9951\n",
      "Epoch 100/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7580.5215 - val_loss: 7325.7607\n",
      "Epoch 101/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7408.6631 - val_loss: 7183.8428\n",
      "Epoch 102/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7430.6978 - val_loss: 7276.0845\n",
      "Epoch 103/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7350.4922 - val_loss: 7213.4316\n",
      "Epoch 104/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7419.0649 - val_loss: 7426.7808\n",
      "Epoch 105/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7488.9585 - val_loss: 7409.4233\n",
      "Epoch 106/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7393.4111 - val_loss: 7069.9639\n",
      "Epoch 107/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7340.8896 - val_loss: 7043.9766\n",
      "Epoch 108/300\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 7375.1265\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7355.3115 - val_loss: 7155.8452\n",
      "Epoch 109/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7199.4854 - val_loss: 7066.6440\n",
      "Epoch 110/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7195.1924 - val_loss: 7001.5000\n",
      "Epoch 111/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7146.0518 - val_loss: 7448.8149\n",
      "Epoch 112/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7183.2798 - val_loss: 7252.8906\n",
      "Epoch 113/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7307.1606 - val_loss: 7317.8555\n",
      "Epoch 114/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7172.8867 - val_loss: 7104.2983\n",
      "Epoch 115/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7197.1929 - val_loss: 7205.8701\n",
      "Epoch 116/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7080.1987 - val_loss: 7241.5586\n",
      "Epoch 117/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7176.2354 - val_loss: 7307.9302\n",
      "Epoch 118/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7025.7217 - val_loss: 7224.0254\n",
      "Epoch 119/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7260.5190 - val_loss: 7145.0142\n",
      "Epoch 120/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7136.4341 - val_loss: 6997.8047\n",
      "Epoch 121/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7119.9634 - val_loss: 7147.5825\n",
      "Epoch 122/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7074.4180 - val_loss: 6938.9639\n",
      "Epoch 123/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7115.0532 - val_loss: 7074.4634\n",
      "Epoch 124/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7006.5020 - val_loss: 7032.2368\n",
      "Epoch 125/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7069.8374 - val_loss: 7283.9780\n",
      "Epoch 126/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7045.6030 - val_loss: 6966.2686\n",
      "Epoch 127/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7025.8208 - val_loss: 7024.7891\n",
      "Epoch 128/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7205.4048 - val_loss: 6887.9878\n",
      "Epoch 129/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6986.7788 - val_loss: 6903.2080\n",
      "Epoch 130/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6840.5493 - val_loss: 6884.6401\n",
      "Epoch 131/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7000.8950 - val_loss: 6959.1929\n",
      "Epoch 132/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6948.0088 - val_loss: 6987.8926\n",
      "Epoch 133/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7089.6499 - val_loss: 6825.1655\n",
      "Epoch 134/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7053.7539 - val_loss: 6865.5356\n",
      "Epoch 135/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7044.0234 - val_loss: 7154.5151\n",
      "Epoch 136/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6817.0879 - val_loss: 6995.1929\n",
      "Epoch 137/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7002.5269 - val_loss: 6905.8281\n",
      "Epoch 138/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6859.2627 - val_loss: 7495.2803\n",
      "Epoch 139/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7119.6299 - val_loss: 6953.3281\n",
      "Epoch 140/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6860.2388 - val_loss: 7090.0308\n",
      "Epoch 141/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7200.5049 - val_loss: 6843.5210\n",
      "Epoch 142/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7052.8643 - val_loss: 7016.9800\n",
      "Epoch 143/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6993.2866 - val_loss: 6994.7310\n",
      "Epoch 144/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6871.2563 - val_loss: 6808.5020\n",
      "Epoch 145/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7065.6733 - val_loss: 7126.9839\n",
      "Epoch 146/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6993.6050 - val_loss: 7150.4512\n",
      "Epoch 147/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6963.0054 - val_loss: 7594.2393\n",
      "Epoch 148/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7212.6885 - val_loss: 6995.7793\n",
      "Epoch 149/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6990.3682 - val_loss: 6849.2427\n",
      "Epoch 150/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6834.2793 - val_loss: 7227.0820\n",
      "Epoch 151/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7500.0933 - val_loss: 7188.1001\n",
      "Epoch 152/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7163.3750 - val_loss: 7297.3999\n",
      "Epoch 153/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7111.7788 - val_loss: 7008.7646\n",
      "Epoch 154/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6845.4224 - val_loss: 6900.4136\n",
      "Epoch 155/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6910.3784 - val_loss: 6866.0483\n",
      "Epoch 156/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6909.2646 - val_loss: 7017.4663\n",
      "Epoch 157/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7093.5615 - val_loss: 7178.3140\n",
      "Epoch 158/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6938.4224 - val_loss: 7070.5996\n",
      "Epoch 159/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6909.3784 - val_loss: 6967.7295\n",
      "Epoch 160/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6818.8423 - val_loss: 7341.3052\n",
      "Epoch 161/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6801.1812 - val_loss: 7051.3267\n",
      "Epoch 162/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6762.2109 - val_loss: 6914.7173\n",
      "Epoch 163/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6965.2329 - val_loss: 7272.1279\n",
      "Epoch 164/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6898.2744 - val_loss: 6851.3794\n",
      "Epoch 165/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7190.5894 - val_loss: 7212.6201\n",
      "Epoch 166/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7037.4497 - val_loss: 7135.8125\n",
      "Epoch 167/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7055.2349 - val_loss: 6825.8105\n",
      "Epoch 168/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 7250.1445 - val_loss: 7527.7388\n",
      "Epoch 169/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6918.4111 - val_loss: 6985.9136\n",
      "Epoch 170/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6865.1255 - val_loss: 6808.9507\n",
      "Epoch 171/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6763.8394 - val_loss: 6981.5854\n",
      "Epoch 172/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6747.7437 - val_loss: 6873.8008\n",
      "Epoch 173/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6873.5747 - val_loss: 6901.1997\n",
      "Epoch 174/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 6994.1230\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6994.1230 - val_loss: 7132.4077\n",
      "Epoch 175/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6818.2954 - val_loss: 6907.6523\n",
      "Epoch 176/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6644.9390 - val_loss: 6870.5933\n",
      "Epoch 177/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6727.8618 - val_loss: 6664.2979\n",
      "Epoch 178/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6675.7505 - val_loss: 6854.7178\n",
      "Epoch 179/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6772.0190 - val_loss: 7028.1768\n",
      "Epoch 180/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6592.6196 - val_loss: 6855.8662\n",
      "Epoch 181/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6520.0659 - val_loss: 7096.6235\n",
      "Epoch 182/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6650.5801 - val_loss: 6847.7568\n",
      "Epoch 183/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6658.2534 - val_loss: 7000.1201\n",
      "Epoch 184/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6522.3003 - val_loss: 6831.3701\n",
      "Epoch 185/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6676.9536 - val_loss: 7028.4243\n",
      "Epoch 186/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6737.2568 - val_loss: 7257.7476\n",
      "Epoch 187/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6600.3257 - val_loss: 6846.6792\n",
      "Epoch 188/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6777.0142 - val_loss: 6848.1636\n",
      "Epoch 189/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6606.1660 - val_loss: 6884.6138\n",
      "Epoch 190/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6797.6211 - val_loss: 6936.8828\n",
      "Epoch 191/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6627.8325 - val_loss: 6666.7734\n",
      "Epoch 192/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6689.9858 - val_loss: 7010.4775\n",
      "Epoch 193/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6691.6514 - val_loss: 6886.1494\n",
      "Epoch 194/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6988.2808 - val_loss: 7106.1221\n",
      "Epoch 195/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6666.1631 - val_loss: 6887.4165\n",
      "Epoch 196/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6660.2642 - val_loss: 6776.8442\n",
      "Epoch 197/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6428.7383 - val_loss: 6702.9570\n",
      "Epoch 198/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6662.4644 - val_loss: 7017.9160\n",
      "Epoch 199/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6453.1426 - val_loss: 6987.9077\n",
      "Epoch 200/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6681.3169 - val_loss: 6852.8125\n",
      "Epoch 201/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6656.8965 - val_loss: 6826.9644\n",
      "Epoch 202/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6575.8838 - val_loss: 7018.6602\n",
      "Epoch 203/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6678.6587 - val_loss: 6799.1455\n",
      "Epoch 204/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6612.1406 - val_loss: 6736.3677\n",
      "Epoch 205/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6791.9199 - val_loss: 7236.8511\n",
      "Epoch 206/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6707.6938 - val_loss: 6881.5312\n",
      "Epoch 207/300\n",
      "77/79 [============================>.] - ETA: 0s - loss: 6683.6851\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6731.0156 - val_loss: 6926.5171\n",
      "Epoch 208/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6395.9780 - val_loss: 6860.2554\n",
      "Epoch 209/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6519.5166 - val_loss: 7236.5684\n",
      "Epoch 210/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6491.3618 - val_loss: 6751.9204\n",
      "Epoch 211/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6471.7275 - val_loss: 6886.6064\n",
      "Epoch 212/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6400.1289 - val_loss: 6867.9712\n",
      "Epoch 213/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6536.6499 - val_loss: 7005.0869\n",
      "Epoch 214/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6514.1582 - val_loss: 6753.2236\n",
      "Epoch 215/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6259.9014 - val_loss: 6873.5107\n",
      "Epoch 216/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6430.4482 - val_loss: 6945.1885\n",
      "Epoch 217/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6523.9634 - val_loss: 6824.6436\n",
      "Epoch 218/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6606.1113 - val_loss: 6647.9424\n",
      "Epoch 219/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6375.9165 - val_loss: 6905.4243\n",
      "Epoch 220/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6394.8706 - val_loss: 6820.5967\n",
      "Epoch 221/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6443.9946 - val_loss: 6854.4326\n",
      "Epoch 222/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6409.3784 - val_loss: 6696.4219\n",
      "Epoch 223/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6350.0840 - val_loss: 6697.3853\n",
      "Epoch 224/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6377.7676 - val_loss: 6770.6279\n",
      "Epoch 225/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6320.7363 - val_loss: 7076.6304\n",
      "Epoch 226/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6388.7764 - val_loss: 6769.9165\n",
      "Epoch 227/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6290.9258 - val_loss: 6693.2212\n",
      "Epoch 228/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6337.6533 - val_loss: 6791.5679\n",
      "Epoch 229/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6363.9570 - val_loss: 6939.9961\n",
      "Epoch 230/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6398.8984 - val_loss: 6721.8794\n",
      "Epoch 231/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6356.4961 - val_loss: 6809.5386\n",
      "Epoch 232/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6276.5098 - val_loss: 6696.8848\n",
      "Epoch 233/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6375.7466 - val_loss: 6788.3853\n",
      "Epoch 234/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6661.4570 - val_loss: 7011.1367\n",
      "Epoch 235/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6496.1919 - val_loss: 6806.4653\n",
      "Epoch 236/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6413.9526 - val_loss: 6752.2847\n",
      "Epoch 237/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6398.8716 - val_loss: 6775.7563\n",
      "Epoch 238/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6314.4990 - val_loss: 6971.8037\n",
      "Epoch 239/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6283.6074 - val_loss: 6666.1860\n",
      "Epoch 240/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6527.3657 - val_loss: 6765.8960\n",
      "Epoch 241/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6333.7822 - val_loss: 6795.2910\n",
      "Epoch 242/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6286.2520 - val_loss: 6821.4971\n",
      "Epoch 243/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6269.3169 - val_loss: 6895.7939\n",
      "Epoch 244/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6217.6592 - val_loss: 6764.9272\n",
      "Epoch 245/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6413.2612 - val_loss: 6838.5054\n",
      "Epoch 246/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6202.6094 - val_loss: 6738.0801\n",
      "Epoch 247/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6271.0386 - val_loss: 6753.2964\n",
      "Epoch 248/300\n",
      "70/79 [=========================>....] - ETA: 0s - loss: 6155.7979\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 0.004095999523997307.\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6179.5093 - val_loss: 6764.1372\n",
      "Epoch 249/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6416.5981 - val_loss: 6827.6362\n",
      "Epoch 250/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6236.8975 - val_loss: 6922.0698\n",
      "Epoch 251/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6100.7222 - val_loss: 6753.4805\n",
      "Epoch 252/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6209.9019 - val_loss: 6755.5078\n",
      "Epoch 253/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6351.3804 - val_loss: 6747.9727\n",
      "Epoch 254/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6132.5308 - val_loss: 6715.5986\n",
      "Epoch 255/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6183.9775 - val_loss: 6811.7803\n",
      "Epoch 256/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6315.3945 - val_loss: 6650.1538\n",
      "Epoch 257/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6229.3511 - val_loss: 6764.5884\n",
      "Epoch 258/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6091.7280 - val_loss: 6612.2500\n",
      "Epoch 259/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 6117.9517 - val_loss: 6605.6367\n",
      "Epoch 260/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6117.0459 - val_loss: 6648.2480\n",
      "Epoch 261/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6138.0674 - val_loss: 6674.4590\n",
      "Epoch 262/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6283.0010 - val_loss: 6894.9717\n",
      "Epoch 263/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6222.1055 - val_loss: 6492.7412\n",
      "Epoch 264/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6116.9570 - val_loss: 6669.1455\n",
      "Epoch 265/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6024.3320 - val_loss: 6570.4717\n",
      "Epoch 266/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6051.1826 - val_loss: 6630.2832\n",
      "Epoch 267/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6151.9497 - val_loss: 6707.6807\n",
      "Epoch 268/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6259.1719 - val_loss: 6798.8726\n",
      "Epoch 269/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6179.0913 - val_loss: 6759.1436\n",
      "Epoch 270/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6169.0098 - val_loss: 6727.4829\n",
      "Epoch 271/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6066.9575 - val_loss: 6922.3755\n",
      "Epoch 272/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6067.0288 - val_loss: 6940.3916\n",
      "Epoch 273/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6055.0049 - val_loss: 6591.6611\n",
      "Epoch 274/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6075.6992 - val_loss: 6542.6968\n",
      "Epoch 275/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6063.5874 - val_loss: 6808.3755\n",
      "Epoch 276/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6064.5371 - val_loss: 6560.1367\n",
      "Epoch 277/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6035.0820 - val_loss: 6543.2192\n",
      "Epoch 278/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6047.5024 - val_loss: 6464.1553\n",
      "Epoch 279/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6069.6875 - val_loss: 6627.1230\n",
      "Epoch 280/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6164.1191 - val_loss: 6726.5938\n",
      "Epoch 281/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6350.6050 - val_loss: 6634.8994\n",
      "Epoch 282/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6144.4014 - val_loss: 6718.7939\n",
      "Epoch 283/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6060.4019 - val_loss: 6647.1792\n",
      "Epoch 284/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6151.4814 - val_loss: 6586.8394\n",
      "Epoch 285/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 5972.4160 - val_loss: 6752.8032\n",
      "Epoch 286/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6040.3481 - val_loss: 6621.8892\n",
      "Epoch 287/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6011.4668 - val_loss: 6645.7290\n",
      "Epoch 288/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 5882.4995 - val_loss: 6619.5923\n",
      "Epoch 289/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6074.4985 - val_loss: 6884.4731\n",
      "Epoch 290/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6083.9478 - val_loss: 6477.2261\n",
      "Epoch 291/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 5946.0190 - val_loss: 6894.5605\n",
      "Epoch 292/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6021.9751 - val_loss: 6860.2339\n",
      "Epoch 293/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 5913.2295 - val_loss: 6640.3867\n",
      "Epoch 294/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6055.0522 - val_loss: 6561.7349\n",
      "Epoch 295/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6148.6523 - val_loss: 6732.9390\n",
      "Epoch 296/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6006.2690 - val_loss: 6677.0459\n",
      "Epoch 297/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6109.7690 - val_loss: 6629.9424\n",
      "Epoch 298/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6055.8057 - val_loss: 6552.8896\n",
      "Epoch 299/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 6009.1118 - val_loss: 6659.1187\n",
      "Epoch 300/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6161.3745 - val_loss: 6670.2544\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "variational_list = []\n",
    "while i<5:\n",
    "    MICS_model = get_MICS_model(size_inp_a, size_inp_b, size_inp_c, use_encoders = True, drop_out = 0.25)\n",
    "    callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "            keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                             verbose = 2, mode = \"auto\", \n",
    "                                              min_lr = 1e-6)]\n",
    "    MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "    history = MICS_model.fit(x = [group_A_train_x_op.values, group_B_train_x_op.values, group_C_train_x_op.values], y = trainy.values,  \n",
    "                             validation_data = ([group_A_test_x_op.values, group_B_test_x_op.values, group_C_test_x_op.values], testy.values),\n",
    "                             epochs=300, batch_size = 200, callbacks=callback)\n",
    "    training_val_loss = history.history[\"val_loss\"]\n",
    "    best_row_index = np.argmin(training_val_loss)\n",
    "    best_val_loss = training_val_loss[best_row_index]\n",
    "    variational_list.append(best_val_loss)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa5928fcd30>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6598.58095703125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(variational_list)/len(variational_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
