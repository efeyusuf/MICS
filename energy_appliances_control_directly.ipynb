{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4666111131571035016\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10394785719322287102\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n",
      "2.3.1\n",
      "Num GPUs Available:  0\n",
      "WARNING:tensorflow:From <ipython-input-2-b5bb3d29fd0a>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_gpu_available()\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./Datasets/energydata_complete.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.500000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.600000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.700000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.800000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-11 17:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.026667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>6.016667</td>\n",
       "      <td>734.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>43.833333</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>44.919484</td>\n",
       "      <td>44.919484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-11 18:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>45.766667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>734.100000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>47.233763</td>\n",
       "      <td>47.233763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-11 18:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>734.166667</td>\n",
       "      <td>91.833333</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>33.039890</td>\n",
       "      <td>33.039890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-11 18:20:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.597500</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.433333</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>734.233333</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>31.455702</td>\n",
       "      <td>31.455702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-11 18:30:00</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>46.090000</td>\n",
       "      <td>19.23</td>\n",
       "      <td>44.400000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.863333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>734.300000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>3.089314</td>\n",
       "      <td>3.089314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Appliances  lights         T1       RH_1     T2  \\\n",
       "0  2016-01-11 17:00:00          60      30  19.890000  47.596667  19.20   \n",
       "1  2016-01-11 17:10:00          60      30  19.890000  46.693333  19.20   \n",
       "2  2016-01-11 17:20:00          50      30  19.890000  46.300000  19.20   \n",
       "3  2016-01-11 17:30:00          50      40  19.890000  46.066667  19.20   \n",
       "4  2016-01-11 17:40:00          60      40  19.890000  46.333333  19.20   \n",
       "5  2016-01-11 17:50:00          50      40  19.890000  46.026667  19.20   \n",
       "6  2016-01-11 18:00:00          60      50  19.890000  45.766667  19.20   \n",
       "7  2016-01-11 18:10:00          60      50  19.856667  45.560000  19.20   \n",
       "8  2016-01-11 18:20:00          60      40  19.790000  45.597500  19.20   \n",
       "9  2016-01-11 18:30:00          70      40  19.856667  46.090000  19.23   \n",
       "\n",
       "        RH_2     T3       RH_3         T4  ...         T9   RH_9     T_out  \\\n",
       "0  44.790000  19.79  44.730000  19.000000  ...  17.033333  45.53  6.600000   \n",
       "1  44.722500  19.79  44.790000  19.000000  ...  17.066667  45.56  6.483333   \n",
       "2  44.626667  19.79  44.933333  18.926667  ...  17.000000  45.50  6.366667   \n",
       "3  44.590000  19.79  45.000000  18.890000  ...  17.000000  45.40  6.250000   \n",
       "4  44.530000  19.79  45.000000  18.890000  ...  17.000000  45.40  6.133333   \n",
       "5  44.500000  19.79  44.933333  18.890000  ...  17.000000  45.29  6.016667   \n",
       "6  44.500000  19.79  44.900000  18.890000  ...  17.000000  45.29  5.900000   \n",
       "7  44.500000  19.73  44.900000  18.890000  ...  17.000000  45.29  5.916667   \n",
       "8  44.433333  19.73  44.790000  18.890000  ...  17.000000  45.29  5.933333   \n",
       "9  44.400000  19.79  44.863333  18.890000  ...  17.000000  45.29  5.950000   \n",
       "\n",
       "   Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint        rv1  \\\n",
       "0   733.500000  92.000000   7.000000   63.000000   5.300000  13.275433   \n",
       "1   733.600000  92.000000   6.666667   59.166667   5.200000  18.606195   \n",
       "2   733.700000  92.000000   6.333333   55.333333   5.100000  28.642668   \n",
       "3   733.800000  92.000000   6.000000   51.500000   5.000000  45.410389   \n",
       "4   733.900000  92.000000   5.666667   47.666667   4.900000  10.084097   \n",
       "5   734.000000  92.000000   5.333333   43.833333   4.800000  44.919484   \n",
       "6   734.100000  92.000000   5.000000   40.000000   4.700000  47.233763   \n",
       "7   734.166667  91.833333   5.166667   40.000000   4.683333  33.039890   \n",
       "8   734.233333  91.666667   5.333333   40.000000   4.666667  31.455702   \n",
       "9   734.300000  91.500000   5.500000   40.000000   4.650000   3.089314   \n",
       "\n",
       "         rv2  \n",
       "0  13.275433  \n",
       "1  18.606195  \n",
       "2  28.642668  \n",
       "3  45.410389  \n",
       "4  10.084097  \n",
       "5  44.919484  \n",
       "6  47.233763  \n",
       "7  33.039890  \n",
       "8  31.455702  \n",
       "9   3.089314  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=None)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'Appliances'}>]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(column=\"Appliances\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 28 and number of rows is: 19735\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num-2)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num-2)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 8, 9, 10, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def create_rand_list(max_val, count):\n",
    "    randomlist = random.sample(range(0, max_val + 1), count)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"relu\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = create_rand_list(24, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_rand = trainx_scaled.iloc[:,rand_list]\n",
    "test_x_rand = textx_scaled.iloc[:,rand_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = len(test_x_rand.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_losses = {12:[], 14:[], 16:[], 18:[], 20:[], 22:[], 24:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12861.0801 - val_loss: 9071.1396\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10378.6143 - val_loss: 8761.9131\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10198.3516 - val_loss: 8605.9541\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10070.1631 - val_loss: 8691.7500\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9918.5449 - val_loss: 8615.9971\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9812.3535 - val_loss: 8873.6758\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9851.2334 - val_loss: 8575.6230\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9830.8213 - val_loss: 8567.0088\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9769.2314 - val_loss: 8597.9980\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9661.5254 - val_loss: 8421.7549\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9644.4951 - val_loss: 8355.0234\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9627.8193 - val_loss: 8625.8877\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9621.5723 - val_loss: 8334.8604\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9612.5029 - val_loss: 8561.0527\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9549.3984 - val_loss: 8830.2510\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9562.5244 - val_loss: 8398.9473\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9601.8828 - val_loss: 8349.9219\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9519.3896 - val_loss: 8352.6670\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9487.4834 - val_loss: 8372.7227\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9492.4424 - val_loss: 8412.7822\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9374.2285 - val_loss: 8294.7295\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9400.6064 - val_loss: 8771.2021\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9521.7021 - val_loss: 8790.7803\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9472.5430 - val_loss: 8307.2627\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9326.5732 - val_loss: 8625.9365\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9262.6084 - val_loss: 8754.8252\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9602.0684 - val_loss: 8628.0527\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9369.2383 - val_loss: 8416.3037\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9408.6787 - val_loss: 8238.2959\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9343.4629 - val_loss: 8198.8652\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9427.8965 - val_loss: 8520.9883\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9381.7617 - val_loss: 8559.6338\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9338.8721 - val_loss: 8676.9766\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9175.7852 - val_loss: 8337.3037\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9398.7002 - val_loss: 9112.4443\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9368.5293 - val_loss: 8650.0029\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9336.4346 - val_loss: 8260.8730\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9304.1836 - val_loss: 8212.2432\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9278.4512 - val_loss: 8479.9258\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9183.5605 - val_loss: 8359.9316\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9204.0898 - val_loss: 8326.4736\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9333.1338 - val_loss: 8388.4326\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9186.2139 - val_loss: 8378.4561\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9177.6367 - val_loss: 8388.1699\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9156.3438 - val_loss: 8544.2314\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9124.0303 - val_loss: 8209.2676\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9209.4199 - val_loss: 8368.5293\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9095.7393 - val_loss: 8689.5400\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9272.4824 - val_loss: 8639.2031\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9134.8398 - val_loss: 8684.4219\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9386.4971 - val_loss: 8512.7344\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9127.2676 - val_loss: 8359.6934\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9111.3330 - val_loss: 8380.9014\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9133.6104 - val_loss: 8622.6123\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9034.9160 - val_loss: 8284.5088\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9090.2539 - val_loss: 8511.3662\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9151.0674 - val_loss: 8391.8486\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9077.2334 - val_loss: 8273.6494\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9073.5869 - val_loss: 8222.7217\n",
      "Epoch 60/300\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 9127.7109\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9121.3516 - val_loss: 8558.9580\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9032.1025 - val_loss: 8569.6367\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8914.3545 - val_loss: 8215.9658\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8984.9893 - val_loss: 8280.4385\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8940.3984 - val_loss: 8521.0166\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8917.0078 - val_loss: 8260.6094\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9023.1865 - val_loss: 8463.5664\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8942.6094 - val_loss: 8247.3340\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9048.7891 - val_loss: 8230.9912\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8939.9619 - val_loss: 8246.4492\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8971.0293 - val_loss: 8321.8086\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8894.6670 - val_loss: 8295.2207\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8923.6797 - val_loss: 8536.5742\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8883.3994 - val_loss: 8299.8721\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8858.1504 - val_loss: 8513.7705\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8968.0635 - val_loss: 8397.3252\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8839.1699 - val_loss: 8441.2119\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8883.4648 - val_loss: 8238.4092\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8976.7305 - val_loss: 8809.2412\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8774.6123 - val_loss: 8186.3052\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8829.7812 - val_loss: 8339.8652\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8822.2188 - val_loss: 8536.1445\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8923.2822 - val_loss: 8250.8965\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8845.8477 - val_loss: 8037.2041\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9054.6055 - val_loss: 8016.5947\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8969.4824 - val_loss: 8513.7480\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8828.0322 - val_loss: 8435.4873\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8935.1729 - val_loss: 8350.5186\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8810.6924 - val_loss: 8386.9707\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8771.2568 - val_loss: 8367.9180\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8806.3896 - val_loss: 8245.7422\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8802.3926 - val_loss: 8479.2158\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8860.1562 - val_loss: 8452.3652\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8820.0117 - val_loss: 8599.7500\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8856.4639 - val_loss: 8983.4639\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9001.0625 - val_loss: 8288.5078\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8758.4268 - val_loss: 8267.9033\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8932.8398 - val_loss: 8258.5918\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8782.3369 - val_loss: 8351.3643\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8716.6953 - val_loss: 8246.5869\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8834.4697 - val_loss: 8403.0986\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8864.0996 - val_loss: 8425.6387\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8721.3008 - val_loss: 8248.9844\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8790.7500 - val_loss: 8324.1416\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8804.0020 - val_loss: 8308.4727\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8738.9385 - val_loss: 8233.0020\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8800.4473 - val_loss: 8120.9170\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8753.4922 - val_loss: 8252.0098\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8630.1084 - val_loss: 8135.5586\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8778.1240 - val_loss: 8257.4287\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8882.5479 - val_loss: 8160.1753\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8780.3008 - val_loss: 8202.0596\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8646.9199 - val_loss: 8218.9199\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8795.0586 - val_loss: 8221.5449\n",
      "Epoch 114/300\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 8792.5615\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8736.3145 - val_loss: 8214.7432\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8622.8340 - val_loss: 8620.5283\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8686.7676 - val_loss: 8283.9619\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8843.6816 - val_loss: 8476.7471\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8712.5215 - val_loss: 8375.3740\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8578.6045 - val_loss: 8357.4141\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8740.7773 - val_loss: 8093.0503\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8570.5273 - val_loss: 8614.0918\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8572.0166 - val_loss: 8494.4766\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8536.6416 - val_loss: 8155.8662\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8747.9590 - val_loss: 8395.8135\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8660.8584 - val_loss: 8339.6885\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8615.3379 - val_loss: 8236.7178\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8683.8926 - val_loss: 8133.6191\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8563.0723 - val_loss: 8151.5864\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8605.7227 - val_loss: 8389.4307\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8655.9209 - val_loss: 8466.2803\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8399.2695 - val_loss: 8651.4189\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8649.6338 - val_loss: 8225.7754\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8604.4307 - val_loss: 8371.0039\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8701.5908 - val_loss: 8349.5195\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12776.6699 - val_loss: 9277.4062\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10430.8027 - val_loss: 8704.9766\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10102.7969 - val_loss: 8608.4043\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9984.1523 - val_loss: 8341.8164\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9748.1904 - val_loss: 8450.0176\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9773.9951 - val_loss: 8717.2002\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9599.2656 - val_loss: 8284.2559\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9534.9570 - val_loss: 8387.0850\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9480.3652 - val_loss: 8341.4277\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9464.8682 - val_loss: 8219.7490\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9338.4229 - val_loss: 8130.1787\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9324.2500 - val_loss: 8074.3774\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9266.3955 - val_loss: 8032.3550\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9315.3984 - val_loss: 8033.8423\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9416.2432 - val_loss: 8071.9893\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9312.1650 - val_loss: 8144.9590\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9341.9297 - val_loss: 8066.1641\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9168.9932 - val_loss: 8096.9116\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9204.3545 - val_loss: 8286.8896\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9198.0322 - val_loss: 7988.7417\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9145.1660 - val_loss: 7942.6074\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9059.8340 - val_loss: 7994.8457\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9165.6719 - val_loss: 7952.6733\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9119.4150 - val_loss: 7963.8394\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9075.0801 - val_loss: 8044.2637\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8985.8799 - val_loss: 7927.1636\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9123.3350 - val_loss: 7978.5249\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9046.0771 - val_loss: 7776.6816\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8909.8105 - val_loss: 7924.1592\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9063.3232 - val_loss: 7880.5190\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8850.9434 - val_loss: 8109.9517\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8958.3252 - val_loss: 7845.8516\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8946.8975 - val_loss: 7882.5601\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8783.6133 - val_loss: 7791.3926\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8994.7129 - val_loss: 7892.6416\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9016.4443 - val_loss: 7957.3267\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8859.8438 - val_loss: 8255.9141\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8851.3672 - val_loss: 7991.3599\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8847.2998 - val_loss: 7849.3721\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8799.4424 - val_loss: 7978.0493\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8775.6016 - val_loss: 7903.7368\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8794.6279 - val_loss: 7755.9688\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8742.3291 - val_loss: 8322.0391\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8869.1885 - val_loss: 8055.7881\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8840.8086 - val_loss: 8173.5669\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8880.6631 - val_loss: 7621.8599\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8827.0869 - val_loss: 7655.3276\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8799.8369 - val_loss: 8256.9150\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8734.6768 - val_loss: 7735.6880\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8732.6562 - val_loss: 7726.0742\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8602.1270 - val_loss: 7625.2759\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8693.2979 - val_loss: 7772.8662\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8676.1523 - val_loss: 7524.0107\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8651.7734 - val_loss: 7806.4595\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8676.2979 - val_loss: 7994.8604\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8775.9199 - val_loss: 7862.2290\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8701.5078 - val_loss: 7817.8711\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8625.1748 - val_loss: 8027.3511\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8508.0283 - val_loss: 7664.0967\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8608.3232 - val_loss: 7576.3516\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8794.1328 - val_loss: 7638.5454\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8811.4922 - val_loss: 8317.9463\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8553.4834 - val_loss: 7633.0220\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8865.6816 - val_loss: 7783.1377\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8637.9424 - val_loss: 7681.9756\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8633.6025 - val_loss: 8067.2207\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8604.4111 - val_loss: 7873.3706\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8554.3779 - val_loss: 7620.5425\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8601.8574 - val_loss: 7585.7520\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8589.8271 - val_loss: 7996.8301\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8660.1494 - val_loss: 7555.9619\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8683.1670 - val_loss: 7788.0210\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8619.2510 - val_loss: 8096.3110\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8573.7217 - val_loss: 7856.3955\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8409.7627 - val_loss: 7997.5317\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8486.3027 - val_loss: 7998.7676\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8535.9756 - val_loss: 7701.7407\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8497.3291 - val_loss: 8442.7627\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8480.1807 - val_loss: 7695.0635\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8467.6699 - val_loss: 7660.4443\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8466.2842 - val_loss: 7442.8320\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8350.6035 - val_loss: 8134.3589\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8678.6611 - val_loss: 8405.2627\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8606.0352 - val_loss: 8195.0566\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8499.1846 - val_loss: 7691.4761\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8424.6875 - val_loss: 8157.4048\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8436.4990 - val_loss: 7740.4951\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8474.6836 - val_loss: 7791.8364\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8504.2109 - val_loss: 7847.4629\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8373.7646 - val_loss: 7745.9292\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8346.5625 - val_loss: 7808.3701\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8293.0430 - val_loss: 7708.0376\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8301.2939 - val_loss: 7587.8433\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8492.8105 - val_loss: 7624.0835\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8378.8252 - val_loss: 7618.8765\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8612.8447 - val_loss: 7510.2910\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8420.0557 - val_loss: 7866.6406\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8375.5205 - val_loss: 7917.3848\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8376.0283 - val_loss: 7718.6309\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8569.4834 - val_loss: 7737.7593\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8406.0996 - val_loss: 7598.1050\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8526.2344 - val_loss: 7455.0635\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8316.7988 - val_loss: 7619.3291\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8245.7334 - val_loss: 7487.3965\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8401.3604 - val_loss: 7508.2749\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8369.7773 - val_loss: 7488.9966\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8279.0068 - val_loss: 7823.7354\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8447.0254 - val_loss: 7931.6147\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8328.9707 - val_loss: 7377.8335\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8306.5000 - val_loss: 7872.0049\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8481.6133 - val_loss: 8235.7910\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8583.9180 - val_loss: 7820.2876\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8352.9512 - val_loss: 7428.1470\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8227.5322 - val_loss: 8013.0830\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8463.5820 - val_loss: 7624.6689\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8374.1035 - val_loss: 7755.9746\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8232.7734 - val_loss: 7560.2373\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8153.5342 - val_loss: 7859.0913\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8290.5186 - val_loss: 7527.9019\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8262.6904 - val_loss: 7554.8193\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8222.7148 - val_loss: 7270.0767\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8259.1602 - val_loss: 7613.8872\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8315.0303 - val_loss: 8000.2104\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8311.7227 - val_loss: 7726.2588\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8183.1714 - val_loss: 8078.7925\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8432.3301 - val_loss: 7790.0703\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8189.2319 - val_loss: 7995.9648\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8418.9551 - val_loss: 7772.4312\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8322.4111 - val_loss: 7732.8628\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8278.2354 - val_loss: 7355.2578\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8262.5488 - val_loss: 7872.8057\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8296.4805 - val_loss: 7640.4253\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8241.5684 - val_loss: 8169.2651\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8395.7236 - val_loss: 7825.5293\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8362.6318 - val_loss: 7572.5386\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8194.6641 - val_loss: 7695.8179\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8400.7334 - val_loss: 7419.0991\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8272.2900 - val_loss: 7711.3691\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8138.5615 - val_loss: 7598.9746\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8212.3965 - val_loss: 7568.2710\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8248.9287 - val_loss: 7813.9336\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8130.4722 - val_loss: 7561.6626\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8170.5156 - val_loss: 7697.7061\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8321.0244 - val_loss: 7390.4346\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8232.0322 - val_loss: 8262.6309\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8212.9307 - val_loss: 7484.7031\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8186.8281 - val_loss: 7594.0474\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8220.1816 - val_loss: 8424.7051\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8184.7949 - val_loss: 7807.7251\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8205.3545 - val_loss: 7582.9858\n",
      "Epoch 151/300\n",
      "27/53 [==============>...............] - ETA: 0s - loss: 8137.2129\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8111.7324 - val_loss: 7894.7354\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8105.4180 - val_loss: 7840.7290\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7996.5952 - val_loss: 7582.3677\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8213.8018 - val_loss: 7909.9575\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8136.8970 - val_loss: 7671.5122\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8135.8594 - val_loss: 7565.7983\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7959.5859 - val_loss: 7442.1958\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8262.3818 - val_loss: 7569.4087\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8222.4014 - val_loss: 7585.1006\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8114.7798 - val_loss: 7738.1499\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7923.0430 - val_loss: 7553.8799\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8105.1846 - val_loss: 7871.1904\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8072.0659 - val_loss: 7675.6597\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8058.4795 - val_loss: 7702.3477\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8086.7080 - val_loss: 7835.0508\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8080.4038 - val_loss: 7502.5864\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8093.6758 - val_loss: 7539.6797\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8121.1943 - val_loss: 7759.1802\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8019.1699 - val_loss: 7583.8970\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8097.2295 - val_loss: 7758.3994\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8100.7236 - val_loss: 7406.9248\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 13076.3271 - val_loss: 9040.7354\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10598.8896 - val_loss: 8692.3838\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10301.6367 - val_loss: 9075.4648\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10287.2461 - val_loss: 8618.4844\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10028.6670 - val_loss: 8425.6807\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9983.7812 - val_loss: 8718.3057\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9791.3115 - val_loss: 8416.1904\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9851.6650 - val_loss: 8309.0254\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9865.7568 - val_loss: 8735.0166\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9809.6406 - val_loss: 8292.9053\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9645.7754 - val_loss: 8588.4951\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9596.5732 - val_loss: 8192.4248\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9676.3564 - val_loss: 8893.4141\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9607.0010 - val_loss: 8185.9087\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9511.5000 - val_loss: 8104.6372\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9439.5215 - val_loss: 8095.7905\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9519.2598 - val_loss: 8377.3896\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9523.1201 - val_loss: 8117.7046\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9443.7578 - val_loss: 8286.7168\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9533.9316 - val_loss: 8295.0547\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9475.5547 - val_loss: 8052.5508\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9479.3750 - val_loss: 8180.9131\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9420.8662 - val_loss: 8083.7124\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9403.2607 - val_loss: 8199.9092\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9353.6406 - val_loss: 8146.9766\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9272.0996 - val_loss: 8267.3271\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9181.2139 - val_loss: 8145.2036\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9305.6650 - val_loss: 8436.5117\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9232.7803 - val_loss: 7929.8369\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9335.5693 - val_loss: 8280.8730\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9259.2314 - val_loss: 8193.7480\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9266.3096 - val_loss: 7985.3462\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9183.2588 - val_loss: 8413.0605\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9287.3867 - val_loss: 7915.2583\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9156.7061 - val_loss: 8157.4263\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9133.4736 - val_loss: 8374.0498\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9073.3252 - val_loss: 8146.5142\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9171.6426 - val_loss: 8036.8159\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9310.6514 - val_loss: 7978.3569\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9025.7207 - val_loss: 8300.5088\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9202.9043 - val_loss: 7907.3867\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9167.7744 - val_loss: 8451.4326\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9097.2383 - val_loss: 8242.0020\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9020.2324 - val_loss: 8028.3159\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9116.8252 - val_loss: 8159.3306\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9168.9629 - val_loss: 7876.5796\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9173.2461 - val_loss: 7842.1357\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8913.5615 - val_loss: 8071.4551\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9036.3408 - val_loss: 8202.5303\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8937.7383 - val_loss: 7925.5615\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8883.3564 - val_loss: 8051.7534\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8997.9170 - val_loss: 7998.3989\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9015.1299 - val_loss: 8111.3691\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8947.0811 - val_loss: 8145.5000\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8861.0469 - val_loss: 8025.5796\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8943.7393 - val_loss: 8256.7998\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8918.2490 - val_loss: 8071.5781\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8897.3770 - val_loss: 7894.3701\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8980.6201 - val_loss: 8173.8555\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8928.1387 - val_loss: 8479.5996\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8988.1387 - val_loss: 8336.5938\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8817.0508 - val_loss: 8195.0566\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8749.9238 - val_loss: 7971.7842\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8920.7070 - val_loss: 7951.5776\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8795.7168 - val_loss: 7876.6592\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8891.6582 - val_loss: 8452.7959\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8877.2812 - val_loss: 8203.5869\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8797.8867 - val_loss: 7982.6470\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8809.7344 - val_loss: 7976.5054\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8872.8271 - val_loss: 7759.9893\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8838.5723 - val_loss: 8164.2026\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8716.3916 - val_loss: 7749.3140\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8801.2461 - val_loss: 8320.8574\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8707.7627 - val_loss: 7778.8384\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8707.0957 - val_loss: 7895.4634\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8880.2871 - val_loss: 7725.8408\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8732.5557 - val_loss: 8036.0767\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8780.5693 - val_loss: 7811.2261\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8684.2236 - val_loss: 7978.0664\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8588.9883 - val_loss: 8283.2217\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8704.3213 - val_loss: 7718.7627\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8739.4619 - val_loss: 8083.8535\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8743.5459 - val_loss: 7880.8594\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8744.1055 - val_loss: 7703.6816\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8747.2686 - val_loss: 8199.3936\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8660.2734 - val_loss: 8106.2383\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8541.7012 - val_loss: 7973.7954\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8813.7695 - val_loss: 8309.2539\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8773.7266 - val_loss: 8066.3535\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8717.1562 - val_loss: 7730.0278\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8768.4951 - val_loss: 8367.4287\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8642.3311 - val_loss: 7969.2021\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8586.9150 - val_loss: 8115.1641\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8611.7568 - val_loss: 8133.3911\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8683.7207 - val_loss: 8183.5474\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8767.6748 - val_loss: 7984.8799\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8687.7285 - val_loss: 8615.6172\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8727.3965 - val_loss: 8316.3965\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8540.8291 - val_loss: 8077.6001\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8644.6104 - val_loss: 7773.6479\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8587.8203 - val_loss: 8372.9854\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8656.3672 - val_loss: 7962.3281\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8656.2100 - val_loss: 8193.3066\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8712.2305 - val_loss: 8169.0723\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8575.5195 - val_loss: 7817.0024\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8537.8311 - val_loss: 8187.5854\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8614.7178 - val_loss: 8192.7275\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8611.7285 - val_loss: 7877.6646\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8632.6631 - val_loss: 8047.1934\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8390.3975 - val_loss: 7769.3457\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8410.3604 - val_loss: 8053.9199\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8564.9551 - val_loss: 7736.7104\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8665.7959 - val_loss: 7944.1821\n",
      "Epoch 114/300\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 8486.5576\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8372.0283 - val_loss: 7931.1011\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8506.3252 - val_loss: 8062.5278\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8477.3896 - val_loss: 8069.2188\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8487.2246 - val_loss: 7622.5044\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8424.8105 - val_loss: 8045.3901\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8544.2422 - val_loss: 7965.6528\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8386.9512 - val_loss: 8020.5288\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8346.7314 - val_loss: 7903.9634\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8488.1270 - val_loss: 7815.4077\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8397.1025 - val_loss: 8144.0439\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8420.6094 - val_loss: 7730.2544\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8423.4727 - val_loss: 8082.5029\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8333.4961 - val_loss: 7630.0747\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8521.8623 - val_loss: 7837.3643\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - ETA: 0s - loss: 8663.533 - 0s 2ms/step - loss: 8387.4092 - val_loss: 7843.7617\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8340.7695 - val_loss: 7858.4697\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8330.2500 - val_loss: 8027.1616\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8321.4834 - val_loss: 7839.3345\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8472.8457 - val_loss: 7981.4209\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8488.7764 - val_loss: 7890.4941\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8205.5049 - val_loss: 7808.8193\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8415.4375 - val_loss: 7920.0986\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8391.5518 - val_loss: 7809.2754\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8340.7324 - val_loss: 8111.9028\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8355.4443 - val_loss: 7841.0107\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8422.2676 - val_loss: 7898.0913\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8388.3711 - val_loss: 8250.9854\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8388.1387 - val_loss: 7776.9458\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8301.4639 - val_loss: 7647.2349\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8332.9580 - val_loss: 7841.7119\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8623.1914 - val_loss: 8271.8623\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8335.2783 - val_loss: 7805.3853\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8326.8301 - val_loss: 7800.5298\n",
      "Epoch 147/300\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 8435.4189\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8341.2480 - val_loss: 7903.0088\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8444.8496 - val_loss: 7750.8125\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8296.0732 - val_loss: 7955.6558\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8324.4600 - val_loss: 7915.3804\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8326.7656 - val_loss: 8193.3193\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8325.6709 - val_loss: 7656.2700\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8157.5308 - val_loss: 7836.4722\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8220.7529 - val_loss: 7990.5835\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8220.0908 - val_loss: 7818.2446\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8329.3896 - val_loss: 7780.8770\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8201.3213 - val_loss: 7762.7559\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8243.5752 - val_loss: 7887.4326\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8120.9346 - val_loss: 7668.2988\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8268.8203 - val_loss: 7883.5171\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8158.7310 - val_loss: 7887.5288\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8144.4307 - val_loss: 7982.0957\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8175.8047 - val_loss: 7759.6094\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8209.8330 - val_loss: 8017.4253\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8238.5264 - val_loss: 7664.2017\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8167.7661 - val_loss: 7794.8818\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8450.5254 - val_loss: 8063.0610\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 13394.3584 - val_loss: 9863.0234\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10671.7734 - val_loss: 8949.8281\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10288.7705 - val_loss: 8599.0000\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10136.4346 - val_loss: 8433.2012\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10054.6436 - val_loss: 8596.2568\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10039.7393 - val_loss: 8329.0293\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9801.1797 - val_loss: 8327.7314\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9883.7354 - val_loss: 8318.9258\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9657.6973 - val_loss: 8514.0830\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9683.2637 - val_loss: 8206.1582\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9560.8311 - val_loss: 8252.6416\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9568.3945 - val_loss: 8063.0767\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9512.0156 - val_loss: 8016.2651\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9489.6719 - val_loss: 7964.7188\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9438.8809 - val_loss: 8474.2422\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9395.0596 - val_loss: 8230.8672\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9419.9746 - val_loss: 8300.2109\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9264.0518 - val_loss: 8097.4941\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9335.4336 - val_loss: 8197.0342\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9265.8047 - val_loss: 8203.6104\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9265.5781 - val_loss: 7931.6929\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9185.2549 - val_loss: 7965.5312\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9170.0557 - val_loss: 7881.7207\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9274.6777 - val_loss: 7979.7065\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9123.1689 - val_loss: 7899.6128\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9218.4492 - val_loss: 7850.2822\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9150.9219 - val_loss: 7788.2681\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - ETA: 0s - loss: 9067.67 - 0s 3ms/step - loss: 9140.7793 - val_loss: 7896.0942\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9137.7471 - val_loss: 7854.3208\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9140.0098 - val_loss: 7876.8691\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9117.9678 - val_loss: 7950.3228\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9013.6045 - val_loss: 7928.0806\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9014.8398 - val_loss: 7876.4263\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9104.7256 - val_loss: 7726.1299\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9076.3779 - val_loss: 7672.2627\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8945.9863 - val_loss: 8052.7710\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8926.2969 - val_loss: 7718.6787\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8942.1211 - val_loss: 7675.5674\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8934.9590 - val_loss: 7871.6943\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8762.8467 - val_loss: 7757.6631\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8864.3955 - val_loss: 7694.6221\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8768.9004 - val_loss: 7901.4409\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8811.9668 - val_loss: 7941.2534\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8785.0391 - val_loss: 7912.0479\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8860.2305 - val_loss: 8370.8564\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8939.1963 - val_loss: 7672.7363\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8951.2510 - val_loss: 7611.6982\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8757.2920 - val_loss: 7896.6357\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8816.7305 - val_loss: 7946.3579\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8777.7510 - val_loss: 8175.6191\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8807.6416 - val_loss: 7839.6904\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8778.4336 - val_loss: 7792.9028\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8824.6865 - val_loss: 7613.5005\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8655.9238 - val_loss: 7662.1870\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8706.1855 - val_loss: 7495.1387\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8723.5264 - val_loss: 7469.4844\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8858.1533 - val_loss: 7572.8184\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8609.1963 - val_loss: 7468.7939\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8725.0527 - val_loss: 7729.4282\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8779.9502 - val_loss: 7739.8506\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8797.0713 - val_loss: 7874.3281\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8562.0859 - val_loss: 7816.0454\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8664.3262 - val_loss: 7878.0063\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8674.2900 - val_loss: 7854.5933\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8769.7900 - val_loss: 7819.1802\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8526.6533 - val_loss: 7690.3389\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8705.6807 - val_loss: 7755.7539\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8549.8223 - val_loss: 7855.2095\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8592.4902 - val_loss: 8159.9106\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8734.6113 - val_loss: 7971.4814\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8656.8926 - val_loss: 7433.6050\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8484.4443 - val_loss: 7449.4531\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8513.2051 - val_loss: 7509.1465\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8553.8350 - val_loss: 7576.4204\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8412.3350 - val_loss: 7943.0361\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8536.0361 - val_loss: 7559.1787\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8448.5010 - val_loss: 7582.6382\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8553.7666 - val_loss: 7596.6499\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8249.8965 - val_loss: 7671.2812\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8440.6250 - val_loss: 7569.9873\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8491.2471 - val_loss: 7471.4902\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8621.1260 - val_loss: 7893.0459\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8494.8584 - val_loss: 7600.0400\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8357.7861 - val_loss: 7681.6191\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8386.1016 - val_loss: 7620.9897\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8481.4727 - val_loss: 7656.8101\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8362.3066 - val_loss: 7605.1431\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8449.6982 - val_loss: 7480.1348\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8333.2812 - val_loss: 7614.3633\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8554.6113 - val_loss: 7761.5850\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8535.1152 - val_loss: 7646.5474\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8475.4727 - val_loss: 7858.5615\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8462.6719 - val_loss: 7785.0112\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8486.3750 - val_loss: 7519.8921\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8493.2422 - val_loss: 7769.0068\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8398.6992 - val_loss: 7494.6763\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8342.1846 - val_loss: 7718.3081\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8375.0000 - val_loss: 7381.2061\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8393.1963 - val_loss: 7690.3267\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8504.3096 - val_loss: 7379.8687\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8290.7910 - val_loss: 7482.4419\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8472.3525 - val_loss: 7643.8154\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8402.4902 - val_loss: 7557.5010\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8392.9473 - val_loss: 7655.2646\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8300.8555 - val_loss: 7551.2129\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8356.0771 - val_loss: 7521.4326\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8282.2822 - val_loss: 7346.8208\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8387.9639 - val_loss: 7329.5684\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8363.4111 - val_loss: 7608.0903\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8434.1787 - val_loss: 7322.5610\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8275.2998 - val_loss: 7383.4683\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8385.9961 - val_loss: 7684.9502\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8336.0000 - val_loss: 7573.6113\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8352.9131 - val_loss: 7288.4043\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8530.6553 - val_loss: 7658.8916\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8269.5908 - val_loss: 7365.8887\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8300.4365 - val_loss: 7378.1470\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8329.7861 - val_loss: 7871.7739\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8394.1201 - val_loss: 7512.9849\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8280.8867 - val_loss: 7617.3242\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8291.3848 - val_loss: 7404.3140\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8342.0771 - val_loss: 7872.5400\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8419.9512 - val_loss: 7782.5479\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8143.2876 - val_loss: 7381.3267\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8353.2930 - val_loss: 7698.3579\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8348.2305 - val_loss: 7880.5229\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8205.1318 - val_loss: 7335.7876\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8190.4561 - val_loss: 7443.1660\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8264.0449 - val_loss: 7498.5444\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8336.2549 - val_loss: 7616.2563\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8322.4287 - val_loss: 7456.3369\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8230.0527 - val_loss: 7556.1992\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8382.7510 - val_loss: 7631.5059\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8401.2012 - val_loss: 7515.0522\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8134.3369 - val_loss: 7375.9238\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8410.1484 - val_loss: 7365.6890\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8205.4209 - val_loss: 7345.4341\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8275.9902 - val_loss: 7666.3579\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8387.6348 - val_loss: 7480.2266\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8212.6826 - val_loss: 7471.3184\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8219.1777 - val_loss: 7426.2163\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8229.6211 - val_loss: 7448.7788\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8241.2412 - val_loss: 7733.1919\n",
      "Epoch 144/300\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 8291.1475\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8275.2529 - val_loss: 7545.4180\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8122.7065 - val_loss: 7177.4009\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8276.5127 - val_loss: 7690.7373\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8139.9141 - val_loss: 7631.0273\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8235.8857 - val_loss: 7317.7744\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8402.4365 - val_loss: 7766.0854\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8253.6211 - val_loss: 7519.8667\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8060.0225 - val_loss: 7533.4941\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8066.0894 - val_loss: 7483.3076\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8169.6611 - val_loss: 7391.8687\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8009.1328 - val_loss: 7268.5537\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8117.4033 - val_loss: 7225.3076\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8094.6104 - val_loss: 7349.2085\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8083.0664 - val_loss: 7395.5889\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8121.6802 - val_loss: 7407.1572\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8161.3345 - val_loss: 7320.5234\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8058.2891 - val_loss: 7200.7725\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8105.6138 - val_loss: 7787.2700\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8200.7324 - val_loss: 7527.3008\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8244.9023 - val_loss: 7845.7271\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8140.5933 - val_loss: 7168.6187\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8086.1377 - val_loss: 7209.5669\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7984.9048 - val_loss: 7750.4580\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8153.3521 - val_loss: 7432.8540\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8068.7930 - val_loss: 7238.7637\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8060.7476 - val_loss: 7635.0278\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8084.7280 - val_loss: 7727.3950\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8090.9902 - val_loss: 7770.2773\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8088.2231 - val_loss: 7637.8086\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8106.8501 - val_loss: 7377.9092\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8095.1538 - val_loss: 7570.2334\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8075.0972 - val_loss: 7660.6743\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8004.6128 - val_loss: 7492.1821\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8187.7886 - val_loss: 7570.4712\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8080.4424 - val_loss: 7389.5874\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8091.8740 - val_loss: 7412.0684\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8016.2446 - val_loss: 7502.5923\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7960.5483 - val_loss: 7270.4438\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8118.2627 - val_loss: 7292.6592\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8090.0020 - val_loss: 7711.5205\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8190.5605 - val_loss: 7401.3984\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8166.4248 - val_loss: 7298.5293\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8067.2056 - val_loss: 7478.7085\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8125.2095 - val_loss: 7546.2144\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8086.8711 - val_loss: 7396.6899\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8019.0288 - val_loss: 7408.2339\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8043.5210 - val_loss: 7483.1987\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8123.8354 - val_loss: 7232.9766\n",
      "Epoch 192/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7906.3179 - val_loss: 7282.4873\n",
      "Epoch 193/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8019.5835 - val_loss: 7813.9585\n",
      "Epoch 194/300\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 7984.7642\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7996.2891 - val_loss: 7426.4580\n",
      "Epoch 195/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8090.2900 - val_loss: 7507.8408\n",
      "Epoch 196/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7893.7437 - val_loss: 7292.1865\n",
      "Epoch 197/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7956.6807 - val_loss: 7241.9551\n",
      "Epoch 198/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7954.3228 - val_loss: 7121.8125\n",
      "Epoch 199/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7978.2339 - val_loss: 7246.8555\n",
      "Epoch 200/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7922.5913 - val_loss: 7531.8794\n",
      "Epoch 201/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7811.1567 - val_loss: 7336.6064\n",
      "Epoch 202/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7928.5322 - val_loss: 7400.2773\n",
      "Epoch 203/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7880.4946 - val_loss: 7549.1821\n",
      "Epoch 204/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7958.1646 - val_loss: 7212.6343\n",
      "Epoch 205/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7936.6099 - val_loss: 7446.4438\n",
      "Epoch 206/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7859.0396 - val_loss: 7478.7886\n",
      "Epoch 207/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8032.6289 - val_loss: 7518.5474\n",
      "Epoch 208/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7910.7856 - val_loss: 7589.9463\n",
      "Epoch 209/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7962.7148 - val_loss: 7484.4907\n",
      "Epoch 210/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7912.6943 - val_loss: 7685.0239\n",
      "Epoch 211/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7820.5278 - val_loss: 7489.1255\n",
      "Epoch 212/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7996.7144 - val_loss: 7505.1030\n",
      "Epoch 213/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7931.1606 - val_loss: 7677.8037\n",
      "Epoch 214/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7960.1987 - val_loss: 7607.1641\n",
      "Epoch 215/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7768.5156 - val_loss: 7462.0459\n",
      "Epoch 216/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7839.4038 - val_loss: 7261.8618\n",
      "Epoch 217/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7868.8633 - val_loss: 7249.1138\n",
      "Epoch 218/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7904.8184 - val_loss: 7269.0942\n",
      "Epoch 219/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7997.8618 - val_loss: 7209.8447\n",
      "Epoch 220/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7811.9609 - val_loss: 7280.1167\n",
      "Epoch 221/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7923.3208 - val_loss: 7676.7974\n",
      "Epoch 222/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7804.4854 - val_loss: 7061.0498\n",
      "Epoch 223/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7905.6943 - val_loss: 7460.1348\n",
      "Epoch 224/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7924.5068 - val_loss: 7464.1279\n",
      "Epoch 225/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7820.0479 - val_loss: 7332.4438\n",
      "Epoch 226/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7899.4224 - val_loss: 7467.8027\n",
      "Epoch 227/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7963.0244 - val_loss: 7320.7129\n",
      "Epoch 228/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7738.7012 - val_loss: 7404.1514\n",
      "Epoch 229/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7897.8960 - val_loss: 7095.8892\n",
      "Epoch 230/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7785.5073 - val_loss: 7490.8398\n",
      "Epoch 231/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7969.4087 - val_loss: 7470.6929\n",
      "Epoch 232/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7855.7705 - val_loss: 7361.0449\n",
      "Epoch 233/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8014.0771 - val_loss: 7952.4077\n",
      "Epoch 234/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7955.3169 - val_loss: 7292.9888\n",
      "Epoch 235/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7980.1831 - val_loss: 7404.0034\n",
      "Epoch 236/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7893.8281 - val_loss: 7406.7793\n",
      "Epoch 237/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7822.2690 - val_loss: 7509.7744\n",
      "Epoch 238/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7885.4116 - val_loss: 7303.7095\n",
      "Epoch 239/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7896.7354 - val_loss: 7525.2822\n",
      "Epoch 240/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7957.3638 - val_loss: 7461.4185\n",
      "Epoch 241/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7902.6904 - val_loss: 7714.3638\n",
      "Epoch 242/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7778.2202 - val_loss: 7358.0142\n",
      "Epoch 243/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7830.6934 - val_loss: 7510.2632\n",
      "Epoch 244/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7982.3140 - val_loss: 7274.9585\n",
      "Epoch 245/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7906.6958 - val_loss: 7371.9678\n",
      "Epoch 246/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7917.2324 - val_loss: 7430.8799\n",
      "Epoch 247/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7782.2222 - val_loss: 7370.7354\n",
      "Epoch 248/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7952.9160 - val_loss: 7422.6689\n",
      "Epoch 249/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7948.7261 - val_loss: 7343.0762\n",
      "Epoch 250/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7802.0552 - val_loss: 7388.0410\n",
      "Epoch 251/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7879.4980 - val_loss: 7307.5244\n",
      "Epoch 252/300\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 7892.7363\n",
      "Epoch 00252: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7764.5361 - val_loss: 7453.4312\n",
      "Epoch 253/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7772.8330 - val_loss: 7439.1558\n",
      "Epoch 254/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7824.1953 - val_loss: 7267.2891\n",
      "Epoch 255/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7653.1377 - val_loss: 7363.5576\n",
      "Epoch 256/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7845.3101 - val_loss: 7187.9219\n",
      "Epoch 257/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7761.3901 - val_loss: 7666.4800\n",
      "Epoch 258/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7755.8169 - val_loss: 7157.9316\n",
      "Epoch 259/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7730.5811 - val_loss: 7164.8008\n",
      "Epoch 260/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7778.9561 - val_loss: 7201.3848\n",
      "Epoch 261/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7741.5415 - val_loss: 7186.5200\n",
      "Epoch 262/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7616.7334 - val_loss: 7111.7271\n",
      "Epoch 263/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7748.7417 - val_loss: 7209.0610\n",
      "Epoch 264/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7839.6816 - val_loss: 7263.9746\n",
      "Epoch 265/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7875.9297 - val_loss: 7702.3765\n",
      "Epoch 266/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7760.1328 - val_loss: 7360.3521\n",
      "Epoch 267/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7926.0278 - val_loss: 7162.9512\n",
      "Epoch 268/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7812.6475 - val_loss: 7238.4185\n",
      "Epoch 269/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7699.9199 - val_loss: 7220.2891\n",
      "Epoch 270/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7702.2490 - val_loss: 7450.1587\n",
      "Epoch 271/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7664.1577 - val_loss: 7311.8618\n",
      "Epoch 272/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7763.2949 - val_loss: 7349.1069\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 13277.8232 - val_loss: 9469.4326\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10586.2959 - val_loss: 8720.5186\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10278.7666 - val_loss: 8866.6504\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10014.2461 - val_loss: 8584.4668\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9958.0254 - val_loss: 8667.3477\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9888.4121 - val_loss: 8821.7520\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9750.9639 - val_loss: 8515.3857\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9554.4775 - val_loss: 8256.3379\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9644.0693 - val_loss: 8252.5771\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9617.0752 - val_loss: 8464.0107\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9469.3936 - val_loss: 8204.2812\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9539.5264 - val_loss: 8260.8047\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9424.2236 - val_loss: 8292.3252\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9558.8018 - val_loss: 8350.2725\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9372.5908 - val_loss: 8177.2988\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9525.7529 - val_loss: 8355.2939\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9419.4316 - val_loss: 8267.9414\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9387.1113 - val_loss: 8515.5332\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9270.3613 - val_loss: 8248.4854\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9363.5967 - val_loss: 8133.4385\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9311.1719 - val_loss: 8197.8311\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9235.7480 - val_loss: 8284.1709\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9254.2803 - val_loss: 8236.8730\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9194.5117 - val_loss: 8218.1279\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9192.7939 - val_loss: 8197.1650\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9257.5664 - val_loss: 8297.8447\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9213.0488 - val_loss: 8117.2290\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9250.9238 - val_loss: 8122.7729\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9363.5283 - val_loss: 8070.0034\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9303.4131 - val_loss: 8290.5967\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9078.3027 - val_loss: 8206.7520\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9245.3037 - val_loss: 8343.9570\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9141.4639 - val_loss: 8097.1729\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9054.4775 - val_loss: 8038.2017\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9090.3047 - val_loss: 8505.0498\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9063.1406 - val_loss: 8140.2002\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9047.8721 - val_loss: 8226.9746\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9012.8965 - val_loss: 8530.2725\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9059.8320 - val_loss: 8095.0767\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9262.6611 - val_loss: 8136.3589\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8999.0254 - val_loss: 8466.9229\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9045.2432 - val_loss: 8229.9014\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8889.3652 - val_loss: 8416.6621\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9172.3643 - val_loss: 8045.3306\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8948.5547 - val_loss: 8027.6719\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8969.1055 - val_loss: 7979.3975\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8947.7051 - val_loss: 8204.8867\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9000.7275 - val_loss: 8560.6934\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8972.7891 - val_loss: 8143.8623\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8845.2822 - val_loss: 7996.1313\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8933.2666 - val_loss: 8224.5322\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8926.7900 - val_loss: 8217.0244\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8812.6592 - val_loss: 8243.7236\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8997.8887 - val_loss: 8073.6172\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8908.2900 - val_loss: 8254.1514\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8947.1855 - val_loss: 8156.1353\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8936.2295 - val_loss: 8027.6172\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9010.4824 - val_loss: 8084.2515\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8924.2559 - val_loss: 8230.8115\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8978.2959 - val_loss: 8388.6963\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8919.4922 - val_loss: 8675.2188\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8966.2832 - val_loss: 8381.6279\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8882.0957 - val_loss: 8166.5723\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8925.0947 - val_loss: 8392.6338\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8765.8027 - val_loss: 8114.2310\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8912.2705 - val_loss: 7974.0991\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8761.6787 - val_loss: 7978.0488\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8957.4932 - val_loss: 8121.3516\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8831.7031 - val_loss: 8093.5918\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8905.5342 - val_loss: 8272.7178\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8789.1152 - val_loss: 7992.6821\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8798.9082 - val_loss: 8126.2417\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8812.8760 - val_loss: 8022.8955\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8720.9785 - val_loss: 8242.5918\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8815.2842 - val_loss: 8095.9727\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8678.8389 - val_loss: 8085.3301\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8680.8867 - val_loss: 8226.1914\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8628.5566 - val_loss: 8203.0898\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8714.7148 - val_loss: 8611.5781\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8770.4668 - val_loss: 8185.4355\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8668.1240 - val_loss: 7975.5186\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8860.3164 - val_loss: 8020.5454\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8755.0986 - val_loss: 7964.5088\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8710.1514 - val_loss: 8203.4482\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8701.6807 - val_loss: 8349.9326\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8872.6543 - val_loss: 8393.8975\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8795.0664 - val_loss: 7911.3423\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8808.2617 - val_loss: 8373.0107\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8765.6465 - val_loss: 8399.3848\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8669.6436 - val_loss: 7977.2905\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8574.2666 - val_loss: 8139.6919\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8632.6934 - val_loss: 7995.3691\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8772.7119 - val_loss: 8404.1699\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8794.4082 - val_loss: 8162.5874\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8721.0596 - val_loss: 8505.4434\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8662.1582 - val_loss: 8050.3096\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8759.6982 - val_loss: 8154.6636\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8613.6279 - val_loss: 8109.2021\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8483.6367 - val_loss: 8488.8730\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8625.9336 - val_loss: 8229.7090\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8565.7305 - val_loss: 8088.6035\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8640.5322 - val_loss: 7890.3789\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8717.2402 - val_loss: 8147.8140\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8537.1475 - val_loss: 7804.8779\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8619.5732 - val_loss: 7809.4346\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8622.5586 - val_loss: 8069.8066\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8628.5654 - val_loss: 7999.3901\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8685.2871 - val_loss: 8014.6440\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8755.8057 - val_loss: 8405.3291\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8696.4746 - val_loss: 8114.1079\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8538.6328 - val_loss: 8029.5767\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8612.5625 - val_loss: 8800.0088\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8596.0430 - val_loss: 7857.2690\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8722.8877 - val_loss: 8370.2080\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8582.1943 - val_loss: 8059.4775\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8586.8613 - val_loss: 8224.2363\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8575.9414 - val_loss: 8623.0830\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8621.6270 - val_loss: 8112.4922\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8753.0107 - val_loss: 8421.0596\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8498.3643 - val_loss: 7988.2852\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8456.4209 - val_loss: 7946.4111\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8398.3477 - val_loss: 8141.2393\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8580.4209 - val_loss: 8006.3364\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8580.3125 - val_loss: 8489.9326\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8520.8076 - val_loss: 8138.6807\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8533.7783 - val_loss: 8076.8667\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8590.8213 - val_loss: 8097.3066\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8411.4814 - val_loss: 8354.1777\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8493.0264 - val_loss: 8020.7236\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8653.0898 - val_loss: 8314.7998\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8539.1777 - val_loss: 8043.6616\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8428.5381 - val_loss: 8076.7944\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8427.1709 - val_loss: 8104.3940\n",
      "Epoch 134/300\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 8536.2871\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8585.9775 - val_loss: 8350.2373\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8522.7744 - val_loss: 8133.7520\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8349.9746 - val_loss: 8055.2827\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8496.0762 - val_loss: 7938.0420\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8434.2646 - val_loss: 7972.4648\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8371.8682 - val_loss: 8025.5063\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8304.1162 - val_loss: 7998.1250\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8471.8447 - val_loss: 7774.4448\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8338.5244 - val_loss: 8098.4878\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8329.5967 - val_loss: 8305.8447\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8417.7256 - val_loss: 8428.7275\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8537.7275 - val_loss: 7946.3521\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8452.4814 - val_loss: 8139.2334\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8404.5068 - val_loss: 7981.9971\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8379.7441 - val_loss: 8067.7397\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8375.4082 - val_loss: 7910.1646\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8410.1582 - val_loss: 8340.0762\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8271.7344 - val_loss: 8456.9092\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8209.6953 - val_loss: 8350.1895\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8396.5029 - val_loss: 8625.8730\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8396.4043 - val_loss: 8024.7803\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8401.1357 - val_loss: 7968.9707\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8461.0928 - val_loss: 8186.7490\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8409.0293 - val_loss: 8140.9189\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8300.8379 - val_loss: 8031.3530\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8299.4570 - val_loss: 8339.6035\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8351.0527 - val_loss: 8058.4028\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8375.4502 - val_loss: 8121.1440\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8439.3525 - val_loss: 7962.5161\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8279.2002 - val_loss: 8517.0020\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8386.2998 - val_loss: 7834.5981\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8389.9678 - val_loss: 7851.0845\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8155.4678 - val_loss: 8091.9209\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8261.9277 - val_loss: 7951.0200\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8267.8213 - val_loss: 8393.4053\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8327.8311 - val_loss: 8024.8496\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8169.8433 - val_loss: 8097.2798\n",
      "Epoch 171/300\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 8277.2754\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8220.5586 - val_loss: 8216.2607\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8341.6045 - val_loss: 8114.9443\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8298.4844 - val_loss: 7954.3228\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8267.6572 - val_loss: 8042.0190\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8241.0459 - val_loss: 7833.7256\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8169.7500 - val_loss: 8261.3662\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8267.5918 - val_loss: 8260.6934\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8249.8105 - val_loss: 8494.7715\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8234.3174 - val_loss: 8120.8911\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8210.8730 - val_loss: 8195.5381\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8232.6201 - val_loss: 8220.7275\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8291.6982 - val_loss: 8182.6470\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8141.6611 - val_loss: 8123.8521\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8203.1436 - val_loss: 7949.0933\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8177.8926 - val_loss: 8019.1943\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8009.4321 - val_loss: 7899.1729\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8250.4102 - val_loss: 8528.5850\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8322.2041 - val_loss: 8026.1514\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8099.0303 - val_loss: 8302.1182\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8195.5674 - val_loss: 7914.9541\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8126.1987 - val_loss: 8197.3057\n"
     ]
    }
   ],
   "source": [
    "min_losses3 = {10:[]}\n",
    "for c in min_losses3.keys():\n",
    "    i = 0\n",
    "    while i<5:\n",
    "        rand_list = create_rand_list(24, c)\n",
    "        train_x_rand = trainx_scaled.iloc[:,rand_list]\n",
    "        test_x_rand = textx_scaled.iloc[:,rand_list]\n",
    "        inp_size = len(test_x_rand.columns)\n",
    "        MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = [train_x_rand], y = trainy.values,  \n",
    "                                 validation_data = ([test_x_rand], testy.values),\n",
    "                                 epochs=300, batch_size = 300, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        min_losses3[c].append(best_val_loss)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [7689.59619140625,\n",
       "  7481.36279296875,\n",
       "  7247.634765625,\n",
       "  7873.80126953125,\n",
       "  7149.958984375],\n",
       " 14: [7135.24658203125,\n",
       "  6991.34423828125,\n",
       "  7087.94091796875,\n",
       "  7667.43408203125,\n",
       "  7400.22607421875],\n",
       " 16: [7182.0791015625,\n",
       "  6900.39892578125,\n",
       "  7286.2451171875,\n",
       "  7464.94580078125,\n",
       "  7165.7890625],\n",
       " 18: [6945.58056640625,\n",
       "  6785.36181640625,\n",
       "  7037.0595703125,\n",
       "  6850.4775390625,\n",
       "  7082.39306640625],\n",
       " 20: [6719.595703125,\n",
       "  7220.4541015625,\n",
       "  6916.8408203125,\n",
       "  7228.6123046875,\n",
       "  7138.6181640625],\n",
       " 22: [6814.2939453125,\n",
       "  7247.37939453125,\n",
       "  6913.01220703125,\n",
       "  7015.9736328125,\n",
       "  6917.2880859375],\n",
       " 24: [6928.3271484375,\n",
       "  6742.07470703125,\n",
       "  6775.55224609375,\n",
       "  7043.486328125,\n",
       "  6863.68115234375]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa5928fcd30>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [7689.59619140625,\n",
       "  7481.36279296875,\n",
       "  7247.634765625,\n",
       "  7873.80126953125,\n",
       "  7149.958984375],\n",
       " 14: [7135.24658203125,\n",
       "  6991.34423828125,\n",
       "  7087.94091796875,\n",
       "  7667.43408203125,\n",
       "  7400.22607421875],\n",
       " 16: [7182.0791015625,\n",
       "  6900.39892578125,\n",
       "  7286.2451171875,\n",
       "  7464.94580078125,\n",
       "  7165.7890625],\n",
       " 18: [6945.58056640625,\n",
       "  6785.36181640625,\n",
       "  7037.0595703125,\n",
       "  6850.4775390625,\n",
       "  7082.39306640625],\n",
       " 20: [6719.595703125,\n",
       "  7220.4541015625,\n",
       "  6916.8408203125,\n",
       "  7228.6123046875,\n",
       "  7138.6181640625],\n",
       " 22: [6814.2939453125,\n",
       "  7247.37939453125,\n",
       "  6913.01220703125,\n",
       "  7015.9736328125,\n",
       "  6917.2880859375],\n",
       " 24: [6928.3271484375,\n",
       "  6742.07470703125,\n",
       "  6775.55224609375,\n",
       "  7043.486328125,\n",
       "  6863.68115234375]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "min_losses2 = copy.deepcopy(min_losses)\n",
    "min_losses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: 7488.47080078125,\n",
       " 14: 7256.43837890625,\n",
       " 16: 7199.8916015625,\n",
       " 18: 6940.17451171875,\n",
       " 20: 7044.82421875,\n",
       " 22: 6981.589453125,\n",
       " 24: 6870.62431640625}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_losses = {}\n",
    "for count in min_losses2.keys():\n",
    "    avg_losses[count] = sum(min_losses2[count])/len(min_losses2[count])\n",
    "avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7548.93408203125"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(min_losses3[10])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
