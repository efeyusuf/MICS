{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17444899225027354846\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6593071867608912148\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n",
      "2.3.1\n",
      "Num GPUs Available:  0\n",
      "WARNING:tensorflow:From <ipython-input-2-b5bb3d29fd0a>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_gpu_available()\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./Datasets/energydata_complete.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.500000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.600000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.700000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:30:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.800000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:40:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 17:50:00</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.026667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>6.016667</td>\n",
       "      <td>734.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>43.833333</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>44.919484</td>\n",
       "      <td>44.919484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:00:00</th>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>45.766667</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>734.100000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>47.233763</td>\n",
       "      <td>47.233763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:10:00</th>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.863333</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>734.166667</td>\n",
       "      <td>91.833333</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>33.039890</td>\n",
       "      <td>33.039890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:20:00</th>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.597500</td>\n",
       "      <td>19.20</td>\n",
       "      <td>44.433333</td>\n",
       "      <td>19.73</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>734.233333</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>31.455702</td>\n",
       "      <td>31.455702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11 18:30:00</th>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>19.856667</td>\n",
       "      <td>46.090000</td>\n",
       "      <td>19.23</td>\n",
       "      <td>44.400000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.863333</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>46.096667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.29</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>734.300000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>3.089314</td>\n",
       "      <td>3.089314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1     T2  \\\n",
       "date                                                                   \n",
       "2016-01-11 17:00:00          60      30  19.890000  47.596667  19.20   \n",
       "2016-01-11 17:10:00          60      30  19.890000  46.693333  19.20   \n",
       "2016-01-11 17:20:00          50      30  19.890000  46.300000  19.20   \n",
       "2016-01-11 17:30:00          50      40  19.890000  46.066667  19.20   \n",
       "2016-01-11 17:40:00          60      40  19.890000  46.333333  19.20   \n",
       "2016-01-11 17:50:00          50      40  19.890000  46.026667  19.20   \n",
       "2016-01-11 18:00:00          60      50  19.890000  45.766667  19.20   \n",
       "2016-01-11 18:10:00          60      50  19.856667  45.560000  19.20   \n",
       "2016-01-11 18:20:00          60      40  19.790000  45.597500  19.20   \n",
       "2016-01-11 18:30:00          70      40  19.856667  46.090000  19.23   \n",
       "\n",
       "                          RH_2     T3       RH_3         T4       RH_4  ...  \\\n",
       "date                                                                    ...   \n",
       "2016-01-11 17:00:00  44.790000  19.79  44.730000  19.000000  45.566667  ...   \n",
       "2016-01-11 17:10:00  44.722500  19.79  44.790000  19.000000  45.992500  ...   \n",
       "2016-01-11 17:20:00  44.626667  19.79  44.933333  18.926667  45.890000  ...   \n",
       "2016-01-11 17:30:00  44.590000  19.79  45.000000  18.890000  45.723333  ...   \n",
       "2016-01-11 17:40:00  44.530000  19.79  45.000000  18.890000  45.530000  ...   \n",
       "2016-01-11 17:50:00  44.500000  19.79  44.933333  18.890000  45.730000  ...   \n",
       "2016-01-11 18:00:00  44.500000  19.79  44.900000  18.890000  45.790000  ...   \n",
       "2016-01-11 18:10:00  44.500000  19.73  44.900000  18.890000  45.863333  ...   \n",
       "2016-01-11 18:20:00  44.433333  19.73  44.790000  18.890000  45.790000  ...   \n",
       "2016-01-11 18:30:00  44.400000  19.79  44.863333  18.890000  46.096667  ...   \n",
       "\n",
       "                            T9   RH_9     T_out  Press_mm_hg     RH_out  \\\n",
       "date                                                                      \n",
       "2016-01-11 17:00:00  17.033333  45.53  6.600000   733.500000  92.000000   \n",
       "2016-01-11 17:10:00  17.066667  45.56  6.483333   733.600000  92.000000   \n",
       "2016-01-11 17:20:00  17.000000  45.50  6.366667   733.700000  92.000000   \n",
       "2016-01-11 17:30:00  17.000000  45.40  6.250000   733.800000  92.000000   \n",
       "2016-01-11 17:40:00  17.000000  45.40  6.133333   733.900000  92.000000   \n",
       "2016-01-11 17:50:00  17.000000  45.29  6.016667   734.000000  92.000000   \n",
       "2016-01-11 18:00:00  17.000000  45.29  5.900000   734.100000  92.000000   \n",
       "2016-01-11 18:10:00  17.000000  45.29  5.916667   734.166667  91.833333   \n",
       "2016-01-11 18:20:00  17.000000  45.29  5.933333   734.233333  91.666667   \n",
       "2016-01-11 18:30:00  17.000000  45.29  5.950000   734.300000  91.500000   \n",
       "\n",
       "                     Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
       "date                                                                         \n",
       "2016-01-11 17:00:00   7.000000   63.000000   5.300000  13.275433  13.275433  \n",
       "2016-01-11 17:10:00   6.666667   59.166667   5.200000  18.606195  18.606195  \n",
       "2016-01-11 17:20:00   6.333333   55.333333   5.100000  28.642668  28.642668  \n",
       "2016-01-11 17:30:00   6.000000   51.500000   5.000000  45.410389  45.410389  \n",
       "2016-01-11 17:40:00   5.666667   47.666667   4.900000  10.084097  10.084097  \n",
       "2016-01-11 17:50:00   5.333333   43.833333   4.800000  44.919484  44.919484  \n",
       "2016-01-11 18:00:00   5.000000   40.000000   4.700000  47.233763  47.233763  \n",
       "2016-01-11 18:10:00   5.166667   40.000000   4.683333  33.039890  33.039890  \n",
       "2016-01-11 18:20:00   5.333333   40.000000   4.666667  31.455702  31.455702  \n",
       "2016-01-11 18:30:00   5.500000   40.000000   4.650000   3.089314   3.089314  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=0)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'Appliances'}>]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(column=\"Appliances\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 28 and number of rows is: 19735\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num-2)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num-2)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_A_train_x_op = trainx_scaled.iloc[:,[0,19,20,21,22,23,24]] \n",
    "group_B_train_x_op = trainx_scaled.iloc[:,[1,2,3,4,5,6,7,8,9,10]]\n",
    "group_C_train_x_op = trainx_scaled.iloc[:,[11,12,13,14,15,16,17,18]]\n",
    "\n",
    "group_A_test_x_op = textx_scaled.iloc[:,[0,19,20,21,22,23,24]]\n",
    "group_B_test_x_op = textx_scaled.iloc[:,[1,2,3,4,5,6,7,8,9,10]]\n",
    "group_C_test_x_op = textx_scaled.iloc[:,[11,12,13,14,15,16,17,18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_inp_a = len(group_A_train_x_op.columns)\n",
    "size_inp_b = len(group_B_train_x_op.columns)\n",
    "size_inp_c = len(group_C_train_x_op.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_sizeA, inp_sizeB, inp_sizeC, use_encoders, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs_A = keras.layers.Input(shape=(inp_sizeA), name=\"input_A\")\n",
    "    inputs_B = keras.layers.Input(shape=(inp_sizeB), name=\"input_B\")\n",
    "    inputs_C = keras.layers.Input(shape=(inp_sizeC), name=\"input_C\")\n",
    "    \n",
    "    #If encoders are not to be used, inputs will be directly given to global model\n",
    "    \n",
    "    if use_encoders == True:\n",
    "        encoder_A = get_encoder_model(inp_sizeA)\n",
    "        encoder_B = get_encoder_model(inp_sizeB)\n",
    "        encoder_C = get_encoder_model(inp_sizeC)\n",
    "    \n",
    "        global_inp_A = encoder_A(inputs_A)\n",
    "        global_inp_B = encoder_B(inputs_B)\n",
    "        global_inp_C = encoder_C(inputs_C)\n",
    "\n",
    "        global_inp = keras.layers.concatenate([global_inp_A, global_inp_B, global_inp_C])\n",
    "    else:\n",
    "        global_inp = keras.layers.concatenate([inputs_A, inputs_A, inputs_A])\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_inp)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"relu\")(h)    \n",
    "    return keras.Model(inputs=[inputs_A, inputs_B, inputs_C], outputs = outputs)\n",
    "\n",
    "def get_encoder_model(inp_size):\n",
    "    inputs = keras.layers.Input(shape=(inp_size))\n",
    "    h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "    return keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_sizeA, inp_sizeB, inp_sizeC, use_encoders, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs_A = keras.layers.Input(shape=(inp_sizeA), name=\"input_A\")\n",
    "    inputs_B = keras.layers.Input(shape=(inp_sizeB), name=\"input_B\")\n",
    "    inputs_C = keras.layers.Input(shape=(inp_sizeC), name=\"input_C\")\n",
    "    \n",
    "    #If encoders are not to be used, inputs will be directly given to global model\n",
    "    \n",
    "    if use_encoders == True:\n",
    "        encoder_A = get_encoder_model(inp_sizeA)\n",
    "        encoder_B = get_encoder_model(inp_sizeB)\n",
    "        encoder_C = get_encoder_model(inp_sizeC)\n",
    "    \n",
    "        global_inp_A = encoder_A(inputs_A)\n",
    "        global_inp_B = encoder_B(inputs_B)\n",
    "        global_inp_C = encoder_C(inputs_C)\n",
    "\n",
    "        global_inp = keras.layers.concatenate([global_inp_A, global_inp_B, global_inp_C])\n",
    "    else:\n",
    "        global_inp = keras.layers.concatenate([inputs_A, inputs_A, inputs_A])\n",
    "        \n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(X_train, Y_train)\n",
    "    \n",
    "    return keras.Model(inputs=[inputs_A, inputs_B, inputs_C], outputs = outputs)\n",
    "\n",
    "def get_encoder_model(inp_size):\n",
    "    inputs = keras.layers.Input(shape=(inp_size))\n",
    "    h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "    return keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 12784.1963 - val_loss: 8897.4795\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10252.8936 - val_loss: 8739.0176\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9818.7070 - val_loss: 8299.8105\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9489.5078 - val_loss: 8662.0889\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9272.0029 - val_loss: 7658.0195\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9002.4004 - val_loss: 8400.9980\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8817.4873 - val_loss: 7628.0586\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8592.9092 - val_loss: 8030.0542\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8589.8945 - val_loss: 7725.1157\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8469.6836 - val_loss: 7312.0239\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8517.1816 - val_loss: 7688.7900\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8390.3154 - val_loss: 7458.7339\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8126.5752 - val_loss: 7415.1396\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8233.1992 - val_loss: 7422.9995\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8152.3545 - val_loss: 7218.6426\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8179.4302 - val_loss: 7465.7935\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7975.3911 - val_loss: 7872.9746\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7863.7666 - val_loss: 7342.6060\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8006.3379 - val_loss: 7701.5806\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7929.1157 - val_loss: 7648.3589\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7974.1865 - val_loss: 7062.6201\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7909.8770 - val_loss: 7043.8862\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7676.7788 - val_loss: 7177.0815\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7682.8545 - val_loss: 7212.0562\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7793.2393 - val_loss: 7080.5850\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7511.1523 - val_loss: 6829.2153\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7570.5566 - val_loss: 7152.0796\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7806.0962 - val_loss: 7264.4873\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7770.2388 - val_loss: 6885.2534\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7579.7534 - val_loss: 7009.5356\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7404.2891 - val_loss: 7058.3477\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7595.7842 - val_loss: 7344.6333\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7629.8999 - val_loss: 7265.7612\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7454.4829 - val_loss: 6919.6182\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7492.8667 - val_loss: 6920.5366\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7314.1875 - val_loss: 7053.1479\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7397.9849 - val_loss: 6980.1919\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7341.2158 - val_loss: 6862.9946\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7237.1709 - val_loss: 7633.3442\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7401.4248 - val_loss: 6998.9233\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7149.6650 - val_loss: 7013.4468\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7085.1680 - val_loss: 7134.2388\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7302.4253 - val_loss: 7286.0352\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7229.8550 - val_loss: 6984.0796\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7228.8647 - val_loss: 6814.8613\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7436.4648 - val_loss: 7348.9946\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7290.6851 - val_loss: 7176.5786\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7231.0308 - val_loss: 7055.7969\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7333.7280 - val_loss: 7666.2681\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7268.9404 - val_loss: 7168.7842\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7288.2910 - val_loss: 6887.9854\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7084.8530 - val_loss: 7003.6621\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6978.3340 - val_loss: 7849.0444\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7340.5903 - val_loss: 7181.2046\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6917.2461 - val_loss: 7058.5859\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6979.5986 - val_loss: 6893.0059\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6893.8506 - val_loss: 6913.2588\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7137.4121 - val_loss: 6850.7188\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6867.3188 - val_loss: 7349.6504\n",
      "Epoch 60/300\n",
      "24/53 [============>.................] - ETA: 0s - loss: 6866.4551"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-98150f947995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                           min_lr = 1e-6)]\n\u001b[1;32m      6\u001b[0m \u001b[0mMICS_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = MICS_model.fit(x = [group_A_train_x_op.values, group_B_train_x_op.values, group_C_train_x_op.values], y = trainy.values,  \n\u001b[0m\u001b[1;32m      8\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_A_test_x_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_B_test_x_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_C_test_x_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                          epochs=300, batch_size = 300, callbacks=callback)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MICS_model = get_MICS_model(size_inp_a, size_inp_b, size_inp_c, use_encoders = True, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-6)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "history = MICS_model.fit(x = [group_A_train_x_op.values, group_B_train_x_op.values, group_C_train_x_op.values], y = trainy.values,  \n",
    "                         validation_data = ([group_A_test_x_op.values, group_B_test_x_op.values, group_C_test_x_op.values], testy.values),\n",
    "                         epochs=300, batch_size = 300, callbacks=callback)\n",
    "training_val_loss = history.history[\"val_loss\"]\n",
    "best_row_index = np.argmin(training_val_loss)\n",
    "best_val_loss = training_val_loss[best_row_index]\n",
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa5928fcd30>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12740.8916 - val_loss: 9259.6953\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10274.4629 - val_loss: 8880.1045\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9621.7842 - val_loss: 8759.0527\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9217.8545 - val_loss: 7978.4395\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9114.3682 - val_loss: 7980.1650\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8834.1758 - val_loss: 7987.6099\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8685.9688 - val_loss: 7671.4712\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8736.8652 - val_loss: 7644.3784\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8503.8242 - val_loss: 7449.0688\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8350.2969 - val_loss: 7934.0210\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8306.8438 - val_loss: 7459.1733\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8220.3037 - val_loss: 7451.5522\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8271.0645 - val_loss: 7678.3135\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8076.8706 - val_loss: 7335.5142\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8000.9805 - val_loss: 7313.9497\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7975.8330 - val_loss: 7299.5903\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7947.5068 - val_loss: 7699.9146\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8027.9517 - val_loss: 7941.0166\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7984.5518 - val_loss: 8114.0459\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7952.7681 - val_loss: 7287.8325\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7932.2466 - val_loss: 7761.3472\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7891.6323 - val_loss: 7334.9155\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8000.7031 - val_loss: 7390.2866\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7711.5303 - val_loss: 7265.1680\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7509.2866 - val_loss: 7536.5581\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7661.0435 - val_loss: 7214.1899\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7687.5620 - val_loss: 7868.8574\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7719.7466 - val_loss: 7211.0659\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7719.6553 - val_loss: 7114.7959\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7606.5698 - val_loss: 7208.9268\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7660.7466 - val_loss: 7079.8120\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7468.4458 - val_loss: 7184.0059\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7464.0801 - val_loss: 7440.2554\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7442.0972 - val_loss: 7293.7173\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7622.8691 - val_loss: 7332.1226\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7404.2495 - val_loss: 7490.7964\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7462.3652 - val_loss: 7683.4165\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7551.2646 - val_loss: 7510.8750\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7332.8496 - val_loss: 6958.2212\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7462.0664 - val_loss: 7199.8936\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7233.8296 - val_loss: 7287.7930\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7215.5542 - val_loss: 7204.6157\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7261.7505 - val_loss: 7029.0308\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7180.8833 - val_loss: 6914.6050\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7329.7378 - val_loss: 7632.6689\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7351.4541 - val_loss: 7046.2222\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7377.1606 - val_loss: 6954.8892\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7165.3740 - val_loss: 7167.2642\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7218.7612 - val_loss: 7388.1216\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7318.9692 - val_loss: 6957.9912\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7062.5781 - val_loss: 6905.9727\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7294.8530 - val_loss: 7336.5420\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7098.8052 - val_loss: 6839.9292\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7144.5264 - val_loss: 7106.9390\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7217.5078 - val_loss: 7136.8203\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7295.4810 - val_loss: 6896.2910\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7139.2451 - val_loss: 6905.8062\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6949.2700 - val_loss: 6978.0698\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6995.5747 - val_loss: 6889.0967\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6933.8584 - val_loss: 6976.5029\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6751.0801 - val_loss: 6853.5352\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6994.6045 - val_loss: 6888.6621\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6991.6191 - val_loss: 7037.3320\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6995.2090 - val_loss: 6755.4473\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6962.4268 - val_loss: 6789.2314\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6928.2007 - val_loss: 7110.0503\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6969.2344 - val_loss: 6763.9331\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6922.0532 - val_loss: 7285.9595\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7124.6650 - val_loss: 7245.4062\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7037.1406 - val_loss: 6889.6255\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6814.5942 - val_loss: 6753.9160\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6831.6191 - val_loss: 6840.1069\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6898.4067 - val_loss: 6879.7339\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6785.5840 - val_loss: 6980.3403\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6852.4375 - val_loss: 6891.4702\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7135.9175 - val_loss: 7290.4248\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6964.5332 - val_loss: 6848.5371\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6758.2251 - val_loss: 6996.2026\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6793.7104 - val_loss: 6759.9644\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6761.0352 - val_loss: 6757.8159\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6579.3569 - val_loss: 6970.7168\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6887.6089 - val_loss: 7505.6074\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6837.8760 - val_loss: 7117.0625\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6699.3364 - val_loss: 7129.9600\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6940.3916 - val_loss: 6880.9819\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6740.0938 - val_loss: 6678.6714\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6567.8228 - val_loss: 6960.0396\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6505.6870 - val_loss: 7160.9712\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6788.0312 - val_loss: 7025.0054\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6808.9771 - val_loss: 6967.0854\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6538.3472 - val_loss: 6869.6035\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6494.8115 - val_loss: 6841.9399\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6643.1157 - val_loss: 7010.4248\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6465.6802 - val_loss: 6728.4634\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6742.6045 - val_loss: 6844.4844\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6941.7925 - val_loss: 7157.5078\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6881.5293 - val_loss: 6945.4473\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6714.4668 - val_loss: 6966.3530\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6535.3022 - val_loss: 6981.6572\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6704.0649 - val_loss: 6944.9482\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6509.1313 - val_loss: 6959.6279\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6520.9023 - val_loss: 6988.3779\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6485.3237 - val_loss: 7414.7446\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6497.3306 - val_loss: 6734.3052\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6792.3677 - val_loss: 6706.5928\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6605.4893 - val_loss: 6823.3115\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6462.4873 - val_loss: 7209.3843\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6379.1963 - val_loss: 7185.6460\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6475.8120 - val_loss: 6635.3164\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6374.1094 - val_loss: 6900.5396\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6534.5757 - val_loss: 6852.5088\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6467.0342 - val_loss: 6936.9492\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6692.7075 - val_loss: 7050.4429\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6552.0259 - val_loss: 6949.2583\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6615.6035 - val_loss: 6721.2998\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6574.7876 - val_loss: 7043.3413\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6590.3818 - val_loss: 7289.4390\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6461.4971 - val_loss: 6871.9453\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6468.7148 - val_loss: 7522.8394\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6443.3906 - val_loss: 6660.7397\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6435.7344 - val_loss: 7175.3042\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6464.8711 - val_loss: 6900.6826\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6350.7114 - val_loss: 6839.1875\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6296.5645 - val_loss: 6986.9941\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6463.9712 - val_loss: 6746.2998\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6375.1045 - val_loss: 6752.6870\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6303.1577 - val_loss: 6808.0435\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6440.4146 - val_loss: 6726.5405\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6209.1533 - val_loss: 6947.8506\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6224.7891 - val_loss: 6767.5952\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6328.8501 - val_loss: 6920.4219\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6502.0806 - val_loss: 6601.6113\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6238.7241 - val_loss: 7130.1499\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6357.3271 - val_loss: 6668.2983\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6516.4429 - val_loss: 6681.7148\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6469.6162 - val_loss: 6994.0552\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6428.6748 - val_loss: 6508.2827\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6179.5283 - val_loss: 6729.3452\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6318.2524 - val_loss: 6858.3608\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6292.0791 - val_loss: 6646.9136\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6908.9814 - val_loss: 6974.3506\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6309.7393 - val_loss: 6761.5566\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6281.8540 - val_loss: 6798.0107\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6298.5894 - val_loss: 6941.7534\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6377.9253 - val_loss: 6614.0952\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6111.6304 - val_loss: 7175.0640\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6419.8203 - val_loss: 6572.8970\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6018.6299 - val_loss: 7055.3447\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6302.7485 - val_loss: 6647.9526\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6182.9214 - val_loss: 7206.1553\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6211.2891 - val_loss: 6516.5420\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6395.9116 - val_loss: 7083.6929\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6197.0171 - val_loss: 6839.5327\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6252.1465 - val_loss: 6873.4653\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6068.1304 - val_loss: 6499.7964\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6516.8936 - val_loss: 6562.2861\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6335.4478 - val_loss: 7018.1030\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6164.9897 - val_loss: 6945.9888\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6199.1401 - val_loss: 7131.1353\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6459.4268 - val_loss: 6934.6084\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6209.6011 - val_loss: 6700.5317\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6268.0264 - val_loss: 6759.6958\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6138.6318 - val_loss: 6855.5830\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6056.6270 - val_loss: 6771.7925\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6225.0645 - val_loss: 6536.5425\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6063.8047 - val_loss: 6903.8892\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6276.5728 - val_loss: 6777.3960\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6511.7480 - val_loss: 6555.8545\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6030.7158 - val_loss: 6949.8960\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6388.5127 - val_loss: 6786.9170\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6399.8794 - val_loss: 6835.4233\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6205.4189 - val_loss: 6921.3062\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6188.1753 - val_loss: 6809.2900\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6121.8604 - val_loss: 6915.0449\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6135.3179 - val_loss: 6913.6313\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6256.0767 - val_loss: 6490.4912\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6094.3765 - val_loss: 6671.9814\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6095.5229 - val_loss: 6458.2666\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6243.6240 - val_loss: 6835.1934\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6052.9956 - val_loss: 6642.2314\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6012.3896 - val_loss: 6988.1172\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6002.4429 - val_loss: 6661.3999\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6007.1846 - val_loss: 6592.2949\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6098.3770 - val_loss: 7046.1904\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6029.3638 - val_loss: 6476.3838\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6085.6460 - val_loss: 6791.1187\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6123.7129 - val_loss: 6922.7925\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6010.9092 - val_loss: 6766.1021\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6029.1187 - val_loss: 6544.4746\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6332.1982 - val_loss: 6568.8931\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6052.3081 - val_loss: 6522.2676\n",
      "Epoch 192/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6172.9644 - val_loss: 6405.2842\n",
      "Epoch 193/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6066.8848 - val_loss: 6656.7148\n",
      "Epoch 194/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6188.3853 - val_loss: 6812.0625\n",
      "Epoch 195/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6078.3862 - val_loss: 6656.5049\n",
      "Epoch 196/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6169.2710 - val_loss: 6795.0337\n",
      "Epoch 197/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6248.6636 - val_loss: 6811.7026\n",
      "Epoch 198/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6300.1128 - val_loss: 6622.2012\n",
      "Epoch 199/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6070.7144 - val_loss: 7180.5132\n",
      "Epoch 200/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6065.8203 - val_loss: 6768.8853\n",
      "Epoch 201/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6042.0176 - val_loss: 6367.7563\n",
      "Epoch 202/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5899.4502 - val_loss: 7325.6572\n",
      "Epoch 203/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6071.2617 - val_loss: 6512.4224\n",
      "Epoch 204/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6035.3452 - val_loss: 6666.1494\n",
      "Epoch 205/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5994.1055 - val_loss: 6464.4805\n",
      "Epoch 206/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6231.8423 - val_loss: 6887.1040\n",
      "Epoch 207/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6430.1592 - val_loss: 6542.7666\n",
      "Epoch 208/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5995.8975 - val_loss: 6842.3701\n",
      "Epoch 209/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6057.6509 - val_loss: 6536.5474\n",
      "Epoch 210/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5944.8457 - val_loss: 6820.2661\n",
      "Epoch 211/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6057.0156 - val_loss: 6894.7153\n",
      "Epoch 212/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5971.5913 - val_loss: 6627.3071\n",
      "Epoch 213/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6060.5449 - val_loss: 6734.7266\n",
      "Epoch 214/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6058.9199 - val_loss: 6753.3203\n",
      "Epoch 215/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5959.4331 - val_loss: 6785.9023\n",
      "Epoch 216/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6040.8145 - val_loss: 6629.6963\n",
      "Epoch 217/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5858.0601 - val_loss: 6538.3848\n",
      "Epoch 218/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5913.4414 - val_loss: 6400.0552\n",
      "Epoch 219/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5991.7881 - val_loss: 6720.7007\n",
      "Epoch 220/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5859.4756 - val_loss: 6792.4487\n",
      "Epoch 221/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5851.5527 - val_loss: 6473.0864\n",
      "Epoch 222/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6216.0293 - val_loss: 6994.0928\n",
      "Epoch 223/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6117.7646 - val_loss: 6579.1445\n",
      "Epoch 224/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5940.3813 - val_loss: 6945.8208\n",
      "Epoch 225/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5809.0234 - val_loss: 6870.0972\n",
      "Epoch 226/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5778.8037 - val_loss: 6515.7822\n",
      "Epoch 227/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5871.1924 - val_loss: 6738.3315\n",
      "Epoch 228/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5697.2217 - val_loss: 6541.1040\n",
      "Epoch 229/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6013.6958 - val_loss: 6770.4175\n",
      "Epoch 230/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6004.4492 - val_loss: 6641.5420\n",
      "Epoch 231/300\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 5970.8579\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6032.8506 - val_loss: 7878.8223\n",
      "Epoch 232/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5940.3716 - val_loss: 6552.0229\n",
      "Epoch 233/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5679.7144 - val_loss: 6521.2666\n",
      "Epoch 234/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5545.6543 - val_loss: 6544.9663\n",
      "Epoch 235/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5629.3735 - val_loss: 6720.9707\n",
      "Epoch 236/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5599.4766 - val_loss: 6472.4873\n",
      "Epoch 237/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5572.3159 - val_loss: 6492.9438\n",
      "Epoch 238/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5648.8174 - val_loss: 6626.3081\n",
      "Epoch 239/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5949.1729 - val_loss: 6645.8037\n",
      "Epoch 240/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5719.2104 - val_loss: 6480.8325\n",
      "Epoch 241/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5753.8486 - val_loss: 6417.7090\n",
      "Epoch 242/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6010.4043 - val_loss: 6616.8022\n",
      "Epoch 243/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5691.0405 - val_loss: 7089.3921\n",
      "Epoch 244/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5691.0854 - val_loss: 6745.2349\n",
      "Epoch 245/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5773.5591 - val_loss: 6564.7354\n",
      "Epoch 246/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5604.8027 - val_loss: 6450.2974\n",
      "Epoch 247/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5592.0312 - val_loss: 6786.3940\n",
      "Epoch 248/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5639.7432 - val_loss: 6373.5884\n",
      "Epoch 249/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5691.1689 - val_loss: 6495.2378\n",
      "Epoch 250/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5771.1982 - val_loss: 6599.0591\n",
      "Epoch 251/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5651.2290 - val_loss: 6676.1602\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12847.6533 - val_loss: 8993.0928\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10468.5469 - val_loss: 9019.8643\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9846.7490 - val_loss: 8391.7979\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9336.6738 - val_loss: 7870.3115\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9087.2900 - val_loss: 8029.7222\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9039.8281 - val_loss: 7781.8931\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9003.9893 - val_loss: 7855.4731\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8841.5244 - val_loss: 8028.4268\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8700.0752 - val_loss: 7518.4731\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8497.8018 - val_loss: 8049.5200\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8647.5713 - val_loss: 7876.8545\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8367.2529 - val_loss: 7630.0586\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8462.1621 - val_loss: 7662.1377\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8323.6729 - val_loss: 7947.7148\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8065.0918 - val_loss: 7435.0757\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8259.0156 - val_loss: 7474.5640\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8058.5361 - val_loss: 7380.4019\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8059.2793 - val_loss: 7315.3149\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8039.9282 - val_loss: 7577.7686\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7908.8379 - val_loss: 7399.8247\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7874.1885 - val_loss: 7114.4893\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7936.9478 - val_loss: 7281.0210\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7903.1904 - val_loss: 7333.4985\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8199.6045 - val_loss: 7805.8530\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7825.8115 - val_loss: 7190.2188\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7609.1821 - val_loss: 6995.2158\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7778.0679 - val_loss: 7283.8491\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7533.9741 - val_loss: 7015.1465\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7637.5205 - val_loss: 7256.1167\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7576.1626 - val_loss: 7228.0942\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7527.7495 - val_loss: 7124.3906\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7843.3330 - val_loss: 7131.9941\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7623.8521 - val_loss: 7101.5000\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7562.1816 - val_loss: 7176.4141\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7449.6797 - val_loss: 7305.0312\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7393.7104 - val_loss: 7346.2417\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7571.9829 - val_loss: 7160.0000\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7439.3711 - val_loss: 7025.8047\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7448.9106 - val_loss: 6741.5684\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7442.5688 - val_loss: 7054.6367\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7392.0249 - val_loss: 7393.1021\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7371.0146 - val_loss: 6793.6265\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7319.0977 - val_loss: 7017.7520\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7221.7729 - val_loss: 7043.6016\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7224.6343 - val_loss: 6918.6172\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7269.6914 - val_loss: 7045.2324\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7171.1846 - val_loss: 6802.4673\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7041.9038 - val_loss: 6857.2227\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7272.9434 - val_loss: 7132.0947\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7229.3184 - val_loss: 6746.0674\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7052.7207 - val_loss: 7073.4043\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7068.8911 - val_loss: 6878.6167\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7171.2568 - val_loss: 6869.5264\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7168.3403 - val_loss: 6729.2300\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7307.4395 - val_loss: 6880.8491\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7041.0962 - val_loss: 6694.2183\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7064.7930 - val_loss: 6867.0835\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7332.6440 - val_loss: 6728.9951\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7012.1514 - val_loss: 6797.7778\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6862.8638 - val_loss: 6817.6201\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6943.1641 - val_loss: 6846.6802\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6943.2256 - val_loss: 6666.6406\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6934.4448 - val_loss: 7008.7559\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7015.5854 - val_loss: 6884.1250\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6968.8965 - val_loss: 7220.7393\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6864.5083 - val_loss: 6559.5586\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6771.6855 - val_loss: 7106.2388\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6744.2852 - val_loss: 6811.9053\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7011.2227 - val_loss: 6948.4106\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6886.9990 - val_loss: 6665.8525\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6878.1055 - val_loss: 6816.5781\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6857.0015 - val_loss: 6958.0708\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6719.3188 - val_loss: 7270.1870\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6826.3428 - val_loss: 6563.1177\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6681.5811 - val_loss: 6954.0215\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6852.6743 - val_loss: 6639.7871\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6484.5371 - val_loss: 6656.1753\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6814.8452 - val_loss: 7048.5679\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6790.1338 - val_loss: 6797.1597\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6745.3477 - val_loss: 6676.5151\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6630.3228 - val_loss: 6711.4946\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6703.8862 - val_loss: 6817.1968\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6747.8105 - val_loss: 6601.7422\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6709.1162 - val_loss: 6774.7866\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6873.8491 - val_loss: 6745.2856\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6717.3530 - val_loss: 7006.9443\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6670.5718 - val_loss: 6742.1738\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6509.2725 - val_loss: 6666.9907\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6864.8110 - val_loss: 6836.6357\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6563.6079 - val_loss: 7056.6440\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6859.7217 - val_loss: 6998.3462\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6847.1147 - val_loss: 6921.8853\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6631.9038 - val_loss: 6787.8999\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6514.8979 - val_loss: 6815.1479\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6308.5215 - val_loss: 7148.5752\n",
      "Epoch 96/300\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 6702.8936\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6664.5615 - val_loss: 7065.8516\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6460.8667 - val_loss: 7081.1753\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6378.2256 - val_loss: 6955.1143\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6424.7002 - val_loss: 7148.2939\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6444.2969 - val_loss: 6779.5991\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6352.1567 - val_loss: 6842.7500\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6219.6274 - val_loss: 6758.3496\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6283.0723 - val_loss: 6722.7480\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6387.7690 - val_loss: 6460.5449\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6176.8340 - val_loss: 6802.9453\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6392.0884 - val_loss: 6820.5205\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6177.4263 - val_loss: 6842.5317\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6244.1636 - val_loss: 6970.9556\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6333.9736 - val_loss: 6651.3516\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6275.6182 - val_loss: 6575.1782\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6144.3462 - val_loss: 6717.6240\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6322.0679 - val_loss: 6910.1221\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6331.1304 - val_loss: 6725.7217\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6225.5024 - val_loss: 6558.6899\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6338.3345 - val_loss: 6639.1606\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6349.8618 - val_loss: 6776.9736\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6401.0010 - val_loss: 6894.4238\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6200.8921 - val_loss: 6742.9683\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6266.6313 - val_loss: 6800.2925\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6150.5459 - val_loss: 6777.3613\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6101.0444 - val_loss: 6791.3398\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5919.8345 - val_loss: 6746.5161\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6190.1606 - val_loss: 7182.5464\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6110.4517 - val_loss: 6638.0303\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6073.7290 - val_loss: 6548.1055\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6334.9658 - val_loss: 6758.7632\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6195.0825 - val_loss: 7317.3340\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6048.6289 - val_loss: 7056.5562\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5960.2720 - val_loss: 6812.9688\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5959.0122 - val_loss: 6729.7280\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6198.1909 - val_loss: 6785.4111\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6252.6025 - val_loss: 6812.1089\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6112.4971 - val_loss: 7308.4956\n",
      "Epoch 134/300\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 6220.7109\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6144.4380 - val_loss: 6662.9956\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5984.0361 - val_loss: 6834.8213\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5984.8677 - val_loss: 6893.4878\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5944.3496 - val_loss: 6916.7446\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5936.5474 - val_loss: 6923.2871\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6084.9741 - val_loss: 6878.4053\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6020.0439 - val_loss: 6759.4370\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5880.2959 - val_loss: 6826.4424\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6002.1655 - val_loss: 6600.9634\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5940.7056 - val_loss: 6790.4146\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5925.9385 - val_loss: 7003.0820\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6026.1221 - val_loss: 6774.8320\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5870.5679 - val_loss: 6651.3584\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5895.3037 - val_loss: 6651.3789\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5946.9111 - val_loss: 7102.9580\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5846.8325 - val_loss: 6712.0693\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5856.4053 - val_loss: 7001.3286\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5767.7764 - val_loss: 7050.9917\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5754.2480 - val_loss: 6814.6924\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5703.7939 - val_loss: 6984.6177\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5716.6699 - val_loss: 6557.4014\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12947.9639 - val_loss: 9511.5488\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10273.6992 - val_loss: 8756.3887\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9650.3408 - val_loss: 7966.3135\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9156.6650 - val_loss: 7714.3828\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9090.4434 - val_loss: 7897.9258\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8929.6426 - val_loss: 7560.4595\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8682.5518 - val_loss: 7653.4536\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8628.2041 - val_loss: 7584.4375\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8557.3154 - val_loss: 7662.3384\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8534.5938 - val_loss: 7801.6333\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8458.2949 - val_loss: 7824.9941\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8224.9619 - val_loss: 7676.9517\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8344.2354 - val_loss: 7632.2520\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8287.6055 - val_loss: 7555.5659\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7986.7949 - val_loss: 7164.3413\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8256.6016 - val_loss: 7710.1235\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7878.2100 - val_loss: 7309.7188\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7922.1348 - val_loss: 7632.8398\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8155.0361 - val_loss: 7703.0718\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7889.3271 - val_loss: 7289.4883\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7689.6636 - val_loss: 7566.5684\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7791.8232 - val_loss: 7339.1426\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7801.9551 - val_loss: 7240.6958\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7798.3999 - val_loss: 6922.3848\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7597.6411 - val_loss: 7063.6016\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7633.5015 - val_loss: 7177.0942\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7555.4307 - val_loss: 7172.6587\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7538.0854 - val_loss: 7236.8545\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7604.5488 - val_loss: 7147.4619\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7407.3032 - val_loss: 7253.9917\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7598.6567 - val_loss: 7008.8081\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7550.1953 - val_loss: 7083.1265\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7475.1494 - val_loss: 6993.6689\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7320.5874 - val_loss: 7172.7979\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7200.2407 - val_loss: 6968.8511\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7449.2808 - val_loss: 7064.5610\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7294.4175 - val_loss: 6976.7964\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7589.0845 - val_loss: 6876.3428\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7360.2261 - val_loss: 7012.6348\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7182.9551 - val_loss: 7278.5645\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7395.5889 - val_loss: 7048.4175\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7350.9790 - val_loss: 6971.5967\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7277.0693 - val_loss: 7298.5479\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7114.4858 - val_loss: 7402.6030\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7122.7402 - val_loss: 6952.0225\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7106.7886 - val_loss: 7309.3271\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7150.3008 - val_loss: 6816.4443\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7308.0029 - val_loss: 6933.1489\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6919.8506 - val_loss: 6767.2720\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6951.9146 - val_loss: 6843.3057\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7105.9805 - val_loss: 6853.0542\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7055.9321 - val_loss: 6812.1943\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6994.1802 - val_loss: 7059.3906\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6993.0454 - val_loss: 6992.1826\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7032.1890 - val_loss: 6811.1528\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6955.4370 - val_loss: 6900.7944\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6955.8047 - val_loss: 6987.3931\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7161.1221 - val_loss: 7293.5127\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6898.4844 - val_loss: 7082.8159\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6694.3848 - val_loss: 6881.7939\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6826.7817 - val_loss: 6767.7251\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6869.9946 - val_loss: 6879.2476\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6923.5166 - val_loss: 6855.2793\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6956.3662 - val_loss: 6810.6470\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6990.4189 - val_loss: 7019.2935\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6716.5566 - val_loss: 6881.6567\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6848.4717 - val_loss: 6744.3130\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6715.6274 - val_loss: 7213.7046\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7039.9331 - val_loss: 7231.7739\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7081.3057 - val_loss: 6794.6831\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6850.2437 - val_loss: 7259.2612\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6967.1841 - val_loss: 6997.6089\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6715.9995 - val_loss: 6952.5679\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6764.0825 - val_loss: 6936.6782\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6920.3540 - val_loss: 6647.8735\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6734.3145 - val_loss: 6856.2251\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6771.4810 - val_loss: 6747.0908\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6795.4854 - val_loss: 7136.3169\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6698.5601 - val_loss: 7106.5752\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6735.8057 - val_loss: 6878.5161\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6798.7437 - val_loss: 6850.9927\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6418.4048 - val_loss: 6743.3555\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6538.2109 - val_loss: 6786.1230\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6686.5430 - val_loss: 6991.3862\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6622.2827 - val_loss: 6892.3037\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6628.0137 - val_loss: 7001.9185\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6628.8613 - val_loss: 6632.8140\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6810.5005 - val_loss: 6784.1128\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6700.5991 - val_loss: 6736.6206\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6570.5400 - val_loss: 6963.8965\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6704.8599 - val_loss: 6664.4438\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6397.8408 - val_loss: 6882.4780\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6907.9038 - val_loss: 6844.6948\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6613.8350 - val_loss: 6919.7329\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6668.3433 - val_loss: 6841.4951\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6691.6719 - val_loss: 6755.8921\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6521.4331 - val_loss: 6797.4180\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6664.9014 - val_loss: 7035.8687\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6550.6401 - val_loss: 6975.9790\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6447.4604 - val_loss: 7199.2017\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6520.2744 - val_loss: 7257.4644\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6498.6533 - val_loss: 6588.2554\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6440.9424 - val_loss: 6742.7715\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6398.5088 - val_loss: 7690.5918\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6457.0894 - val_loss: 6695.8271\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6562.8550 - val_loss: 6715.3462\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6368.8276 - val_loss: 7030.1426\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6336.8696 - val_loss: 6692.0410\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6550.7617 - val_loss: 7012.4321\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6333.6133 - val_loss: 6785.7222\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6361.1870 - val_loss: 6868.5132\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6221.6094 - val_loss: 6683.5566\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6771.4995 - val_loss: 6653.2231\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6671.0947 - val_loss: 6641.0293\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6354.9922 - val_loss: 6671.2119\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6278.3203 - val_loss: 6705.4595\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6284.6772 - val_loss: 7079.4756\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6368.5015 - val_loss: 6455.3071\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6431.4492 - val_loss: 6842.6885\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6255.5078 - val_loss: 6723.4009\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6310.8262 - val_loss: 6779.5029\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6259.2715 - val_loss: 6744.7803\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6364.7231 - val_loss: 6676.8506\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6300.4922 - val_loss: 6673.6348\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6203.5991 - val_loss: 6832.8608\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6358.1968 - val_loss: 7126.3994\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6185.3706 - val_loss: 6822.6665\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6188.8779 - val_loss: 6679.6948\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6204.1987 - val_loss: 6678.8237\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6054.6616 - val_loss: 6768.7803\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6231.2422 - val_loss: 6752.4355\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6627.1797 - val_loss: 6583.2783\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6153.6470 - val_loss: 6813.7632\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6285.4390 - val_loss: 6587.9702\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6119.8003 - val_loss: 7179.2388\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6129.9053 - val_loss: 7286.8945\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6309.0918 - val_loss: 6715.5537\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6055.4673 - val_loss: 6550.6704\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6261.0918 - val_loss: 6674.0107\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6118.7632 - val_loss: 6565.1567\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6133.3521 - val_loss: 6418.0986\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6296.7446 - val_loss: 6812.3135\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6144.0171 - val_loss: 6487.8789\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6088.8174 - val_loss: 6889.6484\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6014.6602 - val_loss: 6408.6606\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6113.5107 - val_loss: 6702.5645\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6051.4053 - val_loss: 6929.4399\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6323.8716 - val_loss: 6843.6274\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6247.0376 - val_loss: 6688.5728\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6484.1523 - val_loss: 6761.4722\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6277.3730 - val_loss: 6669.4409\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6167.9194 - val_loss: 6560.9482\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6145.6694 - val_loss: 6586.0679\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6110.0122 - val_loss: 6537.0420\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6112.8828 - val_loss: 6703.3408\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6125.0557 - val_loss: 6999.7056\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6129.4707 - val_loss: 6455.7876\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5959.6367 - val_loss: 6652.2095\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6172.9243 - val_loss: 6562.6919\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6105.5278 - val_loss: 6579.2812\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5994.5830 - val_loss: 6633.9326\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6142.1733 - val_loss: 7242.1396\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6211.1567 - val_loss: 6966.5498\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6104.5259 - val_loss: 6510.8687\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5958.9912 - val_loss: 6773.7700\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5969.2700 - val_loss: 6404.3169\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6020.9473 - val_loss: 6665.0347\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5998.4844 - val_loss: 6610.0801\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6096.8027 - val_loss: 6823.0029\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6004.8433 - val_loss: 6438.9282\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6048.5117 - val_loss: 6828.2153\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6445.4907 - val_loss: 6719.1392\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6234.9058 - val_loss: 6650.6162\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6135.4497 - val_loss: 7131.1665\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5995.3857 - val_loss: 6609.4819\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5840.9258 - val_loss: 6955.7363\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6015.1030 - val_loss: 6863.3931\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6028.5728 - val_loss: 6621.7124\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6179.2393 - val_loss: 6542.1646\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5973.0264 - val_loss: 6777.0811\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5943.0435 - val_loss: 6912.2090\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6020.2051 - val_loss: 6549.5210\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6021.7944 - val_loss: 6703.6113\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5841.9717 - val_loss: 6546.7534\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5930.6826 - val_loss: 6739.4922\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6009.7935 - val_loss: 6602.8687\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6274.6177 - val_loss: 7642.0698\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6258.3062 - val_loss: 6676.8306\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6003.2217 - val_loss: 6596.0186\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5969.6230 - val_loss: 6770.6411\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5936.7295 - val_loss: 6574.1187\n",
      "Epoch 192/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6056.7490 - val_loss: 7102.3188\n",
      "Epoch 193/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6126.7432 - val_loss: 6458.8384\n",
      "Epoch 194/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5917.3950 - val_loss: 6880.4517\n",
      "Epoch 195/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5930.9824 - val_loss: 6953.5688\n",
      "Epoch 196/300\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 5851.2603\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6072.3516 - val_loss: 7208.7388\n",
      "Epoch 197/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5924.7310 - val_loss: 6451.5020\n",
      "Epoch 198/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5578.8667 - val_loss: 6510.2007\n",
      "Epoch 199/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5791.8604 - val_loss: 6615.6738\n",
      "Epoch 200/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5822.2852 - val_loss: 6422.7930\n",
      "Epoch 201/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5519.0298 - val_loss: 6495.4819\n",
      "Epoch 202/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5800.9824 - val_loss: 6395.2285\n",
      "Epoch 203/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5619.2554 - val_loss: 6500.6865\n",
      "Epoch 204/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5828.4238 - val_loss: 6448.6343\n",
      "Epoch 205/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5793.9380 - val_loss: 6582.5337\n",
      "Epoch 206/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5777.0474 - val_loss: 6659.9375\n",
      "Epoch 207/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5673.6123 - val_loss: 6694.5464\n",
      "Epoch 208/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5621.2627 - val_loss: 6770.6421\n",
      "Epoch 209/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5677.9609 - val_loss: 6434.7817\n",
      "Epoch 210/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5653.9287 - val_loss: 6700.9663\n",
      "Epoch 211/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5657.8081 - val_loss: 6698.1797\n",
      "Epoch 212/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5558.8315 - val_loss: 6569.0850\n",
      "Epoch 213/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5546.2979 - val_loss: 6457.1055\n",
      "Epoch 214/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5602.2295 - val_loss: 6586.7700\n",
      "Epoch 215/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5521.4575 - val_loss: 6876.3955\n",
      "Epoch 216/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5672.6421 - val_loss: 6987.2915\n",
      "Epoch 217/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5559.6084 - val_loss: 6667.4316\n",
      "Epoch 218/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5756.2285 - val_loss: 6822.1592\n",
      "Epoch 219/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5519.9277 - val_loss: 6754.6670\n",
      "Epoch 220/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5828.0386 - val_loss: 6739.1318\n",
      "Epoch 221/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5465.2397 - val_loss: 6580.7100\n",
      "Epoch 222/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5725.0771 - val_loss: 6853.5708\n",
      "Epoch 223/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5601.9653 - val_loss: 6983.6587\n",
      "Epoch 224/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5567.7168 - val_loss: 6649.6206\n",
      "Epoch 225/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5553.2339 - val_loss: 6444.5605\n",
      "Epoch 226/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5630.2383 - val_loss: 6771.2290\n",
      "Epoch 227/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5702.2622 - val_loss: 6482.0049\n",
      "Epoch 228/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5491.3296 - val_loss: 6706.5825\n",
      "Epoch 229/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5718.7686 - val_loss: 6683.4268\n",
      "Epoch 230/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5661.1958 - val_loss: 6650.7954\n",
      "Epoch 231/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5659.5894 - val_loss: 6534.0420\n",
      "Epoch 232/300\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 5506.3062\n",
      "Epoch 00232: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5571.8618 - val_loss: 6614.6904\n",
      "Epoch 233/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5383.9111 - val_loss: 6482.4390\n",
      "Epoch 234/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5497.2065 - val_loss: 6723.8076\n",
      "Epoch 235/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5257.7490 - val_loss: 7054.4980\n",
      "Epoch 236/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5436.1479 - val_loss: 6847.9712\n",
      "Epoch 237/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5480.8105 - val_loss: 6352.1523\n",
      "Epoch 238/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5304.7578 - val_loss: 6641.3691\n",
      "Epoch 239/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5216.5967 - val_loss: 6478.3335\n",
      "Epoch 240/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5489.5386 - val_loss: 6861.4819\n",
      "Epoch 241/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5380.0684 - val_loss: 6717.9741\n",
      "Epoch 242/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5417.0044 - val_loss: 6401.1572\n",
      "Epoch 243/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5478.6406 - val_loss: 6607.8008\n",
      "Epoch 244/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5343.9331 - val_loss: 6507.3545\n",
      "Epoch 245/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5274.6128 - val_loss: 6435.8604\n",
      "Epoch 246/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5393.7432 - val_loss: 6774.2275\n",
      "Epoch 247/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5472.8921 - val_loss: 6509.4302\n",
      "Epoch 248/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5666.5991 - val_loss: 6457.0122\n",
      "Epoch 249/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5587.8247 - val_loss: 6657.2031\n",
      "Epoch 250/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5448.4570 - val_loss: 6481.3799\n",
      "Epoch 251/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5470.8516 - val_loss: 6458.5679\n",
      "Epoch 252/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5221.3848 - val_loss: 6329.8096\n",
      "Epoch 253/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5388.7256 - val_loss: 6562.7627\n",
      "Epoch 254/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5300.7607 - val_loss: 6921.0674\n",
      "Epoch 255/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5485.9141 - val_loss: 6424.1982\n",
      "Epoch 256/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5313.3003 - val_loss: 6561.5308\n",
      "Epoch 257/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5348.2207 - val_loss: 6507.0547\n",
      "Epoch 258/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5156.2339 - val_loss: 6311.8418\n",
      "Epoch 259/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5442.8857 - val_loss: 6398.0752\n",
      "Epoch 260/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5342.8354 - val_loss: 6944.5820\n",
      "Epoch 261/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5375.8896 - val_loss: 6543.4165\n",
      "Epoch 262/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5336.4590 - val_loss: 6421.6704\n",
      "Epoch 263/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5280.6797 - val_loss: 6530.5850\n",
      "Epoch 264/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5335.9927 - val_loss: 6870.8809\n",
      "Epoch 265/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5812.5220 - val_loss: 6637.0322\n",
      "Epoch 266/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5422.8906 - val_loss: 6673.1240\n",
      "Epoch 267/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5393.3262 - val_loss: 6632.8071\n",
      "Epoch 268/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5389.2510 - val_loss: 6412.9468\n",
      "Epoch 269/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5425.4941 - val_loss: 6536.7168\n",
      "Epoch 270/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5332.2563 - val_loss: 6478.8208\n",
      "Epoch 271/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5215.5889 - val_loss: 6321.2651\n",
      "Epoch 272/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5189.1938 - val_loss: 6620.0034\n",
      "Epoch 273/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5384.4600 - val_loss: 6480.5645\n",
      "Epoch 274/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5422.2207 - val_loss: 6523.2100\n",
      "Epoch 275/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5522.5518 - val_loss: 6759.7319\n",
      "Epoch 276/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5303.6323 - val_loss: 6552.4683\n",
      "Epoch 277/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5051.9766 - val_loss: 6350.6104\n",
      "Epoch 278/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5366.1616 - val_loss: 6683.4854\n",
      "Epoch 279/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5308.2686 - val_loss: 6467.2886\n",
      "Epoch 280/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5247.1011 - val_loss: 6763.0640\n",
      "Epoch 281/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5314.7871 - val_loss: 6347.4604\n",
      "Epoch 282/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5499.8887 - val_loss: 6567.5649\n",
      "Epoch 283/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5372.4878 - val_loss: 6514.8364\n",
      "Epoch 284/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5241.2510 - val_loss: 6513.2095\n",
      "Epoch 285/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5429.8037 - val_loss: 6547.1924\n",
      "Epoch 286/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5238.1392 - val_loss: 6493.7681\n",
      "Epoch 287/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5136.3252 - val_loss: 6789.7866\n",
      "Epoch 288/300\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 5436.5859\n",
      "Epoch 00288: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5380.0386 - val_loss: 6609.0591\n",
      "Epoch 289/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5151.5410 - val_loss: 6796.6587\n",
      "Epoch 290/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5197.5186 - val_loss: 6751.7104\n",
      "Epoch 291/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5236.9487 - val_loss: 6609.9316\n",
      "Epoch 292/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5067.6343 - val_loss: 6533.8447\n",
      "Epoch 293/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5205.8159 - val_loss: 6575.2134\n",
      "Epoch 294/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5210.5317 - val_loss: 6828.4902\n",
      "Epoch 295/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5118.0947 - val_loss: 6614.1240\n",
      "Epoch 296/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5156.1157 - val_loss: 6552.4043\n",
      "Epoch 297/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5180.7085 - val_loss: 6620.5498\n",
      "Epoch 298/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5157.1587 - val_loss: 6751.6880\n",
      "Epoch 299/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5135.2144 - val_loss: 6541.9878\n",
      "Epoch 300/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5315.9536 - val_loss: 6773.3433\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12533.6338 - val_loss: 9358.3477\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9963.7578 - val_loss: 8892.2598\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9437.4893 - val_loss: 7926.0190\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9502.5791 - val_loss: 7998.7837\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9247.7266 - val_loss: 7767.7378\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8885.6357 - val_loss: 7832.5137\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8881.2402 - val_loss: 7663.3228\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8821.7100 - val_loss: 7564.1616\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8773.6006 - val_loss: 7540.7407\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8648.7930 - val_loss: 7689.2334\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8643.9561 - val_loss: 7408.2275\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8546.2822 - val_loss: 7711.2573\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8358.8115 - val_loss: 7414.6929\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8490.0088 - val_loss: 7408.8906\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8421.5137 - val_loss: 7186.4321\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8227.0273 - val_loss: 7674.5078\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8324.9150 - val_loss: 7293.2075\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8290.5166 - val_loss: 7376.0713\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8290.4131 - val_loss: 7426.2510\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8116.5693 - val_loss: 7521.1748\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8057.8594 - val_loss: 7406.0234\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8223.0840 - val_loss: 7893.1191\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8144.0000 - val_loss: 7589.0171\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8087.4551 - val_loss: 7520.0220\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8050.7705 - val_loss: 7344.1001\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7998.0898 - val_loss: 7985.6182\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8077.4702 - val_loss: 7557.9766\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7998.5952 - val_loss: 7230.0049\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7927.3511 - val_loss: 7187.4951\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7673.5786 - val_loss: 7336.7251\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7940.1465 - val_loss: 7719.7495\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7886.9951 - val_loss: 7171.4727\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7689.2876 - val_loss: 7587.8770\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7812.8730 - val_loss: 7563.6836\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7620.3628 - val_loss: 7155.7510\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7787.6812 - val_loss: 7713.6670\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7925.3198 - val_loss: 7394.8584\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7617.9980 - val_loss: 7192.8779\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7522.0381 - val_loss: 7802.9741\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7433.5547 - val_loss: 7243.0210\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7471.0049 - val_loss: 7552.3345\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7537.0376 - val_loss: 7159.2300\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7320.1733 - val_loss: 7562.9971\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7531.9741 - val_loss: 7296.6455\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7243.0273 - val_loss: 7164.0938\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7312.9097 - val_loss: 7117.9580\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7399.9277 - val_loss: 7268.5229\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7423.6626 - val_loss: 7620.6748\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7294.5908 - val_loss: 7169.6357\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7419.6045 - val_loss: 7282.9067\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7132.8467 - val_loss: 7183.3911\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7367.7183 - val_loss: 7822.4829\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7358.9893 - val_loss: 7437.1631\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7095.9609 - val_loss: 7271.8887\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7183.4399 - val_loss: 7197.3979\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7257.1606 - val_loss: 7288.1357\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7321.1089 - val_loss: 7167.2153\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7242.2847 - val_loss: 7338.8477\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7149.0049 - val_loss: 7382.9800\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7123.3628 - val_loss: 7192.0059\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6952.8379 - val_loss: 6805.8071\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7051.5132 - val_loss: 7129.7881\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7226.6157 - val_loss: 7242.5103\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7035.9233 - val_loss: 7766.5713\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7182.3589 - val_loss: 7286.7310\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6902.9419 - val_loss: 7932.8174\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6928.5317 - val_loss: 7337.1357\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6995.8721 - val_loss: 7000.7915\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7105.8628 - val_loss: 7016.9272\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7291.1797 - val_loss: 7111.4414\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7036.1265 - val_loss: 7051.6030\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7065.3130 - val_loss: 7173.9941\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7023.0679 - val_loss: 7796.2329\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7050.8325 - val_loss: 7305.4775\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6926.2769 - val_loss: 6995.9487\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7046.3872 - val_loss: 7158.9414\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6827.5312 - val_loss: 7109.5708\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6960.2876 - val_loss: 6697.8340\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6812.4922 - val_loss: 7313.2661\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6941.2949 - val_loss: 7392.6733\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6853.5850 - val_loss: 7677.7554\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6741.7646 - val_loss: 7385.9526\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6809.3232 - val_loss: 7368.6396\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6960.1069 - val_loss: 7060.2148\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6808.1782 - val_loss: 6878.8662\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6797.0254 - val_loss: 6816.4468\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6715.7832 - val_loss: 7270.9888\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6950.6426 - val_loss: 8068.8716\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6850.3579 - val_loss: 6576.6885\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6899.7812 - val_loss: 7158.4883\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6738.1060 - val_loss: 7783.4399\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6628.8105 - val_loss: 7259.8335\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6852.4619 - val_loss: 7758.4707\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6733.6641 - val_loss: 8085.4033\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6683.7393 - val_loss: 7460.5522\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6797.6924 - val_loss: 7127.4917\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6891.0459 - val_loss: 7275.8076\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6674.1826 - val_loss: 6779.7134\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6817.0117 - val_loss: 6926.6641\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6462.2002 - val_loss: 7042.6870\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6748.1919 - val_loss: 7065.8599\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6587.5845 - val_loss: 6605.5962\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6573.7476 - val_loss: 6826.9902\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6591.3667 - val_loss: 6728.4907\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6467.7017 - val_loss: 6970.6162\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6563.6777 - val_loss: 7797.7080\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6703.2832 - val_loss: 7146.5200\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6452.6436 - val_loss: 6850.6030\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6695.7720 - val_loss: 6914.3950\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6727.2388 - val_loss: 7025.9438\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6457.4907 - val_loss: 6886.8887\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6486.6929 - val_loss: 6774.4839\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6672.5581 - val_loss: 7327.5352\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6719.7842 - val_loss: 7377.9834\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6587.9370 - val_loss: 7276.8955\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6497.9512 - val_loss: 7358.6694\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6729.7666 - val_loss: 7658.4189\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6700.5781 - val_loss: 7674.2300\n",
      "Epoch 119/300\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 6498.3164\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6478.6167 - val_loss: 7603.7471\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6475.6396 - val_loss: 6958.1494\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6250.9189 - val_loss: 7248.9878\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6116.5830 - val_loss: 7221.4561\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6110.3184 - val_loss: 6669.1436\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6440.0483 - val_loss: 7188.3350\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6431.7998 - val_loss: 7200.3369\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6287.0439 - val_loss: 7566.7417\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6216.0649 - val_loss: 7817.4980\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6152.7593 - val_loss: 7696.8828\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6258.7388 - val_loss: 7399.5576\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6382.4072 - val_loss: 7190.8276\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6090.4351 - val_loss: 7271.8652\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6222.4922 - val_loss: 6647.9985\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6331.3101 - val_loss: 6940.5645\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6350.9585 - val_loss: 7050.9448\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6092.6890 - val_loss: 7816.6763\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6152.2822 - val_loss: 6883.5518\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6221.8555 - val_loss: 7112.8711\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6052.7349 - val_loss: 7701.7256\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6006.4189 - val_loss: 6923.5400\n",
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 12426.7939 - val_loss: 8873.8350\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9826.4463 - val_loss: 8346.5303\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9419.3477 - val_loss: 7918.0254\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9001.7764 - val_loss: 7979.6948\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9110.4463 - val_loss: 7947.6353\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8814.3086 - val_loss: 7651.7207\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8898.1973 - val_loss: 7704.7070\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8562.3408 - val_loss: 7640.6611\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8466.6826 - val_loss: 7766.1030\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8423.6943 - val_loss: 7461.3154\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8554.3594 - val_loss: 7392.1011\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8559.3535 - val_loss: 7591.1597\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8295.9033 - val_loss: 7373.6509\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8301.6621 - val_loss: 7368.4912\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8193.3877 - val_loss: 7353.3599\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8207.5293 - val_loss: 7933.2456\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8180.8350 - val_loss: 7467.3477\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8029.4658 - val_loss: 7376.5762\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8055.8267 - val_loss: 7347.8193\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7891.7817 - val_loss: 7198.0146\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8089.3745 - val_loss: 7631.8955\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8051.5498 - val_loss: 7537.0459\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7889.9302 - val_loss: 7602.7495\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7945.7031 - val_loss: 7900.1592\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7714.8350 - val_loss: 7959.2690\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7948.4146 - val_loss: 7206.4087\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7726.6953 - val_loss: 7686.4307\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7746.2100 - val_loss: 7101.1523\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7687.8135 - val_loss: 7533.7173\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7674.6831 - val_loss: 7688.6704\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7744.7998 - val_loss: 7956.5503\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7501.7002 - val_loss: 7332.0977\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7734.2612 - val_loss: 8351.4404\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7647.3599 - val_loss: 7418.5015\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7705.2969 - val_loss: 8290.7832\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7621.2212 - val_loss: 7244.7227\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7608.7529 - val_loss: 7015.5996\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7409.2812 - val_loss: 7108.2925\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7404.6797 - val_loss: 7064.8174\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7662.7676 - val_loss: 7081.2290\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7379.5747 - val_loss: 7192.4058\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7534.4839 - val_loss: 8083.4678\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7651.5923 - val_loss: 8641.1494\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7464.3794 - val_loss: 7474.4473\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7304.9346 - val_loss: 7551.2886\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7500.0430 - val_loss: 7514.8506\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7288.6514 - val_loss: 7430.1455\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7342.2876 - val_loss: 7295.0566\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7114.3330 - val_loss: 7402.5684\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7078.5186 - val_loss: 7368.2280\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7299.5015 - val_loss: 7310.9204\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7311.2886 - val_loss: 7099.7046\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7580.2124 - val_loss: 7245.4551\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7041.1230 - val_loss: 7645.9253\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7264.8691 - val_loss: 7226.3428\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7192.0049 - val_loss: 7309.1924\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7106.5557 - val_loss: 7200.1987\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6987.0552 - val_loss: 7131.5518\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7112.0142 - val_loss: 7985.8579\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7033.1416 - val_loss: 6826.9585\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7090.3027 - val_loss: 7184.7666\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7115.4092 - val_loss: 7219.4912\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7006.0688 - val_loss: 7376.0347\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7415.6763 - val_loss: 7712.0596\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7233.6934 - val_loss: 7121.5874\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7016.3359 - val_loss: 7161.9121\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7178.4795 - val_loss: 7963.1768\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7031.9795 - val_loss: 7699.5674\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6995.7339 - val_loss: 7658.4390\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6955.5151 - val_loss: 7042.1943\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6861.1353 - val_loss: 7261.1812\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6886.5542 - val_loss: 7671.3701\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7085.5967 - val_loss: 7105.4517\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6945.3447 - val_loss: 7246.0215\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6954.1665 - val_loss: 7179.7368\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6877.6968 - val_loss: 7367.2539\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6900.0693 - val_loss: 7168.0488\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6834.0405 - val_loss: 7468.5659\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6879.2007 - val_loss: 7072.2461\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6853.2007 - val_loss: 7170.5796\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7257.4878 - val_loss: 6859.4282\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6741.0425 - val_loss: 7455.1821\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6843.0811 - val_loss: 7622.0454\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6951.6865 - val_loss: 7214.6719\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6952.8975 - val_loss: 7965.5728\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6846.9014 - val_loss: 7411.2876\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6936.4155 - val_loss: 7001.4268\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6670.3306 - val_loss: 7484.4341\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6680.6499 - val_loss: 7313.7158\n",
      "Epoch 90/300\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 6741.7212\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6818.5645 - val_loss: 6988.5908\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6680.7686 - val_loss: 6955.5469\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6515.6772 - val_loss: 7492.2236\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6502.6699 - val_loss: 7381.2119\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6451.8623 - val_loss: 7206.6372\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6523.0972 - val_loss: 7386.8745\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6426.7178 - val_loss: 7196.8545\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6656.0215 - val_loss: 7758.4521\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6360.6216 - val_loss: 6921.2266\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6328.2310 - val_loss: 7078.5933\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6504.3062 - val_loss: 6988.9526\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6469.5942 - val_loss: 7505.0630\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6509.9712 - val_loss: 7187.9673\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6403.3550 - val_loss: 7000.2632\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6341.7036 - val_loss: 7338.6875\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6435.5977 - val_loss: 7261.5581\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6402.0913 - val_loss: 7621.3403\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6257.4844 - val_loss: 6926.7607\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6333.0444 - val_loss: 7565.4062\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6478.7363 - val_loss: 6932.9688\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6242.2690 - val_loss: 8181.2632\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "classical_list = []\n",
    "while i<5:\n",
    "    MICS_model = get_MICS_model(size_inp_a, size_inp_b, size_inp_c, use_encoders = True, drop_out = 0.25)\n",
    "    callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "            keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                             verbose = 2, mode = \"auto\", \n",
    "                                              min_lr = 1e-6)]\n",
    "    MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "    history = MICS_model.fit(x = [group_A_train_x_op.values, group_B_train_x_op.values, group_C_train_x_op.values], y = trainy.values,  \n",
    "                             validation_data = ([group_A_test_x_op.values, group_B_test_x_op.values, group_C_test_x_op.values], testy.values),\n",
    "                             epochs=300, batch_size = 300, callbacks=callback)\n",
    "    training_val_loss = history.history[\"val_loss\"]\n",
    "    best_row_index = np.argmin(training_val_loss)\n",
    "    best_val_loss = training_val_loss[best_row_index]\n",
    "    classical_list.append(best_val_loss)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6367.75634765625,\n",
       " 6460.544921875,\n",
       " 6311.841796875,\n",
       " 6576.6884765625,\n",
       " 6826.95849609375]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classical_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508.7580078125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classical_avg = sum(classical_list)/len(classical_list)\n",
    "classical_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
