{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\"):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: label column is in the end and all other columns are features. There is no index column in the data.\n",
    "    def get_raw_data(self):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=None)\n",
    "        raw_data = raw_data.fillna(df.mean())\n",
    "        self.raw_data = raw_data\n",
    "                \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #Here, new sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "                global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_inp)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=\"relu\")(h)    \n",
    "        return keras.Model(inputs=[inputs_A, inputs_B, inputs_C], outputs = outputs) \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def var_lis(lis):\n",
    "    for i in range(len(lis)):\n",
    "        print(lis[i])\n",
    "var_lis([1,4,6,8])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 9, None, None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [None]*8\n",
    "b = [3,7,9]\n",
    "for i in range(len(b)):\n",
    "    a[i] = b[i]\n",
    "a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
