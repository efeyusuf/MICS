{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 3):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self, index_col=0):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.8*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.8*row_num), 0]\n",
    "        testx_df = self.raw_data.iloc[int(0.8*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.8*row_num):, 0]        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(2)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_scaled.iloc[:,group]\n",
    "            features_and_labels[0][index] = train_temp.values\n",
    "            test_temp = textx_scaled.iloc[:,group]\n",
    "            features_and_labels[1][index] = test_temp.values            \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = testy_df.values   \n",
    "        return features_and_labels\n",
    "    \n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32, activation=\"relu\"):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=activation)(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "        \n",
    "    def default_exp_house(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes, activation=\"sigmoid\")\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        \n",
    "        training_acc = history.history[\"val_accuracy\"]\n",
    "        best_row_index_acc = np.argmax(training_acc)\n",
    "        best_val_acc = training_acc[best_row_index_acc]        \n",
    "        print(\"best val loss is: \" + str(best_val_loss))\n",
    "        print(\"best val accuracy is: \" + str(best_val_acc))\n",
    "        return best_val_acc\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 13109.8086 - val_loss: 9614.2871\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10177.5186 - val_loss: 8459.7764\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9770.9971 - val_loss: 8177.3047\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9751.5273 - val_loss: 8073.0054\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9428.1650 - val_loss: 8623.4980\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9315.1943 - val_loss: 8077.5474\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9261.9297 - val_loss: 7736.1147\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9189.3506 - val_loss: 8135.8569\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9148.8760 - val_loss: 7741.5586\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9112.2588 - val_loss: 7765.2192\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8980.0059 - val_loss: 7630.4375\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8958.3916 - val_loss: 7838.0459\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8894.2354 - val_loss: 7784.7939\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8807.6777 - val_loss: 7545.9668\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8886.5850 - val_loss: 7520.9995\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8815.8389 - val_loss: 8196.1377\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8834.1895 - val_loss: 8231.6904\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8899.0332 - val_loss: 7688.3682\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8778.8564 - val_loss: 8040.4336\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8778.7617 - val_loss: 7826.0693\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8607.6836 - val_loss: 7445.9258\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8795.8662 - val_loss: 8157.2886\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8614.2500 - val_loss: 7601.0996\n",
      "Epoch 24/300\n",
      " 1/53 [..............................] - ETA: 0s - loss: 9504.1748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f5279c582abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdeneyelim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMics_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeneyelim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdeneyelim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b0319ac46ed5>\u001b[0m in \u001b[0;36mdefault_exp\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m                                                   min_lr = 1e-6)]\n\u001b[1;32m    151\u001b[0m         \u001b[0mMICS_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n\u001b[0m\u001b[1;32m    153\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures_and_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_and_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                                  epochs=300, batch_size = batch_size, callbacks=callback)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=False, group_number=3)\n",
    "deneyelim.get_raw_data()\n",
    "deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6393 - accuracy: 0.7774 - val_loss: 0.3689 - val_accuracy: 0.9007\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8622 - val_loss: 0.3326 - val_accuracy: 0.8973\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8724 - val_loss: 0.3222 - val_accuracy: 0.8973\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8673 - val_loss: 0.3034 - val_accuracy: 0.8836\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3844 - accuracy: 0.8836 - val_loss: 0.3006 - val_accuracy: 0.8904\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8801 - val_loss: 0.2985 - val_accuracy: 0.9041\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8896 - val_loss: 0.3006 - val_accuracy: 0.8801\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8870 - val_loss: 0.2773 - val_accuracy: 0.8870\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8861 - val_loss: 0.3473 - val_accuracy: 0.8801\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.9033 - val_loss: 0.3229 - val_accuracy: 0.8938\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8947 - val_loss: 0.3051 - val_accuracy: 0.8904\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.9033 - val_loss: 0.2795 - val_accuracy: 0.8904\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8887 - val_loss: 0.3471 - val_accuracy: 0.8973\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8844 - val_loss: 0.2873 - val_accuracy: 0.8904\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8955 - val_loss: 0.2713 - val_accuracy: 0.8973\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.9067 - val_loss: 0.2775 - val_accuracy: 0.9041\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.9015 - val_loss: 0.2922 - val_accuracy: 0.8836\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8930 - val_loss: 0.3480 - val_accuracy: 0.9007\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8844 - val_loss: 0.2970 - val_accuracy: 0.8938\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8913 - val_loss: 0.3320 - val_accuracy: 0.8870\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8913 - val_loss: 0.2855 - val_accuracy: 0.8973\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8861 - val_loss: 0.3177 - val_accuracy: 0.8801\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.9033 - val_loss: 0.3125 - val_accuracy: 0.8801\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.9033 - val_loss: 0.2689 - val_accuracy: 0.8904\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8973 - val_loss: 0.4060 - val_accuracy: 0.8904\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8973 - val_loss: 0.2805 - val_accuracy: 0.9041\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8998 - val_loss: 0.2863 - val_accuracy: 0.8904\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8947 - val_loss: 0.2884 - val_accuracy: 0.8767\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.9007 - val_loss: 0.2653 - val_accuracy: 0.9007\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8887 - val_loss: 0.3073 - val_accuracy: 0.8870\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8844 - val_loss: 0.2778 - val_accuracy: 0.9075\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8938 - val_loss: 0.3708 - val_accuracy: 0.8801\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8947 - val_loss: 0.2689 - val_accuracy: 0.8973\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8998 - val_loss: 0.2787 - val_accuracy: 0.8904\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.9033 - val_loss: 0.2826 - val_accuracy: 0.9041\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8707 - val_loss: 0.2945 - val_accuracy: 0.9007\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8818 - val_loss: 0.2809 - val_accuracy: 0.8938\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8861 - val_loss: 0.3295 - val_accuracy: 0.8973\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8904 - val_loss: 0.3181 - val_accuracy: 0.8973\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.9024 - val_loss: 0.2613 - val_accuracy: 0.8973\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9007 - val_loss: 0.2881 - val_accuracy: 0.8904\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8844 - val_loss: 0.3191 - val_accuracy: 0.9007\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.9050 - val_loss: 0.2855 - val_accuracy: 0.8904\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8998 - val_loss: 0.2937 - val_accuracy: 0.8870\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.9007 - val_loss: 0.3034 - val_accuracy: 0.8938\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.9067 - val_loss: 0.3155 - val_accuracy: 0.8870\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8990 - val_loss: 0.2890 - val_accuracy: 0.8904\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8904 - val_loss: 0.2793 - val_accuracy: 0.8938\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2809 - accuracy: 0.8973 - val_loss: 0.2866 - val_accuracy: 0.9007\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8998 - val_loss: 0.2932 - val_accuracy: 0.8938\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8947 - val_loss: 0.2826 - val_accuracy: 0.9041\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.8990 - val_loss: 0.2924 - val_accuracy: 0.8973\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8836 - val_loss: 0.2762 - val_accuracy: 0.9110\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.9092 - val_loss: 0.2898 - val_accuracy: 0.8973\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8870 - val_loss: 0.2859 - val_accuracy: 0.9007\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9033 - val_loss: 0.3443 - val_accuracy: 0.8973\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8990 - val_loss: 0.2711 - val_accuracy: 0.9144\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.9075 - val_loss: 0.2717 - val_accuracy: 0.9007\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8981 - val_loss: 0.2760 - val_accuracy: 0.9075\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9033 - val_loss: 0.2653 - val_accuracy: 0.9007\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.9015 - val_loss: 0.2613 - val_accuracy: 0.9041\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.8990 - val_loss: 0.2708 - val_accuracy: 0.9007\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.9033 - val_loss: 0.3010 - val_accuracy: 0.8904\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.9041 - val_loss: 0.2481 - val_accuracy: 0.9075\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8930 - val_loss: 0.3358 - val_accuracy: 0.9007\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.9118 - val_loss: 0.2882 - val_accuracy: 0.8870\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.9084 - val_loss: 0.3011 - val_accuracy: 0.8938\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.9101 - val_loss: 0.3521 - val_accuracy: 0.8630\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8947 - val_loss: 0.3171 - val_accuracy: 0.8870\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8981 - val_loss: 0.2657 - val_accuracy: 0.9110\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8964 - val_loss: 0.2848 - val_accuracy: 0.9007\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.9007 - val_loss: 0.2662 - val_accuracy: 0.9075\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.9067 - val_loss: 0.2759 - val_accuracy: 0.9007\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.9033 - val_loss: 0.2585 - val_accuracy: 0.9007\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8887 - val_loss: 0.3264 - val_accuracy: 0.9041\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8973 - val_loss: 0.2826 - val_accuracy: 0.9007\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.9041 - val_loss: 0.2808 - val_accuracy: 0.9007\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8904 - val_loss: 0.2740 - val_accuracy: 0.9075\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.9110 - val_loss: 0.2613 - val_accuracy: 0.9075\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.9050 - val_loss: 0.2795 - val_accuracy: 0.9075\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.9135 - val_loss: 0.2621 - val_accuracy: 0.9110\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.9050 - val_loss: 0.2709 - val_accuracy: 0.8904\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.9033 - val_loss: 0.2768 - val_accuracy: 0.8870\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.9024 - val_loss: 0.2840 - val_accuracy: 0.8870\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8981 - val_loss: 0.2651 - val_accuracy: 0.9075\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.9007 - val_loss: 0.2467 - val_accuracy: 0.9110\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.9015 - val_loss: 0.2867 - val_accuracy: 0.8973\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.9067 - val_loss: 0.2676 - val_accuracy: 0.9144\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9033 - val_loss: 0.2805 - val_accuracy: 0.9110\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.9007 - val_loss: 0.2822 - val_accuracy: 0.9110\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9101 - val_loss: 0.2687 - val_accuracy: 0.8973\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.9075 - val_loss: 0.3136 - val_accuracy: 0.8973\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8938 - val_loss: 0.2547 - val_accuracy: 0.9110\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.9050 - val_loss: 0.2683 - val_accuracy: 0.9110\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9067 - val_loss: 0.2574 - val_accuracy: 0.9075\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9033 - val_loss: 0.2706 - val_accuracy: 0.8870\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9144 - val_loss: 0.2604 - val_accuracy: 0.9110\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.9127 - val_loss: 0.2980 - val_accuracy: 0.9041\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8896 - val_loss: 0.2614 - val_accuracy: 0.9041\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.8981 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.9033 - val_loss: 0.2740 - val_accuracy: 0.9075\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.9033 - val_loss: 0.2993 - val_accuracy: 0.8904\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8973 - val_loss: 0.2781 - val_accuracy: 0.8973\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8930 - val_loss: 0.2566 - val_accuracy: 0.9110\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.9135 - val_loss: 0.2892 - val_accuracy: 0.8938\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.9050 - val_loss: 0.2511 - val_accuracy: 0.9041\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.9127 - val_loss: 0.2760 - val_accuracy: 0.9110\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.9118 - val_loss: 0.2766 - val_accuracy: 0.9075\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9041 - val_loss: 0.2660 - val_accuracy: 0.9041\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8998 - val_loss: 0.2659 - val_accuracy: 0.9075\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.9007 - val_loss: 0.2780 - val_accuracy: 0.9041\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.9110 - val_loss: 0.2756 - val_accuracy: 0.9007\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.9058 - val_loss: 0.2703 - val_accuracy: 0.9007\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.9161 - val_loss: 0.2958 - val_accuracy: 0.9075\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2809 - accuracy: 0.9007 - val_loss: 0.2731 - val_accuracy: 0.9007\n",
      "Epoch 116/300\n",
      "30/37 [=======================>......] - ETA: 0s - loss: 0.2711 - accuracy: 0.9146\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.9092 - val_loss: 0.3000 - val_accuracy: 0.8870\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9024 - val_loss: 0.2721 - val_accuracy: 0.9041\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.9101 - val_loss: 0.2740 - val_accuracy: 0.9041\n",
      "Epoch 119/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9092 - val_loss: 0.2941 - val_accuracy: 0.9007\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.9041 - val_loss: 0.2994 - val_accuracy: 0.9041\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.9127 - val_loss: 0.2725 - val_accuracy: 0.8938\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.9144 - val_loss: 0.3140 - val_accuracy: 0.8801\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9033 - val_loss: 0.2696 - val_accuracy: 0.9007\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8981 - val_loss: 0.2681 - val_accuracy: 0.9110\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.9127 - val_loss: 0.2715 - val_accuracy: 0.9075\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.9118 - val_loss: 0.2654 - val_accuracy: 0.9075\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.9041 - val_loss: 0.2785 - val_accuracy: 0.9110\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.9101 - val_loss: 0.2592 - val_accuracy: 0.9075\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.9127 - val_loss: 0.2561 - val_accuracy: 0.9075\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9178 - val_loss: 0.2743 - val_accuracy: 0.9110\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.9127 - val_loss: 0.2597 - val_accuracy: 0.9110\n",
      "Epoch 132/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.9092 - val_loss: 0.3112 - val_accuracy: 0.9041\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.9075 - val_loss: 0.2767 - val_accuracy: 0.8870\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8981 - val_loss: 0.2769 - val_accuracy: 0.9041\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9075 - val_loss: 0.2754 - val_accuracy: 0.9007\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.9092 - val_loss: 0.2637 - val_accuracy: 0.8973\n",
      "best val loss is: 0.2466609627008438\n",
      "best val accuracy is: 0.914383590221405\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.7021 - val_loss: 0.3772 - val_accuracy: 0.8562\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8716 - val_loss: 0.3117 - val_accuracy: 0.9075\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3674 - accuracy: 0.8878 - val_loss: 0.2955 - val_accuracy: 0.8973\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8836 - val_loss: 0.3042 - val_accuracy: 0.8973\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8861 - val_loss: 0.3169 - val_accuracy: 0.8938\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8930 - val_loss: 0.2941 - val_accuracy: 0.8904\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8955 - val_loss: 0.2855 - val_accuracy: 0.8904\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.9007 - val_loss: 0.2894 - val_accuracy: 0.8973\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8921 - val_loss: 0.2725 - val_accuracy: 0.8733\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8990 - val_loss: 0.3350 - val_accuracy: 0.8801\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8904 - val_loss: 0.2826 - val_accuracy: 0.8904\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.9007 - val_loss: 0.3022 - val_accuracy: 0.8973\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.9024 - val_loss: 0.2818 - val_accuracy: 0.8870\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8870 - val_loss: 0.2795 - val_accuracy: 0.8767\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.8930 - val_loss: 0.2867 - val_accuracy: 0.9041\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.8921 - val_loss: 0.2970 - val_accuracy: 0.8767\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.9007 - val_loss: 0.2919 - val_accuracy: 0.8767\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3173 - val_accuracy: 0.8836\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8913 - val_loss: 0.2687 - val_accuracy: 0.9007\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.8938 - val_loss: 0.2851 - val_accuracy: 0.8836\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.9041 - val_loss: 0.2744 - val_accuracy: 0.8938\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.8964 - val_loss: 0.2774 - val_accuracy: 0.8973\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8998 - val_loss: 0.3047 - val_accuracy: 0.8836\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.8973 - val_loss: 0.3106 - val_accuracy: 0.8938\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.9067 - val_loss: 0.2841 - val_accuracy: 0.9041\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8990 - val_loss: 0.2991 - val_accuracy: 0.8801\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2622 - accuracy: 0.9118 - val_loss: 0.2872 - val_accuracy: 0.8904\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.9058 - val_loss: 0.2773 - val_accuracy: 0.8904\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9033 - val_loss: 0.2963 - val_accuracy: 0.9041\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.9050 - val_loss: 0.2997 - val_accuracy: 0.8836\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.9024 - val_loss: 0.2827 - val_accuracy: 0.8973\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.9084 - val_loss: 0.3136 - val_accuracy: 0.8904\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8990 - val_loss: 0.2948 - val_accuracy: 0.8870\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9058 - val_loss: 0.2878 - val_accuracy: 0.8938\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.9024 - val_loss: 0.3152 - val_accuracy: 0.8973\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8973 - val_loss: 0.3182 - val_accuracy: 0.8904\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8998 - val_loss: 0.3112 - val_accuracy: 0.9041\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8930 - val_loss: 0.2858 - val_accuracy: 0.8904\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9058 - val_loss: 0.3149 - val_accuracy: 0.8836\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9007 - val_loss: 0.3150 - val_accuracy: 0.8973\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8930 - val_loss: 0.2806 - val_accuracy: 0.9041\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.9092 - val_loss: 0.3260 - val_accuracy: 0.8904\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.9015 - val_loss: 0.3018 - val_accuracy: 0.8870\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.9033 - val_loss: 0.3111 - val_accuracy: 0.8870\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.9015 - val_loss: 0.2746 - val_accuracy: 0.8904\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9075 - val_loss: 0.2758 - val_accuracy: 0.8904\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9118 - val_loss: 0.2838 - val_accuracy: 0.8938\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9092 - val_loss: 0.2924 - val_accuracy: 0.8664\n",
      "Epoch 49/300\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2610 - accuracy: 0.9102\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9092 - val_loss: 0.3599 - val_accuracy: 0.8973\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9127 - val_loss: 0.2933 - val_accuracy: 0.8904\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.9101 - val_loss: 0.2794 - val_accuracy: 0.8904\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9084 - val_loss: 0.3219 - val_accuracy: 0.8767\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.9135 - val_loss: 0.3129 - val_accuracy: 0.8938\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9033 - val_loss: 0.2796 - val_accuracy: 0.8801\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9135 - val_loss: 0.3081 - val_accuracy: 0.8904\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.9110 - val_loss: 0.2742 - val_accuracy: 0.8870\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8998 - val_loss: 0.2772 - val_accuracy: 0.8938\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.9204 - val_loss: 0.2891 - val_accuracy: 0.8870\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.9101 - val_loss: 0.2803 - val_accuracy: 0.9007\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.9101 - val_loss: 0.2855 - val_accuracy: 0.9007\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9110 - val_loss: 0.2837 - val_accuracy: 0.8904\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2708 - accuracy: 0.9101 - val_loss: 0.2769 - val_accuracy: 0.8904\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9110 - val_loss: 0.2822 - val_accuracy: 0.9007\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8998 - val_loss: 0.3043 - val_accuracy: 0.8973\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9007 - val_loss: 0.3156 - val_accuracy: 0.8973\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9024 - val_loss: 0.2822 - val_accuracy: 0.8870\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.8938 - val_loss: 0.3296 - val_accuracy: 0.8938\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.9144 - val_loss: 0.2907 - val_accuracy: 0.8904\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8981 - val_loss: 0.2825 - val_accuracy: 0.8973\n",
      "best val loss is: 0.2686607539653778\n",
      "best val accuracy is: 0.9075342416763306\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6445 - accuracy: 0.7568 - val_loss: 0.3770 - val_accuracy: 0.8699\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.8339 - val_loss: 0.3285 - val_accuracy: 0.8870\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8510 - val_loss: 0.3276 - val_accuracy: 0.8630\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8527 - val_loss: 0.3309 - val_accuracy: 0.8801\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8553 - val_loss: 0.2991 - val_accuracy: 0.9075\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8690 - val_loss: 0.3000 - val_accuracy: 0.8904\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8647 - val_loss: 0.2899 - val_accuracy: 0.8938\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8784 - val_loss: 0.2962 - val_accuracy: 0.8938\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3589 - accuracy: 0.8836 - val_loss: 0.2851 - val_accuracy: 0.8870\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8784 - val_loss: 0.2923 - val_accuracy: 0.8938\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8699 - val_loss: 0.2869 - val_accuracy: 0.8973\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8818 - val_loss: 0.2879 - val_accuracy: 0.8904\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8810 - val_loss: 0.2739 - val_accuracy: 0.9007\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8784 - val_loss: 0.2791 - val_accuracy: 0.9007\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8793 - val_loss: 0.2785 - val_accuracy: 0.8870\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8741 - val_loss: 0.3234 - val_accuracy: 0.8836\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8699 - val_loss: 0.2828 - val_accuracy: 0.8870\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8853 - val_loss: 0.3269 - val_accuracy: 0.8938\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8853 - val_loss: 0.2972 - val_accuracy: 0.8938\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8904 - val_loss: 0.2722 - val_accuracy: 0.8904\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8853 - val_loss: 0.2837 - val_accuracy: 0.8904\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8921 - val_loss: 0.3064 - val_accuracy: 0.8836\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8870 - val_loss: 0.2835 - val_accuracy: 0.8836\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8887 - val_loss: 0.2780 - val_accuracy: 0.8836\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8870 - val_loss: 0.2874 - val_accuracy: 0.8767\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8836 - val_loss: 0.2897 - val_accuracy: 0.8801\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8836 - val_loss: 0.2808 - val_accuracy: 0.8836\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8870 - val_loss: 0.3560 - val_accuracy: 0.8699\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8664 - val_loss: 0.3055 - val_accuracy: 0.8733\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8870 - val_loss: 0.2919 - val_accuracy: 0.8904\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8964 - val_loss: 0.3252 - val_accuracy: 0.8938\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8904 - val_loss: 0.3087 - val_accuracy: 0.8733\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8793 - val_loss: 0.2926 - val_accuracy: 0.8767\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8955 - val_loss: 0.2757 - val_accuracy: 0.9007\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8990 - val_loss: 0.2874 - val_accuracy: 0.8904\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8810 - val_loss: 0.2925 - val_accuracy: 0.8836\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3198 - accuracy: 0.8878 - val_loss: 0.2765 - val_accuracy: 0.8904\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8896 - val_loss: 0.3056 - val_accuracy: 0.8973\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3284 - accuracy: 0.8861 - val_loss: 0.2761 - val_accuracy: 0.8973\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8853 - val_loss: 0.2801 - val_accuracy: 0.8904\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8827 - val_loss: 0.2727 - val_accuracy: 0.9007\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8853 - val_loss: 0.2750 - val_accuracy: 0.8973\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8844 - val_loss: 0.2780 - val_accuracy: 0.8973\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8818 - val_loss: 0.2969 - val_accuracy: 0.8904\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8853 - val_loss: 0.2733 - val_accuracy: 0.9041\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8887 - val_loss: 0.2753 - val_accuracy: 0.9110\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3424 - accuracy: 0.8896 - val_loss: 0.2742 - val_accuracy: 0.8938\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8861 - val_loss: 0.3079 - val_accuracy: 0.8973\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8896 - val_loss: 0.2695 - val_accuracy: 0.8938\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8887 - val_loss: 0.2820 - val_accuracy: 0.8973\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8904 - val_loss: 0.2839 - val_accuracy: 0.8938\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8861 - val_loss: 0.3008 - val_accuracy: 0.8938\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8921 - val_loss: 0.2861 - val_accuracy: 0.8938\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8827 - val_loss: 0.2771 - val_accuracy: 0.8870\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8878 - val_loss: 0.2698 - val_accuracy: 0.9007\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8938 - val_loss: 0.2842 - val_accuracy: 0.8973\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8921 - val_loss: 0.3032 - val_accuracy: 0.8870\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8861 - val_loss: 0.2922 - val_accuracy: 0.8870\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8921 - val_loss: 0.2892 - val_accuracy: 0.8836\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8981 - val_loss: 0.2893 - val_accuracy: 0.8904\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8904 - val_loss: 0.3179 - val_accuracy: 0.8836\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.8913 - val_loss: 0.3333 - val_accuracy: 0.8904\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8913 - val_loss: 0.3025 - val_accuracy: 0.8836\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8964 - val_loss: 0.2772 - val_accuracy: 0.8938\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8955 - val_loss: 0.3095 - val_accuracy: 0.8801\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8938 - val_loss: 0.2739 - val_accuracy: 0.9007\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8870 - val_loss: 0.3092 - val_accuracy: 0.8767\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8861 - val_loss: 0.2777 - val_accuracy: 0.8938\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3181 - accuracy: 0.8887 - val_loss: 0.2776 - val_accuracy: 0.8904\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8938 - val_loss: 0.2837 - val_accuracy: 0.8870\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8818 - val_loss: 0.2789 - val_accuracy: 0.8904\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8930 - val_loss: 0.3104 - val_accuracy: 0.9041\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.8878 - val_loss: 0.3011 - val_accuracy: 0.8870\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8896 - val_loss: 0.2965 - val_accuracy: 0.8836\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8896 - val_loss: 0.2914 - val_accuracy: 0.8870\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8955 - val_loss: 0.2916 - val_accuracy: 0.8870\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8930 - val_loss: 0.2817 - val_accuracy: 0.8938\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8870 - val_loss: 0.2867 - val_accuracy: 0.8836\n",
      "Epoch 79/300\n",
      "27/37 [====================>.........] - ETA: 0s - loss: 0.3407 - accuracy: 0.8854\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8776 - val_loss: 0.2782 - val_accuracy: 0.8836\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8964 - val_loss: 0.2862 - val_accuracy: 0.8938\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8904 - val_loss: 0.3001 - val_accuracy: 0.8870\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8998 - val_loss: 0.2925 - val_accuracy: 0.8836\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8827 - val_loss: 0.3000 - val_accuracy: 0.9144\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8955 - val_loss: 0.2751 - val_accuracy: 0.8973\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8896 - val_loss: 0.2855 - val_accuracy: 0.8904\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8887 - val_loss: 0.2834 - val_accuracy: 0.8801\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8981 - val_loss: 0.2763 - val_accuracy: 0.8938\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.9007 - val_loss: 0.2859 - val_accuracy: 0.8904\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8913 - val_loss: 0.2835 - val_accuracy: 0.8904\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.9007 - val_loss: 0.2918 - val_accuracy: 0.8938\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8870 - val_loss: 0.2734 - val_accuracy: 0.9041\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8964 - val_loss: 0.2674 - val_accuracy: 0.9144\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.9067 - val_loss: 0.2862 - val_accuracy: 0.9007\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8938 - val_loss: 0.3125 - val_accuracy: 0.8630\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8896 - val_loss: 0.3013 - val_accuracy: 0.8973\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.8870 - val_loss: 0.3246 - val_accuracy: 0.8630\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8827 - val_loss: 0.2999 - val_accuracy: 0.8767\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8938 - val_loss: 0.2700 - val_accuracy: 0.9075\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8836 - val_loss: 0.3122 - val_accuracy: 0.8870\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8930 - val_loss: 0.2751 - val_accuracy: 0.8904\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.8964 - val_loss: 0.2783 - val_accuracy: 0.8973\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8878 - val_loss: 0.2605 - val_accuracy: 0.9041\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8938 - val_loss: 0.2806 - val_accuracy: 0.8836\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8973 - val_loss: 0.2752 - val_accuracy: 0.9075\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8964 - val_loss: 0.2802 - val_accuracy: 0.8938\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8938 - val_loss: 0.2917 - val_accuracy: 0.8938\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8930 - val_loss: 0.2712 - val_accuracy: 0.9007\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8913 - val_loss: 0.2798 - val_accuracy: 0.8904\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3000 - accuracy: 0.9015 - val_loss: 0.2675 - val_accuracy: 0.9075\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8938 - val_loss: 0.2780 - val_accuracy: 0.8904\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8981 - val_loss: 0.2788 - val_accuracy: 0.8870\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.9041 - val_loss: 0.2871 - val_accuracy: 0.8836\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8921 - val_loss: 0.3078 - val_accuracy: 0.8836\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8921 - val_loss: 0.2750 - val_accuracy: 0.9007\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8913 - val_loss: 0.2800 - val_accuracy: 0.8801\n",
      "Epoch 116/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.9015 - val_loss: 0.2728 - val_accuracy: 0.8973\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8844 - val_loss: 0.2777 - val_accuracy: 0.8938\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.8998 - val_loss: 0.2779 - val_accuracy: 0.8938\n",
      "Epoch 119/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3016 - accuracy: 0.9015 - val_loss: 0.2785 - val_accuracy: 0.8973\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.9007 - val_loss: 0.2782 - val_accuracy: 0.8973\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.9041 - val_loss: 0.2753 - val_accuracy: 0.8870\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.9007 - val_loss: 0.2870 - val_accuracy: 0.8973\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8896 - val_loss: 0.2861 - val_accuracy: 0.8870\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.8964 - val_loss: 0.2812 - val_accuracy: 0.8904\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8964 - val_loss: 0.2886 - val_accuracy: 0.8801\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8973 - val_loss: 0.2632 - val_accuracy: 0.8973\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8947 - val_loss: 0.2738 - val_accuracy: 0.8870\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.8930 - val_loss: 0.2869 - val_accuracy: 0.8904\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.9024 - val_loss: 0.3027 - val_accuracy: 0.8904\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8981 - val_loss: 0.2941 - val_accuracy: 0.8836\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8964 - val_loss: 0.2861 - val_accuracy: 0.8938\n",
      "Epoch 132/300\n",
      "28/37 [=====================>........] - ETA: 0s - loss: 0.2983 - accuracy: 0.9040\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8964 - val_loss: 0.2789 - val_accuracy: 0.8904\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8964 - val_loss: 0.2737 - val_accuracy: 0.9007\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8947 - val_loss: 0.2870 - val_accuracy: 0.8870\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.9024 - val_loss: 0.2838 - val_accuracy: 0.8973\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8998 - val_loss: 0.2702 - val_accuracy: 0.8870\n",
      "Epoch 137/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8913 - val_loss: 0.2923 - val_accuracy: 0.8870\n",
      "Epoch 138/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3099 - accuracy: 0.8921 - val_loss: 0.2769 - val_accuracy: 0.9007\n",
      "Epoch 139/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3046 - accuracy: 0.8955 - val_loss: 0.2766 - val_accuracy: 0.8938\n",
      "Epoch 140/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.9058 - val_loss: 0.2759 - val_accuracy: 0.8973\n",
      "Epoch 141/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.9033 - val_loss: 0.2796 - val_accuracy: 0.8870\n",
      "Epoch 142/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.8973 - val_loss: 0.2684 - val_accuracy: 0.8836\n",
      "Epoch 143/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8981 - val_loss: 0.2858 - val_accuracy: 0.8870\n",
      "Epoch 144/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8870 - val_loss: 0.2676 - val_accuracy: 0.8938\n",
      "Epoch 145/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.9084 - val_loss: 0.2816 - val_accuracy: 0.8904\n",
      "Epoch 146/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8998 - val_loss: 0.2773 - val_accuracy: 0.8904\n",
      "Epoch 147/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8947 - val_loss: 0.2711 - val_accuracy: 0.8973\n",
      "Epoch 148/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2988 - accuracy: 0.8947 - val_loss: 0.2781 - val_accuracy: 0.8870\n",
      "Epoch 149/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8930 - val_loss: 0.2784 - val_accuracy: 0.8938\n",
      "Epoch 150/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8921 - val_loss: 0.2889 - val_accuracy: 0.8836\n",
      "Epoch 151/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8990 - val_loss: 0.2748 - val_accuracy: 0.8973\n",
      "Epoch 152/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8981 - val_loss: 0.2738 - val_accuracy: 0.8836\n",
      "best val loss is: 0.2605009078979492\n",
      "best val accuracy is: 0.914383590221405\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6132 - accuracy: 0.7937 - val_loss: 0.3614 - val_accuracy: 0.8801\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.8784 - val_loss: 0.3207 - val_accuracy: 0.8938\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.8938 - val_loss: 0.3131 - val_accuracy: 0.8938\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8750 - val_loss: 0.3284 - val_accuracy: 0.8973\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8990 - val_loss: 0.2866 - val_accuracy: 0.8904\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8930 - val_loss: 0.2797 - val_accuracy: 0.9075\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8921 - val_loss: 0.2897 - val_accuracy: 0.8938\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8921 - val_loss: 0.2769 - val_accuracy: 0.9075\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8887 - val_loss: 0.2727 - val_accuracy: 0.9041\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8741 - val_loss: 0.3416 - val_accuracy: 0.9041\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8913 - val_loss: 0.2759 - val_accuracy: 0.8973\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8964 - val_loss: 0.2642 - val_accuracy: 0.9041\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8973 - val_loss: 0.2826 - val_accuracy: 0.9144\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.8853 - val_loss: 0.2781 - val_accuracy: 0.8973\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8921 - val_loss: 0.2796 - val_accuracy: 0.9041\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8990 - val_loss: 0.2742 - val_accuracy: 0.8904\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.9007 - val_loss: 0.2645 - val_accuracy: 0.9075\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8938 - val_loss: 0.2789 - val_accuracy: 0.9007\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8981 - val_loss: 0.2990 - val_accuracy: 0.8973\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3068 - accuracy: 0.9050 - val_loss: 0.2556 - val_accuracy: 0.9007\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8990 - val_loss: 0.2729 - val_accuracy: 0.8870\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8896 - val_loss: 0.2703 - val_accuracy: 0.8973\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8810 - val_loss: 0.2661 - val_accuracy: 0.8973\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8887 - val_loss: 0.3197 - val_accuracy: 0.8973\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8861 - val_loss: 0.2633 - val_accuracy: 0.8938\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3068 - accuracy: 0.8964 - val_loss: 0.2618 - val_accuracy: 0.9007\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8878 - val_loss: 0.2903 - val_accuracy: 0.9075\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.8947 - val_loss: 0.3141 - val_accuracy: 0.9041\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8955 - val_loss: 0.2709 - val_accuracy: 0.9007\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8904 - val_loss: 0.2973 - val_accuracy: 0.9041\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8947 - val_loss: 0.2700 - val_accuracy: 0.9075\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3035 - accuracy: 0.8981 - val_loss: 0.2627 - val_accuracy: 0.8938\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8930 - val_loss: 0.2816 - val_accuracy: 0.9007\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8964 - val_loss: 0.2728 - val_accuracy: 0.8973\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3046 - accuracy: 0.8947 - val_loss: 0.2945 - val_accuracy: 0.9007\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8973 - val_loss: 0.2847 - val_accuracy: 0.9041\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8955 - val_loss: 0.2711 - val_accuracy: 0.8938\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8990 - val_loss: 0.2777 - val_accuracy: 0.9007\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8990 - val_loss: 0.2922 - val_accuracy: 0.8870\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8887 - val_loss: 0.2776 - val_accuracy: 0.9110\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.9101 - val_loss: 0.2730 - val_accuracy: 0.8904\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8973 - val_loss: 0.2656 - val_accuracy: 0.9110\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8913 - val_loss: 0.2959 - val_accuracy: 0.9075\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3150 - accuracy: 0.8930 - val_loss: 0.2827 - val_accuracy: 0.9041\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8810 - val_loss: 0.2903 - val_accuracy: 0.8904\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8955 - val_loss: 0.2827 - val_accuracy: 0.9041\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8793 - val_loss: 0.2634 - val_accuracy: 0.8904\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8921 - val_loss: 0.2805 - val_accuracy: 0.9178\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.9092 - val_loss: 0.2658 - val_accuracy: 0.8904\n",
      "Epoch 50/300\n",
      "29/37 [======================>.......] - ETA: 0s - loss: 0.2795 - accuracy: 0.9052\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8930 - val_loss: 0.3275 - val_accuracy: 0.9041\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.8870 - val_loss: 0.2622 - val_accuracy: 0.8938\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8998 - val_loss: 0.2746 - val_accuracy: 0.9007\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.9007 - val_loss: 0.2922 - val_accuracy: 0.9144\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.8998 - val_loss: 0.2663 - val_accuracy: 0.9075\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8998 - val_loss: 0.2579 - val_accuracy: 0.9007\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8947 - val_loss: 0.2799 - val_accuracy: 0.9041\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.9041 - val_loss: 0.2732 - val_accuracy: 0.8973\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.9084 - val_loss: 0.2924 - val_accuracy: 0.8938\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2878 - accuracy: 0.8998 - val_loss: 0.3216 - val_accuracy: 0.8938\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8981 - val_loss: 0.2589 - val_accuracy: 0.8973\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.9007 - val_loss: 0.2628 - val_accuracy: 0.9041\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.9075 - val_loss: 0.2687 - val_accuracy: 0.8938\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8913 - val_loss: 0.2727 - val_accuracy: 0.8904\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8955 - val_loss: 0.2712 - val_accuracy: 0.8938\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.9041 - val_loss: 0.2757 - val_accuracy: 0.9144\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8973 - val_loss: 0.2602 - val_accuracy: 0.9041\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.9110 - val_loss: 0.2686 - val_accuracy: 0.9041\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.9041 - val_loss: 0.2832 - val_accuracy: 0.8973\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.9067 - val_loss: 0.2642 - val_accuracy: 0.8904\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.9024 - val_loss: 0.2859 - val_accuracy: 0.8904\n",
      "best val loss is: 0.2555866837501526\n",
      "best val accuracy is: 0.9178082346916199\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6017 - accuracy: 0.7791 - val_loss: 0.3491 - val_accuracy: 0.8973\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.8622 - val_loss: 0.3389 - val_accuracy: 0.9007\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4000 - accuracy: 0.8716 - val_loss: 0.2879 - val_accuracy: 0.9007\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8878 - val_loss: 0.2877 - val_accuracy: 0.9110\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8947 - val_loss: 0.3014 - val_accuracy: 0.9144\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.9041 - val_loss: 0.2690 - val_accuracy: 0.9007\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8836 - val_loss: 0.2950 - val_accuracy: 0.9144\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8990 - val_loss: 0.2613 - val_accuracy: 0.9212\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.8990 - val_loss: 0.2727 - val_accuracy: 0.9041\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8861 - val_loss: 0.2793 - val_accuracy: 0.9144\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.9007 - val_loss: 0.2563 - val_accuracy: 0.9178\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8913 - val_loss: 0.3311 - val_accuracy: 0.8938\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8921 - val_loss: 0.2842 - val_accuracy: 0.8973\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3221 - accuracy: 0.9058 - val_loss: 0.2648 - val_accuracy: 0.9110\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.9007 - val_loss: 0.3161 - val_accuracy: 0.8938\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.9033 - val_loss: 0.2643 - val_accuracy: 0.9075\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8973 - val_loss: 0.2546 - val_accuracy: 0.9110\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3020 - accuracy: 0.9007 - val_loss: 0.2637 - val_accuracy: 0.9144\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8938 - val_loss: 0.2585 - val_accuracy: 0.9178\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8887 - val_loss: 0.2955 - val_accuracy: 0.9007\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8973 - val_loss: 0.2660 - val_accuracy: 0.9178\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3077 - accuracy: 0.8947 - val_loss: 0.2794 - val_accuracy: 0.9110\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8896 - val_loss: 0.2641 - val_accuracy: 0.9144\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8973 - val_loss: 0.2652 - val_accuracy: 0.9110\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.9067 - val_loss: 0.2772 - val_accuracy: 0.9075\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8887 - val_loss: 0.3014 - val_accuracy: 0.8733\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8981 - val_loss: 0.2836 - val_accuracy: 0.9075\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8878 - val_loss: 0.2752 - val_accuracy: 0.9110\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8990 - val_loss: 0.2898 - val_accuracy: 0.9041\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.8921 - val_loss: 0.2601 - val_accuracy: 0.9110\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.9058 - val_loss: 0.2709 - val_accuracy: 0.9144\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8904 - val_loss: 0.3151 - val_accuracy: 0.8904\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.9058 - val_loss: 0.2751 - val_accuracy: 0.9110\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8981 - val_loss: 0.2684 - val_accuracy: 0.9110\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8990 - val_loss: 0.2893 - val_accuracy: 0.9075\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8938 - val_loss: 0.2690 - val_accuracy: 0.9144\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3013 - accuracy: 0.8921 - val_loss: 0.3026 - val_accuracy: 0.8938\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8955 - val_loss: 0.2679 - val_accuracy: 0.9178\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2979 - accuracy: 0.9024 - val_loss: 0.2687 - val_accuracy: 0.9110\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3129 - accuracy: 0.9015 - val_loss: 0.2819 - val_accuracy: 0.8973\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8973 - val_loss: 0.2743 - val_accuracy: 0.8938\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8947 - val_loss: 0.2685 - val_accuracy: 0.9178\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.9015 - val_loss: 0.2716 - val_accuracy: 0.9110\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.9015 - val_loss: 0.2682 - val_accuracy: 0.9075\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8947 - val_loss: 0.2686 - val_accuracy: 0.9110\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8938 - val_loss: 0.2853 - val_accuracy: 0.9007\n",
      "Epoch 47/300\n",
      "28/37 [=====================>........] - ETA: 0s - loss: 0.2967 - accuracy: 0.8984\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8955 - val_loss: 0.2709 - val_accuracy: 0.9075\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8973 - val_loss: 0.2707 - val_accuracy: 0.9247\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.9092 - val_loss: 0.2683 - val_accuracy: 0.9212\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.9110 - val_loss: 0.2633 - val_accuracy: 0.9212\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.9058 - val_loss: 0.2822 - val_accuracy: 0.9007\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.9050 - val_loss: 0.2908 - val_accuracy: 0.8870\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.9033 - val_loss: 0.2665 - val_accuracy: 0.9007\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3096 - accuracy: 0.9075 - val_loss: 0.2668 - val_accuracy: 0.9144\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8947 - val_loss: 0.2662 - val_accuracy: 0.9041\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2930 - accuracy: 0.9033 - val_loss: 0.2609 - val_accuracy: 0.9144\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.9033 - val_loss: 0.2627 - val_accuracy: 0.9178\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.9067 - val_loss: 0.2629 - val_accuracy: 0.9075\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.9024 - val_loss: 0.2855 - val_accuracy: 0.8870\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2984 - accuracy: 0.9050 - val_loss: 0.2665 - val_accuracy: 0.9041\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.9075 - val_loss: 0.2674 - val_accuracy: 0.9212\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.9007 - val_loss: 0.2802 - val_accuracy: 0.8973\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.9007 - val_loss: 0.2691 - val_accuracy: 0.9144\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.9075 - val_loss: 0.2726 - val_accuracy: 0.9075\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8973 - val_loss: 0.2768 - val_accuracy: 0.9110\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8998 - val_loss: 0.2883 - val_accuracy: 0.8904\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.9084 - val_loss: 0.2721 - val_accuracy: 0.9178\n",
      "best val loss is: 0.25455009937286377\n",
      "best val accuracy is: 0.9246575236320496\n"
     ]
    }
   ],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, sampling_method=\"Vanilla\", group_number=5)\n",
    "    deneyelim2.get_raw_data()\n",
    "    acc = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.7697 - val_loss: 0.3884 - val_accuracy: 0.8938\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8733 - val_loss: 0.3383 - val_accuracy: 0.9007\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8887 - val_loss: 0.3667 - val_accuracy: 0.8973\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8801 - val_loss: 0.3067 - val_accuracy: 0.8938\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8896 - val_loss: 0.2846 - val_accuracy: 0.8904\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8904 - val_loss: 0.2721 - val_accuracy: 0.8973\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9050 - val_loss: 0.2583 - val_accuracy: 0.9041\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.9067 - val_loss: 0.2701 - val_accuracy: 0.8973\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.9058 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8998 - val_loss: 0.2983 - val_accuracy: 0.8973\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.9084 - val_loss: 0.2828 - val_accuracy: 0.8904\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9135 - val_loss: 0.3033 - val_accuracy: 0.8973\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.9075 - val_loss: 0.2979 - val_accuracy: 0.9007\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9067 - val_loss: 0.2783 - val_accuracy: 0.8870\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.9084 - val_loss: 0.2775 - val_accuracy: 0.9041\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8998 - val_loss: 0.2869 - val_accuracy: 0.9007\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9092 - val_loss: 0.2971 - val_accuracy: 0.9041\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9101 - val_loss: 0.2758 - val_accuracy: 0.8938\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.9075 - val_loss: 0.2806 - val_accuracy: 0.8870\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.9135 - val_loss: 0.2951 - val_accuracy: 0.8973\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9110 - val_loss: 0.2770 - val_accuracy: 0.8904\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9195 - val_loss: 0.3252 - val_accuracy: 0.8973\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9118 - val_loss: 0.2975 - val_accuracy: 0.8938\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.9041 - val_loss: 0.2956 - val_accuracy: 0.8904\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9135 - val_loss: 0.3207 - val_accuracy: 0.8904\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.9067 - val_loss: 0.2685 - val_accuracy: 0.9041\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9075 - val_loss: 0.2909 - val_accuracy: 0.8904\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9110 - val_loss: 0.2804 - val_accuracy: 0.8904\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.9092 - val_loss: 0.2729 - val_accuracy: 0.8973\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.9092 - val_loss: 0.2711 - val_accuracy: 0.8973\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9110 - val_loss: 0.2739 - val_accuracy: 0.8870\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.9187 - val_loss: 0.2804 - val_accuracy: 0.8870\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.9075 - val_loss: 0.2897 - val_accuracy: 0.8973\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.9118 - val_loss: 0.2806 - val_accuracy: 0.8904\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9187 - val_loss: 0.2916 - val_accuracy: 0.8904\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.9067 - val_loss: 0.2758 - val_accuracy: 0.8801\n",
      "Epoch 37/300\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9175\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9161 - val_loss: 0.3178 - val_accuracy: 0.8870\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9127 - val_loss: 0.2724 - val_accuracy: 0.8836\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9178 - val_loss: 0.2661 - val_accuracy: 0.9007\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9161 - val_loss: 0.2945 - val_accuracy: 0.8836\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.9187 - val_loss: 0.2762 - val_accuracy: 0.8836\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9221 - val_loss: 0.2755 - val_accuracy: 0.8836\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9264 - val_loss: 0.2771 - val_accuracy: 0.8836\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9229 - val_loss: 0.2602 - val_accuracy: 0.8836\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9272 - val_loss: 0.2816 - val_accuracy: 0.9007\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9170 - val_loss: 0.2646 - val_accuracy: 0.8904\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9238 - val_loss: 0.2738 - val_accuracy: 0.8904\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9315 - val_loss: 0.2778 - val_accuracy: 0.8973\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.9255 - val_loss: 0.2794 - val_accuracy: 0.8836\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9264 - val_loss: 0.3046 - val_accuracy: 0.8904\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9212 - val_loss: 0.2668 - val_accuracy: 0.8904\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9195 - val_loss: 0.2519 - val_accuracy: 0.8973\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9144 - val_loss: 0.3042 - val_accuracy: 0.8973\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.9101 - val_loss: 0.2684 - val_accuracy: 0.9007\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.9170 - val_loss: 0.2753 - val_accuracy: 0.8973\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9212 - val_loss: 0.2845 - val_accuracy: 0.8836\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9238 - val_loss: 0.2805 - val_accuracy: 0.8973\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.9272 - val_loss: 0.2706 - val_accuracy: 0.8938\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9255 - val_loss: 0.2906 - val_accuracy: 0.8904\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9247 - val_loss: 0.3026 - val_accuracy: 0.8973\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9324 - val_loss: 0.2930 - val_accuracy: 0.8973\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9238 - val_loss: 0.2976 - val_accuracy: 0.8870\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9229 - val_loss: 0.2564 - val_accuracy: 0.8870\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9272 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9264 - val_loss: 0.2583 - val_accuracy: 0.9110\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9255 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9289 - val_loss: 0.2593 - val_accuracy: 0.9007\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9272 - val_loss: 0.2805 - val_accuracy: 0.9041\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9281 - val_loss: 0.2737 - val_accuracy: 0.8938\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9247 - val_loss: 0.2855 - val_accuracy: 0.9007\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9264 - val_loss: 0.2709 - val_accuracy: 0.9110\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9298 - val_loss: 0.3050 - val_accuracy: 0.9007\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9135 - val_loss: 0.2627 - val_accuracy: 0.8973\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9281 - val_loss: 0.2856 - val_accuracy: 0.9075\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9247 - val_loss: 0.3255 - val_accuracy: 0.8938\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9058 - val_loss: 0.2671 - val_accuracy: 0.9041\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9238 - val_loss: 0.3048 - val_accuracy: 0.8973\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9281 - val_loss: 0.2967 - val_accuracy: 0.8973\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9178 - val_loss: 0.2964 - val_accuracy: 0.8973\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9315 - val_loss: 0.3316 - val_accuracy: 0.8973\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9212 - val_loss: 0.2860 - val_accuracy: 0.8973\n",
      "Epoch 82/300\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2433 - accuracy: 0.9304\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9298 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9298 - val_loss: 0.3124 - val_accuracy: 0.8938\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9238 - val_loss: 0.3067 - val_accuracy: 0.8904\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9324 - val_loss: 0.3308 - val_accuracy: 0.8938\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9332 - val_loss: 0.3049 - val_accuracy: 0.8904\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9358 - val_loss: 0.3226 - val_accuracy: 0.8870\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9324 - val_loss: 0.2741 - val_accuracy: 0.8904\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9298 - val_loss: 0.3048 - val_accuracy: 0.8870\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9238 - val_loss: 0.3301 - val_accuracy: 0.9007\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9221 - val_loss: 0.3131 - val_accuracy: 0.9007\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9247 - val_loss: 0.2832 - val_accuracy: 0.8870\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9238 - val_loss: 0.3121 - val_accuracy: 0.8904\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9212 - val_loss: 0.2892 - val_accuracy: 0.8870\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9281 - val_loss: 0.2692 - val_accuracy: 0.8938\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9195 - val_loss: 0.2798 - val_accuracy: 0.8870\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9255 - val_loss: 0.2958 - val_accuracy: 0.8938\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9375 - val_loss: 0.3294 - val_accuracy: 0.9007\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9229 - val_loss: 0.3181 - val_accuracy: 0.8938\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9264 - val_loss: 0.2994 - val_accuracy: 0.8904\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9366 - val_loss: 0.3187 - val_accuracy: 0.8904\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9307 - val_loss: 0.3049 - val_accuracy: 0.8870\n",
      "best val loss is: 0.2519095838069916\n",
      "best val accuracy is: 0.9109588861465454\n"
     ]
    }
   ],
   "source": [
    "deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=3)\n",
    "deneyelim2.get_raw_data()\n",
    "a = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  0.769623 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:10:00  0.762633 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:20:00  0.729854 -0.810232 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:30:00  0.739495 -0.834542 -1.983431  0.786817 -1.385479   \n",
       "  2016-01-11 17:40:00  0.792640 -0.834542 -1.983431  0.786817 -1.359694   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.294374 -0.569336 -0.106203  1.096424  0.110056   \n",
       "  2016-04-30 07:40:00 -0.287143 -0.609117 -0.106203  1.070624  0.058486   \n",
       "  2016-04-30 07:50:00 -0.281118 -0.569336 -0.159166  1.044823  0.110056   \n",
       "  2016-04-30 08:00:00 -0.300159 -0.569336 -0.159166  1.019022  0.161626   \n",
       "  2016-04-30 08:10:00 -0.329323 -0.569336 -0.159166  0.928720  0.257031   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-01-11 17:00:00   1.065608 -1.217218  1.554962  \n",
       "  2016-01-11 17:10:00   0.935169 -1.193835  1.654543  \n",
       "  2016-01-11 17:20:00   0.804731 -1.240601  1.630574  \n",
       "  2016-01-11 17:30:00   0.674293 -1.240601  1.591599  \n",
       "  2016-01-11 17:40:00   0.543854 -1.240601  1.546388  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-04-30 07:30:00  -1.282284  0.302667 -0.310373  \n",
       "  2016-04-30 07:40:00  -1.282284  0.302667 -0.279193  \n",
       "  2016-04-30 07:50:00  -1.282284  0.302667 -0.081980  \n",
       "  2016-04-30 08:00:00  -1.282284  0.302667 -0.050800  \n",
       "  2016-04-30 08:10:00  -1.282284  0.302667 -0.036769  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-01-11 17:00:00 -1.039517   0.729188  3.071295  0.180377  1.014203   \n",
       "  2016-01-11 17:10:00 -1.039517   0.700803  3.071295  0.152122  1.021582   \n",
       "  2016-01-11 17:20:00 -1.039517   0.672418  3.071295  0.123867  1.006824   \n",
       "  2016-01-11 17:30:00 -1.039517   0.644033  4.267005  0.095612  0.982227   \n",
       "  2016-01-11 17:40:00 -1.039517   0.615647  4.267005  0.067357  0.982227   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00  0.113503   0.487914 -0.515833 -0.182903  0.029511   \n",
       "  2016-04-30 07:40:00  0.168409   0.502106 -0.515833 -0.158684  0.014753   \n",
       "  2016-04-30 07:50:00  0.195862   0.516299 -0.515833 -0.134466 -0.001645   \n",
       "  2016-04-30 08:00:00  0.195862   0.530492 -0.515833 -0.110247 -0.059858   \n",
       "  2016-04-30 08:10:00  0.195862   0.521030 -0.515833 -0.077955 -0.092653   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-01-11 17:00:00  1.681976  2.103814 -1.200652  \n",
       "  2016-01-11 17:10:00  1.700697  1.852407 -1.200652  \n",
       "  2016-01-11 17:20:00  1.745419  1.742937 -1.200652  \n",
       "  2016-01-11 17:30:00  1.766219  1.677998 -1.200652  \n",
       "  2016-01-11 17:40:00  1.766219  1.752214 -1.200652  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-04-30 07:30:00 -0.667471 -0.771142  0.801833  \n",
       "  2016-04-30 07:40:00 -0.752754 -0.661673  0.761783  \n",
       "  2016-04-30 07:50:00 -0.906680 -0.580962  0.675009  \n",
       "  2016-04-30 08:00:00 -0.867158 -0.567047  0.675009  \n",
       "  2016-04-30 08:10:00 -0.885879 -0.457578  0.675009  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  1.311407 -1.494334  0.434884  1.246472  1.160518   \n",
       "  2016-01-11 17:10:00  1.298168 -1.494334  0.434884  1.226599  1.153506   \n",
       "  2016-01-11 17:20:00  1.273015 -1.494334  0.422695  1.198385  1.128006   \n",
       "  2016-01-11 17:30:00  1.244551 -1.536665  0.422695  1.187590  1.101232   \n",
       "  2016-01-11 17:40:00  1.232637 -1.494334  0.422695  1.169926  1.101232   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.284520 -0.287906 -0.894794 -0.045972  0.350278   \n",
       "  2016-04-30 07:40:00 -0.277239 -0.287906 -0.902920  0.009966  0.336253   \n",
       "  2016-04-30 07:50:00 -0.250761 -0.287906 -0.912246 -0.007699  0.305654   \n",
       "  2016-04-30 08:00:00 -0.237523 -0.287906 -0.961647 -0.078356  0.267405   \n",
       "  2016-04-30 08:10:00 -0.222960 -0.287906 -0.995628 -0.078356  0.210031   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-01-11 17:00:00    1.961544  0.188895 -0.284588    -2.815843  \n",
       "  2016-01-11 17:10:00    1.652462  0.147545 -0.284588    -2.802987  \n",
       "  2016-01-11 17:20:00    1.343380  0.089083 -0.284588    -2.790130  \n",
       "  2016-01-11 17:30:00    1.034298  0.061991 -0.284588    -2.777274  \n",
       "  2016-01-11 17:40:00    0.725216  0.047732 -0.284588    -2.764417  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-04-30 07:30:00    0.107051  0.131860 -0.554938     0.314740  \n",
       "  2016-04-30 07:40:00    0.107051  0.188895 -0.488999     0.325453  \n",
       "  2016-04-30 07:50:00    0.107051  0.311522 -0.440644     0.336167  \n",
       "  2016-04-30 08:00:00    0.107051  0.382817 -0.416466     0.346881  \n",
       "  2016-04-30 08:10:00    0.107051  0.445556 -0.416466     0.355452  \n",
       "  \n",
       "  [15788 rows x 9 columns],\n",
       "  date\n",
       "  2016-01-11 17:00:00     60\n",
       "  2016-01-11 17:10:00     60\n",
       "  2016-01-11 17:20:00     50\n",
       "  2016-01-11 17:30:00     50\n",
       "  2016-01-11 17:40:00     60\n",
       "                        ... \n",
       "  2016-04-30 07:30:00     80\n",
       "  2016-04-30 07:40:00     80\n",
       "  2016-04-30 07:50:00     50\n",
       "  2016-04-30 08:00:00     70\n",
       "  2016-04-30 08:10:00    300\n",
       "  Name: Appliances, Length: 15788, dtype: int64],\n",
       " [                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.365476 -0.569336 -0.159166  0.838418  0.210618   \n",
       "  2016-04-30 08:30:00 -0.430311 -0.525135 -0.159166  0.748116  0.187411   \n",
       "  2016-04-30 08:40:00 -0.534192 -0.503035 -0.198397  0.657814  0.187411   \n",
       "  2016-04-30 08:50:00 -0.740207 -0.454414 -0.159166  0.567512  0.187411   \n",
       "  2016-04-30 09:00:00 -0.789677 -0.430103 -0.159166  0.477210  0.187411   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00 -2.240395  3.017574  1.900489 -2.025449  3.281621   \n",
       "  2016-05-27 17:30:00 -2.240395  3.017574  1.900489 -1.999648  3.304827   \n",
       "  2016-05-27 17:40:00 -2.240395  3.017574  1.900489 -1.973847  3.304827   \n",
       "  2016-05-27 17:50:00 -2.240395  3.017574  1.878421 -1.948047  3.281621   \n",
       "  2016-05-27 18:00:00 -2.240395  3.017574  1.921674 -1.922246  3.281621   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-04-30 08:20:00  -1.282284  0.302667 -0.043785  \n",
       "  2016-04-30 08:30:00  -1.282284  0.302667  0.017016  \n",
       "  2016-04-30 08:40:00  -1.282284  0.302667  0.095745  \n",
       "  2016-04-30 08:50:00  -1.282284  0.302667  0.128484  \n",
       "  2016-04-30 09:00:00  -1.282284  0.365800  0.136279  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-05-27 17:20:00  -0.369215  3.108607  1.560419  \n",
       "  2016-05-27 17:30:00  -0.303996  3.108607  1.560419  \n",
       "  2016-05-27 17:40:00  -0.238776  3.108607  1.593158  \n",
       "  2016-05-27 17:50:00  -0.173557  3.108607  1.607189  \n",
       "  2016-05-27 18:00:00  -0.108338  3.108607  1.647723  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-04-30 08:20:00  0.195862   0.511568 -0.515833 -0.045664 -0.159065   \n",
       "  2016-04-30 08:30:00  0.195862   0.502106 -0.515833 -0.013372 -0.206618   \n",
       "  2016-04-30 08:40:00  0.195862   0.492645 -0.515833  0.018919 -0.256632   \n",
       "  2016-04-30 08:50:00  0.195862   0.483183  0.679876  0.051211 -0.274670   \n",
       "  2016-04-30 09:00:00  0.140956   0.473721 -0.515833  0.083502 -0.247613   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  3.635705   3.009470 -0.515833  4.087654  1.324122   \n",
       "  2016-05-27 17:30:00  3.580799   3.000008 -0.515833  4.055362  1.324122   \n",
       "  2016-05-27 17:40:00  3.580799   2.990547  0.679876  4.023071  1.324122   \n",
       "  2016-05-27 17:50:00  3.580799   2.981085  0.679876  3.990779  1.330886   \n",
       "  2016-05-27 18:00:00  3.580799   2.971623  0.679876  3.958488  1.337651   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-04-30 08:20:00 -0.885879 -0.494686  0.675009  \n",
       "  2016-04-30 08:30:00 -0.871838 -0.531794  0.675009  \n",
       "  2016-04-30 08:40:00 -0.857798 -0.541999  0.697259  \n",
       "  2016-04-30 08:50:00 -0.857798 -0.558697  0.741758  \n",
       "  2016-04-30 09:00:00 -0.857798 -0.594878  0.741758  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-05-27 17:20:00  0.569135  1.815298  3.745485  \n",
       "  2016-05-27 17:30:00  0.587856  1.798600  3.700986  \n",
       "  2016-05-27 17:40:00  0.733461  1.825503  3.645361  \n",
       "  2016-05-27 17:50:00  0.608657  1.934972  3.538562  \n",
       "  2016-05-27 18:00:00  0.567055  1.826431  3.494062  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.222960 -0.287906 -0.995628 -0.056767  0.171782   \n",
       "  2016-04-30 08:30:00 -0.222960 -0.287906 -0.958323 -0.096021  0.139271   \n",
       "  2016-04-30 08:40:00 -0.237523 -0.287906 -0.943549 -0.155883  0.127796   \n",
       "  2016-04-30 08:50:00 -0.244804 -0.287906 -0.928036 -0.232429  0.127796   \n",
       "  2016-04-30 09:00:00 -0.244804 -0.287906 -0.936162 -0.370800  0.139271   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  1.881996  3.140891  0.124626  0.432649  1.385039   \n",
       "  2016-05-27 17:30:00  1.864974  3.177175  0.116500  0.448631  1.330726   \n",
       "  2016-05-27 17:40:00  1.862138  3.166290  0.109851  0.651350  1.305864   \n",
       "  2016-05-27 17:50:00  1.841428  3.140891  0.102464  0.730083  1.278851   \n",
       "  2016-05-27 18:00:00  1.793429  3.140891  0.102464  0.711073  1.320398   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-04-30 08:20:00    0.107051  0.489759 -0.328547     0.364023  \n",
       "  2016-04-30 08:30:00    0.107051  0.595988 -0.284588     0.372594  \n",
       "  2016-04-30 08:40:00    0.107051  0.667283 -0.242827     0.381165  \n",
       "  2016-04-30 08:50:00    0.107051  0.749985 -0.066989     0.389736  \n",
       "  2016-04-30 09:00:00    0.107051  0.959591  0.064889     0.398307  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-05-27 17:20:00   -1.209907  3.989616  4.126733    -0.025960  \n",
       "  2016-05-27 17:30:00   -1.142716  3.861286  4.037056    -0.025960  \n",
       "  2016-05-27 17:40:00   -1.075524  3.739372  3.954349    -0.025960  \n",
       "  2016-05-27 17:50:00   -1.008332  3.484137  3.812863    -0.025960  \n",
       "  2016-05-27 18:00:00   -0.941140  3.183273  3.714143    -0.025960  \n",
       "  \n",
       "  [3947 rows x 9 columns],\n",
       "  date\n",
       "  2016-04-30 08:20:00    370\n",
       "  2016-04-30 08:30:00    590\n",
       "  2016-04-30 08:40:00    320\n",
       "  2016-04-30 08:50:00    310\n",
       "  2016-04-30 09:00:00    260\n",
       "                        ... \n",
       "  2016-05-27 17:20:00    100\n",
       "  2016-05-27 17:30:00     90\n",
       "  2016-05-27 17:40:00    270\n",
       "  2016-05-27 17:50:00    420\n",
       "  2016-05-27 18:00:00    430\n",
       "  Name: Appliances, Length: 3947, dtype: int64]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [None]*3\n",
    "b = [a,a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, 5, None], [None, 5, None]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][1] = 5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[None]*3]*2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, 5], [None, None, 5]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]=5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[None for _ in range(3)] for _ in range(2)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, 2]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][2]=2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,4,5]\n",
    "(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[2,4,6],[1,3,5]]\n",
    "c[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
