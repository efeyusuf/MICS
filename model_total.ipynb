{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 3):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self, index_col=0):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.8*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.8*row_num), 0]\n",
    "        testx_df = self.raw_data.iloc[int(0.8*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.8*row_num):, 0]        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(2)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_scaled.iloc[:,group]\n",
    "            features_and_labels[0][index] = train_temp.values\n",
    "            test_temp = textx_scaled.iloc[:,group]\n",
    "            features_and_labels[1][index] = test_temp.values            \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = testy_df.values   \n",
    "        return features_and_labels\n",
    "    \n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32, activation=\"relu\"):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=activation)(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "        \n",
    "    def default_exp_house(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes, activation=\"sigmoid\")\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        \n",
    "        training_acc = history.history[\"val_accuracy\"]\n",
    "        best_row_index_acc = np.argmax(training_acc)\n",
    "        best_val_acc = training_acc[best_row_index_acc]        \n",
    "        print(\"best val loss is: \" + str(best_val_loss))\n",
    "        print(\"best val accuracy is: \" + str(best_val_acc))\n",
    "        return best_val_acc\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 13119.5498 - val_loss: 8738.3174\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10271.2900 - val_loss: 8557.2266\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9659.9287 - val_loss: 8218.1934\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9552.3984 - val_loss: 8872.4307\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9434.6973 - val_loss: 8398.0352\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9165.0244 - val_loss: 7749.7622\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9170.0713 - val_loss: 8383.3096\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9178.5596 - val_loss: 7652.7446\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9093.2236 - val_loss: 7604.3677\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8966.5273 - val_loss: 7948.0493\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8982.7549 - val_loss: 7718.2612\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8875.8721 - val_loss: 7843.3677\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8874.6514 - val_loss: 8137.6748\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8769.0479 - val_loss: 7762.5137\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8665.5645 - val_loss: 7528.5913\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8596.0586 - val_loss: 7580.3433\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8583.9004 - val_loss: 7503.2197\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8704.8145 - val_loss: 7391.5083\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8620.2354 - val_loss: 7480.3682\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8567.1631 - val_loss: 7411.4551\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8498.7324 - val_loss: 7418.1147\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8576.7500 - val_loss: 7657.5337\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8503.9082 - val_loss: 7528.2773\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8492.5107 - val_loss: 7476.4507\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8591.9268 - val_loss: 7224.1831\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8363.6631 - val_loss: 7684.8613\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8411.2949 - val_loss: 7501.4062\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8274.4160 - val_loss: 7376.2471\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8472.9688 - val_loss: 7330.4692\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8398.4668 - val_loss: 7673.6416\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8442.9443 - val_loss: 7441.6094\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8403.4756 - val_loss: 7538.5869\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8239.1680 - val_loss: 7191.3672\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8371.8438 - val_loss: 7331.2036\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8264.4238 - val_loss: 7516.7271\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8207.9531 - val_loss: 7550.4053\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8254.9658 - val_loss: 7316.5474\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8191.4097 - val_loss: 7198.9390\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8062.0864 - val_loss: 7341.0039\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8051.1831 - val_loss: 7017.7290\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8186.6182 - val_loss: 7414.1460\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8110.3413 - val_loss: 7567.3545\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8059.6523 - val_loss: 7385.0884\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8206.6885 - val_loss: 7308.7954\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8178.4722 - val_loss: 7268.1362\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8185.5259 - val_loss: 7709.1816\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7886.2251 - val_loss: 7181.6064\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8174.0366 - val_loss: 7478.9053\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8101.3062 - val_loss: 7342.7905\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8063.7896 - val_loss: 7597.2051\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7897.8564 - val_loss: 7311.0645\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8055.8906 - val_loss: 7260.3311\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7977.6885 - val_loss: 7040.8423\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7861.9834 - val_loss: 7034.2314\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7980.3027 - val_loss: 6924.4009\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7949.1182 - val_loss: 7368.4663\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8036.7202 - val_loss: 7559.3511\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7929.6079 - val_loss: 7259.2710\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7821.7563 - val_loss: 7307.4248\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7926.5093 - val_loss: 7636.0146\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8000.1270 - val_loss: 7180.7256\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8149.6216 - val_loss: 6915.0376\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7854.5947 - val_loss: 7259.6689\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7952.5122 - val_loss: 7527.9336\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7984.1802 - val_loss: 7336.8315\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7956.1357 - val_loss: 7228.4097\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7952.5288 - val_loss: 7437.0869\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7915.0386 - val_loss: 7267.2559\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7973.4658 - val_loss: 6929.2695\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7787.1152 - val_loss: 7238.9121\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7911.8574 - val_loss: 7129.0361\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7817.6729 - val_loss: 7563.2842\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7870.3418 - val_loss: 6937.0811\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7752.9370 - val_loss: 6942.0679\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7759.4937 - val_loss: 7782.7061\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7700.4180 - val_loss: 7172.3643\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7762.2036 - val_loss: 7066.0659\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7653.8975 - val_loss: 7327.9536\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7737.2236 - val_loss: 7639.3042\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7777.7026 - val_loss: 7053.4487\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7702.6475 - val_loss: 7362.2930\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7631.5786 - val_loss: 6992.5757\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7604.1626 - val_loss: 7811.7954\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7706.9878 - val_loss: 7141.3740\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7685.8418 - val_loss: 7026.4971\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7860.8755 - val_loss: 7218.7964\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7595.6455 - val_loss: 7246.4888\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7680.8550 - val_loss: 7091.7158\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7752.5596 - val_loss: 7227.8481\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7646.8506 - val_loss: 7233.6943\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7640.8516 - val_loss: 7434.2290\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7668.2407 - val_loss: 6892.9980\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7688.7373 - val_loss: 7425.1230\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7592.5557 - val_loss: 7483.3311\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7663.4067 - val_loss: 6775.6274\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7690.8237 - val_loss: 7166.6743\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7618.7656 - val_loss: 7434.6226\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7580.2627 - val_loss: 7273.7632\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7532.9937 - val_loss: 6979.7617\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7537.7002 - val_loss: 7225.9458\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7649.6108 - val_loss: 7224.2437\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7693.6841 - val_loss: 7157.8838\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7572.1108 - val_loss: 7717.4487\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7615.5820 - val_loss: 7154.9209\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7560.2988 - val_loss: 7368.1279\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7749.1631 - val_loss: 7612.8818\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7574.1460 - val_loss: 7306.4272\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7566.5864 - val_loss: 7389.0142\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7801.1562 - val_loss: 7380.4180\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7671.9287 - val_loss: 6940.4888\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7650.4604 - val_loss: 7216.2852\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7602.5293 - val_loss: 7023.9404\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7631.5244 - val_loss: 7701.7925\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7672.3271 - val_loss: 7348.5005\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7502.1162 - val_loss: 7192.6196\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7398.4868 - val_loss: 7112.1533\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7605.7749 - val_loss: 7454.9019\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7768.4126 - val_loss: 7709.2002\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7380.1880 - val_loss: 7295.9736\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7460.8398 - val_loss: 7824.0381\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7569.2749 - val_loss: 7605.9956\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7626.3906 - val_loss: 6962.6001\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7623.2627 - val_loss: 7570.4561\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7659.2754 - val_loss: 7396.2422\n",
      "Epoch 125/300\n",
      "33/53 [=================>............] - ETA: 0s - loss: 7788.3008\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7603.7935 - val_loss: 7023.6929\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7269.8862 - val_loss: 6673.4858\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7468.5332 - val_loss: 6945.1909\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7395.1528 - val_loss: 7320.0317\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7283.4141 - val_loss: 6963.9009\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7519.4839 - val_loss: 7356.1519\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7278.0635 - val_loss: 7255.6279\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7351.4194 - val_loss: 7238.2915\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7507.2065 - val_loss: 7141.8545\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7378.2368 - val_loss: 6777.8140\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7449.7349 - val_loss: 7498.6343\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7350.7397 - val_loss: 7079.7095\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7389.5835 - val_loss: 7261.2441\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7181.5068 - val_loss: 7236.9741\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7295.4614 - val_loss: 7296.3838\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7280.0835 - val_loss: 7407.1909\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7322.6782 - val_loss: 6931.3936\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7283.5762 - val_loss: 7213.3472\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7434.3945 - val_loss: 7496.8652\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7228.0107 - val_loss: 7395.1299\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7277.1260 - val_loss: 6842.2271\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7340.7197 - val_loss: 7066.0034\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7263.9946 - val_loss: 7196.5835\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7273.8784 - val_loss: 7009.9609\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7185.9561 - val_loss: 6971.0933\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7298.9014 - val_loss: 7042.8140\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7278.5752 - val_loss: 6731.3472\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7341.7422 - val_loss: 7063.3076\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7259.7310 - val_loss: 6979.6826\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7326.6455 - val_loss: 7063.8101\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7174.0317 - val_loss: 7315.3413\n",
      "Epoch 156/300\n",
      "32/53 [=================>............] - ETA: 0s - loss: 7439.8926 \n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7346.1104 - val_loss: 7007.2969\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7152.3882 - val_loss: 7368.0732\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7166.2856 - val_loss: 6944.4517\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6968.3726 - val_loss: 6984.4648\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6989.9478 - val_loss: 6903.1177\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7016.3496 - val_loss: 7033.4019\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7159.8037 - val_loss: 7032.8091\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7107.4194 - val_loss: 6908.6611\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7060.8682 - val_loss: 6777.6582\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7173.6650 - val_loss: 7358.7876\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7121.3789 - val_loss: 7158.6729\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6998.3149 - val_loss: 7115.4004\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6968.1846 - val_loss: 7072.8955\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7139.5146 - val_loss: 7432.6841\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7104.0049 - val_loss: 6822.3813\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7044.2314 - val_loss: 7158.9258\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7183.0049 - val_loss: 6990.4385\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7129.5776 - val_loss: 7144.8989\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7344.1494 - val_loss: 7007.8174\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7100.2778 - val_loss: 7211.6362\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7069.3696 - val_loss: 7236.7935\n",
      "6673.48583984375\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=False, group_number=3)\n",
    "deneyelim.get_raw_data()\n",
    "deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, sampling_method=\"Vanilla\", group_number=5)\n",
    "    deneyelim2.get_raw_data()\n",
    "    acc = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.7697 - val_loss: 0.3884 - val_accuracy: 0.8938\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8733 - val_loss: 0.3383 - val_accuracy: 0.9007\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8887 - val_loss: 0.3667 - val_accuracy: 0.8973\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8801 - val_loss: 0.3067 - val_accuracy: 0.8938\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8896 - val_loss: 0.2846 - val_accuracy: 0.8904\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8904 - val_loss: 0.2721 - val_accuracy: 0.8973\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9050 - val_loss: 0.2583 - val_accuracy: 0.9041\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.9067 - val_loss: 0.2701 - val_accuracy: 0.8973\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.9058 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8998 - val_loss: 0.2983 - val_accuracy: 0.8973\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.9084 - val_loss: 0.2828 - val_accuracy: 0.8904\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9135 - val_loss: 0.3033 - val_accuracy: 0.8973\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.9075 - val_loss: 0.2979 - val_accuracy: 0.9007\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9067 - val_loss: 0.2783 - val_accuracy: 0.8870\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.9084 - val_loss: 0.2775 - val_accuracy: 0.9041\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8998 - val_loss: 0.2869 - val_accuracy: 0.9007\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9092 - val_loss: 0.2971 - val_accuracy: 0.9041\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9101 - val_loss: 0.2758 - val_accuracy: 0.8938\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.9075 - val_loss: 0.2806 - val_accuracy: 0.8870\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.9135 - val_loss: 0.2951 - val_accuracy: 0.8973\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9110 - val_loss: 0.2770 - val_accuracy: 0.8904\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9195 - val_loss: 0.3252 - val_accuracy: 0.8973\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9118 - val_loss: 0.2975 - val_accuracy: 0.8938\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.9041 - val_loss: 0.2956 - val_accuracy: 0.8904\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9135 - val_loss: 0.3207 - val_accuracy: 0.8904\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.9067 - val_loss: 0.2685 - val_accuracy: 0.9041\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9075 - val_loss: 0.2909 - val_accuracy: 0.8904\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9110 - val_loss: 0.2804 - val_accuracy: 0.8904\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.9092 - val_loss: 0.2729 - val_accuracy: 0.8973\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.9092 - val_loss: 0.2711 - val_accuracy: 0.8973\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9110 - val_loss: 0.2739 - val_accuracy: 0.8870\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.9187 - val_loss: 0.2804 - val_accuracy: 0.8870\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.9075 - val_loss: 0.2897 - val_accuracy: 0.8973\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.9118 - val_loss: 0.2806 - val_accuracy: 0.8904\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9187 - val_loss: 0.2916 - val_accuracy: 0.8904\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.9067 - val_loss: 0.2758 - val_accuracy: 0.8801\n",
      "Epoch 37/300\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9175\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9161 - val_loss: 0.3178 - val_accuracy: 0.8870\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9127 - val_loss: 0.2724 - val_accuracy: 0.8836\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9178 - val_loss: 0.2661 - val_accuracy: 0.9007\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9161 - val_loss: 0.2945 - val_accuracy: 0.8836\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.9187 - val_loss: 0.2762 - val_accuracy: 0.8836\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9221 - val_loss: 0.2755 - val_accuracy: 0.8836\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9264 - val_loss: 0.2771 - val_accuracy: 0.8836\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9229 - val_loss: 0.2602 - val_accuracy: 0.8836\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9272 - val_loss: 0.2816 - val_accuracy: 0.9007\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9170 - val_loss: 0.2646 - val_accuracy: 0.8904\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9238 - val_loss: 0.2738 - val_accuracy: 0.8904\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9315 - val_loss: 0.2778 - val_accuracy: 0.8973\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.9255 - val_loss: 0.2794 - val_accuracy: 0.8836\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9264 - val_loss: 0.3046 - val_accuracy: 0.8904\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9212 - val_loss: 0.2668 - val_accuracy: 0.8904\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9195 - val_loss: 0.2519 - val_accuracy: 0.8973\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9144 - val_loss: 0.3042 - val_accuracy: 0.8973\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.9101 - val_loss: 0.2684 - val_accuracy: 0.9007\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.9170 - val_loss: 0.2753 - val_accuracy: 0.8973\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9212 - val_loss: 0.2845 - val_accuracy: 0.8836\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9238 - val_loss: 0.2805 - val_accuracy: 0.8973\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.9272 - val_loss: 0.2706 - val_accuracy: 0.8938\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9255 - val_loss: 0.2906 - val_accuracy: 0.8904\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9247 - val_loss: 0.3026 - val_accuracy: 0.8973\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9324 - val_loss: 0.2930 - val_accuracy: 0.8973\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9238 - val_loss: 0.2976 - val_accuracy: 0.8870\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9229 - val_loss: 0.2564 - val_accuracy: 0.8870\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9272 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9264 - val_loss: 0.2583 - val_accuracy: 0.9110\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9255 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9289 - val_loss: 0.2593 - val_accuracy: 0.9007\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9272 - val_loss: 0.2805 - val_accuracy: 0.9041\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9281 - val_loss: 0.2737 - val_accuracy: 0.8938\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9247 - val_loss: 0.2855 - val_accuracy: 0.9007\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9264 - val_loss: 0.2709 - val_accuracy: 0.9110\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9298 - val_loss: 0.3050 - val_accuracy: 0.9007\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9135 - val_loss: 0.2627 - val_accuracy: 0.8973\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9281 - val_loss: 0.2856 - val_accuracy: 0.9075\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9247 - val_loss: 0.3255 - val_accuracy: 0.8938\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9058 - val_loss: 0.2671 - val_accuracy: 0.9041\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9238 - val_loss: 0.3048 - val_accuracy: 0.8973\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9281 - val_loss: 0.2967 - val_accuracy: 0.8973\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9178 - val_loss: 0.2964 - val_accuracy: 0.8973\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9315 - val_loss: 0.3316 - val_accuracy: 0.8973\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9212 - val_loss: 0.2860 - val_accuracy: 0.8973\n",
      "Epoch 82/300\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2433 - accuracy: 0.9304\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9298 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9298 - val_loss: 0.3124 - val_accuracy: 0.8938\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9238 - val_loss: 0.3067 - val_accuracy: 0.8904\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9324 - val_loss: 0.3308 - val_accuracy: 0.8938\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9332 - val_loss: 0.3049 - val_accuracy: 0.8904\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9358 - val_loss: 0.3226 - val_accuracy: 0.8870\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9324 - val_loss: 0.2741 - val_accuracy: 0.8904\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9298 - val_loss: 0.3048 - val_accuracy: 0.8870\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9238 - val_loss: 0.3301 - val_accuracy: 0.9007\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9221 - val_loss: 0.3131 - val_accuracy: 0.9007\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9247 - val_loss: 0.2832 - val_accuracy: 0.8870\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9238 - val_loss: 0.3121 - val_accuracy: 0.8904\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9212 - val_loss: 0.2892 - val_accuracy: 0.8870\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9281 - val_loss: 0.2692 - val_accuracy: 0.8938\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9195 - val_loss: 0.2798 - val_accuracy: 0.8870\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9255 - val_loss: 0.2958 - val_accuracy: 0.8938\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9375 - val_loss: 0.3294 - val_accuracy: 0.9007\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9229 - val_loss: 0.3181 - val_accuracy: 0.8938\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9264 - val_loss: 0.2994 - val_accuracy: 0.8904\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9366 - val_loss: 0.3187 - val_accuracy: 0.8904\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9307 - val_loss: 0.3049 - val_accuracy: 0.8870\n",
      "best val loss is: 0.2519095838069916\n",
      "best val accuracy is: 0.9109588861465454\n"
     ]
    }
   ],
   "source": [
    "deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=3)\n",
    "deneyelim2.get_raw_data()\n",
    "a = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  0.769623 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:10:00  0.762633 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:20:00  0.729854 -0.810232 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:30:00  0.739495 -0.834542 -1.983431  0.786817 -1.385479   \n",
       "  2016-01-11 17:40:00  0.792640 -0.834542 -1.983431  0.786817 -1.359694   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.294374 -0.569336 -0.106203  1.096424  0.110056   \n",
       "  2016-04-30 07:40:00 -0.287143 -0.609117 -0.106203  1.070624  0.058486   \n",
       "  2016-04-30 07:50:00 -0.281118 -0.569336 -0.159166  1.044823  0.110056   \n",
       "  2016-04-30 08:00:00 -0.300159 -0.569336 -0.159166  1.019022  0.161626   \n",
       "  2016-04-30 08:10:00 -0.329323 -0.569336 -0.159166  0.928720  0.257031   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-01-11 17:00:00   1.065608 -1.217218  1.554962  \n",
       "  2016-01-11 17:10:00   0.935169 -1.193835  1.654543  \n",
       "  2016-01-11 17:20:00   0.804731 -1.240601  1.630574  \n",
       "  2016-01-11 17:30:00   0.674293 -1.240601  1.591599  \n",
       "  2016-01-11 17:40:00   0.543854 -1.240601  1.546388  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-04-30 07:30:00  -1.282284  0.302667 -0.310373  \n",
       "  2016-04-30 07:40:00  -1.282284  0.302667 -0.279193  \n",
       "  2016-04-30 07:50:00  -1.282284  0.302667 -0.081980  \n",
       "  2016-04-30 08:00:00  -1.282284  0.302667 -0.050800  \n",
       "  2016-04-30 08:10:00  -1.282284  0.302667 -0.036769  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-01-11 17:00:00 -1.039517   0.729188  3.071295  0.180377  1.014203   \n",
       "  2016-01-11 17:10:00 -1.039517   0.700803  3.071295  0.152122  1.021582   \n",
       "  2016-01-11 17:20:00 -1.039517   0.672418  3.071295  0.123867  1.006824   \n",
       "  2016-01-11 17:30:00 -1.039517   0.644033  4.267005  0.095612  0.982227   \n",
       "  2016-01-11 17:40:00 -1.039517   0.615647  4.267005  0.067357  0.982227   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00  0.113503   0.487914 -0.515833 -0.182903  0.029511   \n",
       "  2016-04-30 07:40:00  0.168409   0.502106 -0.515833 -0.158684  0.014753   \n",
       "  2016-04-30 07:50:00  0.195862   0.516299 -0.515833 -0.134466 -0.001645   \n",
       "  2016-04-30 08:00:00  0.195862   0.530492 -0.515833 -0.110247 -0.059858   \n",
       "  2016-04-30 08:10:00  0.195862   0.521030 -0.515833 -0.077955 -0.092653   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-01-11 17:00:00  1.681976  2.103814 -1.200652  \n",
       "  2016-01-11 17:10:00  1.700697  1.852407 -1.200652  \n",
       "  2016-01-11 17:20:00  1.745419  1.742937 -1.200652  \n",
       "  2016-01-11 17:30:00  1.766219  1.677998 -1.200652  \n",
       "  2016-01-11 17:40:00  1.766219  1.752214 -1.200652  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-04-30 07:30:00 -0.667471 -0.771142  0.801833  \n",
       "  2016-04-30 07:40:00 -0.752754 -0.661673  0.761783  \n",
       "  2016-04-30 07:50:00 -0.906680 -0.580962  0.675009  \n",
       "  2016-04-30 08:00:00 -0.867158 -0.567047  0.675009  \n",
       "  2016-04-30 08:10:00 -0.885879 -0.457578  0.675009  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  1.311407 -1.494334  0.434884  1.246472  1.160518   \n",
       "  2016-01-11 17:10:00  1.298168 -1.494334  0.434884  1.226599  1.153506   \n",
       "  2016-01-11 17:20:00  1.273015 -1.494334  0.422695  1.198385  1.128006   \n",
       "  2016-01-11 17:30:00  1.244551 -1.536665  0.422695  1.187590  1.101232   \n",
       "  2016-01-11 17:40:00  1.232637 -1.494334  0.422695  1.169926  1.101232   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.284520 -0.287906 -0.894794 -0.045972  0.350278   \n",
       "  2016-04-30 07:40:00 -0.277239 -0.287906 -0.902920  0.009966  0.336253   \n",
       "  2016-04-30 07:50:00 -0.250761 -0.287906 -0.912246 -0.007699  0.305654   \n",
       "  2016-04-30 08:00:00 -0.237523 -0.287906 -0.961647 -0.078356  0.267405   \n",
       "  2016-04-30 08:10:00 -0.222960 -0.287906 -0.995628 -0.078356  0.210031   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-01-11 17:00:00    1.961544  0.188895 -0.284588    -2.815843  \n",
       "  2016-01-11 17:10:00    1.652462  0.147545 -0.284588    -2.802987  \n",
       "  2016-01-11 17:20:00    1.343380  0.089083 -0.284588    -2.790130  \n",
       "  2016-01-11 17:30:00    1.034298  0.061991 -0.284588    -2.777274  \n",
       "  2016-01-11 17:40:00    0.725216  0.047732 -0.284588    -2.764417  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-04-30 07:30:00    0.107051  0.131860 -0.554938     0.314740  \n",
       "  2016-04-30 07:40:00    0.107051  0.188895 -0.488999     0.325453  \n",
       "  2016-04-30 07:50:00    0.107051  0.311522 -0.440644     0.336167  \n",
       "  2016-04-30 08:00:00    0.107051  0.382817 -0.416466     0.346881  \n",
       "  2016-04-30 08:10:00    0.107051  0.445556 -0.416466     0.355452  \n",
       "  \n",
       "  [15788 rows x 9 columns],\n",
       "  date\n",
       "  2016-01-11 17:00:00     60\n",
       "  2016-01-11 17:10:00     60\n",
       "  2016-01-11 17:20:00     50\n",
       "  2016-01-11 17:30:00     50\n",
       "  2016-01-11 17:40:00     60\n",
       "                        ... \n",
       "  2016-04-30 07:30:00     80\n",
       "  2016-04-30 07:40:00     80\n",
       "  2016-04-30 07:50:00     50\n",
       "  2016-04-30 08:00:00     70\n",
       "  2016-04-30 08:10:00    300\n",
       "  Name: Appliances, Length: 15788, dtype: int64],\n",
       " [                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.365476 -0.569336 -0.159166  0.838418  0.210618   \n",
       "  2016-04-30 08:30:00 -0.430311 -0.525135 -0.159166  0.748116  0.187411   \n",
       "  2016-04-30 08:40:00 -0.534192 -0.503035 -0.198397  0.657814  0.187411   \n",
       "  2016-04-30 08:50:00 -0.740207 -0.454414 -0.159166  0.567512  0.187411   \n",
       "  2016-04-30 09:00:00 -0.789677 -0.430103 -0.159166  0.477210  0.187411   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00 -2.240395  3.017574  1.900489 -2.025449  3.281621   \n",
       "  2016-05-27 17:30:00 -2.240395  3.017574  1.900489 -1.999648  3.304827   \n",
       "  2016-05-27 17:40:00 -2.240395  3.017574  1.900489 -1.973847  3.304827   \n",
       "  2016-05-27 17:50:00 -2.240395  3.017574  1.878421 -1.948047  3.281621   \n",
       "  2016-05-27 18:00:00 -2.240395  3.017574  1.921674 -1.922246  3.281621   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-04-30 08:20:00  -1.282284  0.302667 -0.043785  \n",
       "  2016-04-30 08:30:00  -1.282284  0.302667  0.017016  \n",
       "  2016-04-30 08:40:00  -1.282284  0.302667  0.095745  \n",
       "  2016-04-30 08:50:00  -1.282284  0.302667  0.128484  \n",
       "  2016-04-30 09:00:00  -1.282284  0.365800  0.136279  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-05-27 17:20:00  -0.369215  3.108607  1.560419  \n",
       "  2016-05-27 17:30:00  -0.303996  3.108607  1.560419  \n",
       "  2016-05-27 17:40:00  -0.238776  3.108607  1.593158  \n",
       "  2016-05-27 17:50:00  -0.173557  3.108607  1.607189  \n",
       "  2016-05-27 18:00:00  -0.108338  3.108607  1.647723  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-04-30 08:20:00  0.195862   0.511568 -0.515833 -0.045664 -0.159065   \n",
       "  2016-04-30 08:30:00  0.195862   0.502106 -0.515833 -0.013372 -0.206618   \n",
       "  2016-04-30 08:40:00  0.195862   0.492645 -0.515833  0.018919 -0.256632   \n",
       "  2016-04-30 08:50:00  0.195862   0.483183  0.679876  0.051211 -0.274670   \n",
       "  2016-04-30 09:00:00  0.140956   0.473721 -0.515833  0.083502 -0.247613   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  3.635705   3.009470 -0.515833  4.087654  1.324122   \n",
       "  2016-05-27 17:30:00  3.580799   3.000008 -0.515833  4.055362  1.324122   \n",
       "  2016-05-27 17:40:00  3.580799   2.990547  0.679876  4.023071  1.324122   \n",
       "  2016-05-27 17:50:00  3.580799   2.981085  0.679876  3.990779  1.330886   \n",
       "  2016-05-27 18:00:00  3.580799   2.971623  0.679876  3.958488  1.337651   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-04-30 08:20:00 -0.885879 -0.494686  0.675009  \n",
       "  2016-04-30 08:30:00 -0.871838 -0.531794  0.675009  \n",
       "  2016-04-30 08:40:00 -0.857798 -0.541999  0.697259  \n",
       "  2016-04-30 08:50:00 -0.857798 -0.558697  0.741758  \n",
       "  2016-04-30 09:00:00 -0.857798 -0.594878  0.741758  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-05-27 17:20:00  0.569135  1.815298  3.745485  \n",
       "  2016-05-27 17:30:00  0.587856  1.798600  3.700986  \n",
       "  2016-05-27 17:40:00  0.733461  1.825503  3.645361  \n",
       "  2016-05-27 17:50:00  0.608657  1.934972  3.538562  \n",
       "  2016-05-27 18:00:00  0.567055  1.826431  3.494062  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.222960 -0.287906 -0.995628 -0.056767  0.171782   \n",
       "  2016-04-30 08:30:00 -0.222960 -0.287906 -0.958323 -0.096021  0.139271   \n",
       "  2016-04-30 08:40:00 -0.237523 -0.287906 -0.943549 -0.155883  0.127796   \n",
       "  2016-04-30 08:50:00 -0.244804 -0.287906 -0.928036 -0.232429  0.127796   \n",
       "  2016-04-30 09:00:00 -0.244804 -0.287906 -0.936162 -0.370800  0.139271   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  1.881996  3.140891  0.124626  0.432649  1.385039   \n",
       "  2016-05-27 17:30:00  1.864974  3.177175  0.116500  0.448631  1.330726   \n",
       "  2016-05-27 17:40:00  1.862138  3.166290  0.109851  0.651350  1.305864   \n",
       "  2016-05-27 17:50:00  1.841428  3.140891  0.102464  0.730083  1.278851   \n",
       "  2016-05-27 18:00:00  1.793429  3.140891  0.102464  0.711073  1.320398   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-04-30 08:20:00    0.107051  0.489759 -0.328547     0.364023  \n",
       "  2016-04-30 08:30:00    0.107051  0.595988 -0.284588     0.372594  \n",
       "  2016-04-30 08:40:00    0.107051  0.667283 -0.242827     0.381165  \n",
       "  2016-04-30 08:50:00    0.107051  0.749985 -0.066989     0.389736  \n",
       "  2016-04-30 09:00:00    0.107051  0.959591  0.064889     0.398307  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-05-27 17:20:00   -1.209907  3.989616  4.126733    -0.025960  \n",
       "  2016-05-27 17:30:00   -1.142716  3.861286  4.037056    -0.025960  \n",
       "  2016-05-27 17:40:00   -1.075524  3.739372  3.954349    -0.025960  \n",
       "  2016-05-27 17:50:00   -1.008332  3.484137  3.812863    -0.025960  \n",
       "  2016-05-27 18:00:00   -0.941140  3.183273  3.714143    -0.025960  \n",
       "  \n",
       "  [3947 rows x 9 columns],\n",
       "  date\n",
       "  2016-04-30 08:20:00    370\n",
       "  2016-04-30 08:30:00    590\n",
       "  2016-04-30 08:40:00    320\n",
       "  2016-04-30 08:50:00    310\n",
       "  2016-04-30 09:00:00    260\n",
       "                        ... \n",
       "  2016-05-27 17:20:00    100\n",
       "  2016-05-27 17:30:00     90\n",
       "  2016-05-27 17:40:00    270\n",
       "  2016-05-27 17:50:00    420\n",
       "  2016-05-27 18:00:00    430\n",
       "  Name: Appliances, Length: 3947, dtype: int64]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [None]*3\n",
    "b = [a,a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, 5, None], [None, 5, None]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][1] = 5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[None]*3]*2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, 5], [None, None, 5]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]=5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[None for _ in range(3)] for _ in range(2)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, 2]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][2]=2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,4,5]\n",
    "(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[2,4,6],[1,3,5]]\n",
    "c[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
