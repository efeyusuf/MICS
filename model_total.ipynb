{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 3):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.8*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.8*row_num), 0]\n",
    "        testx_df = self.raw_data.iloc[int(0.8*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.8*row_num):, 0]        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(2)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_scaled.iloc[:,group]\n",
    "            features_and_labels[0][index] = train_temp.values\n",
    "            test_temp = textx_scaled.iloc[:,group]\n",
    "            features_and_labels[1][index] = test_temp.values            \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = testy_df.values   \n",
    "        return features_and_labels\n",
    "    \n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=\"relu\")(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = 300, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 12616.7471 - val_loss: 8667.1895\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10055.8975 - val_loss: 8886.1230\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9860.2812 - val_loss: 9000.0156\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9593.5957 - val_loss: 8169.1792\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9496.1250 - val_loss: 7909.1753\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9331.0840 - val_loss: 7912.4595\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9343.7842 - val_loss: 8396.7432\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9113.4102 - val_loss: 7945.4038\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9112.4893 - val_loss: 7642.9092\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8918.6035 - val_loss: 7800.0474\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9051.1484 - val_loss: 7852.0347\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8890.8135 - val_loss: 7680.2515\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8836.8398 - val_loss: 7642.6895\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8819.4131 - val_loss: 7592.9702\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8728.5781 - val_loss: 7472.9028\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8706.6123 - val_loss: 7420.4834\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8647.8672 - val_loss: 7882.8608\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8666.7197 - val_loss: 7627.4771\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8489.2793 - val_loss: 7653.7163\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8540.7285 - val_loss: 7664.6978\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8625.8037 - val_loss: 7534.7192\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8494.1758 - val_loss: 7701.7510\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8427.9824 - val_loss: 7424.6523\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8404.2295 - val_loss: 7741.6987\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8411.5342 - val_loss: 7721.4048\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8519.0791 - val_loss: 7623.9834\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8456.6465 - val_loss: 7260.0352\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8375.3242 - val_loss: 7668.0308\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8302.1084 - val_loss: 7560.1450\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8198.6729 - val_loss: 7299.7334\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8264.7158 - val_loss: 7534.3853\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8311.9336 - val_loss: 7238.3179\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8240.7100 - val_loss: 7532.1230\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8161.8765 - val_loss: 7484.3286\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8263.6152 - val_loss: 7559.0283\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8101.4116 - val_loss: 7192.0273\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8096.2671 - val_loss: 7231.0444\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8147.9517 - val_loss: 7493.2715\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8119.6064 - val_loss: 7493.5469\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8165.2329 - val_loss: 7494.5918\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8007.1846 - val_loss: 7263.6904\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8163.0610 - val_loss: 7341.6440\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8091.0864 - val_loss: 7132.2998\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7940.1421 - val_loss: 7393.5620\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7946.5757 - val_loss: 7284.3481\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8037.1719 - val_loss: 7144.7510\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7992.6484 - val_loss: 7266.9224\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7867.9717 - val_loss: 7333.7930\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7954.8115 - val_loss: 6933.4585\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8037.6006 - val_loss: 7122.1362\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7919.1621 - val_loss: 7098.7378\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7848.5347 - val_loss: 6933.4810\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7746.0151 - val_loss: 7474.7959\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7819.7539 - val_loss: 7292.6523\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7970.2954 - val_loss: 7482.1860\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7807.9941 - val_loss: 7689.5649\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7917.2153 - val_loss: 7178.6001\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7822.9790 - val_loss: 7437.2368\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7993.1675 - val_loss: 7428.7319\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7732.1611 - val_loss: 7423.0913\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7806.9966 - val_loss: 7191.4990\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7833.8384 - val_loss: 7127.6494\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7806.1113 - val_loss: 7233.0259\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7890.8726 - val_loss: 7166.7222\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7702.9077 - val_loss: 7074.5288\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7765.7368 - val_loss: 7217.2773\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7642.4146 - val_loss: 7061.3662\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7781.1177 - val_loss: 7150.4482\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7738.6694 - val_loss: 7347.4385\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7751.7788 - val_loss: 7286.4849\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7501.0776 - val_loss: 7092.5796\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7739.4238 - val_loss: 7362.6128\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7560.2861 - val_loss: 7373.6621\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7709.7476 - val_loss: 7246.6123\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7555.8750 - val_loss: 7486.5557\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7652.3184 - val_loss: 7505.9795\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7765.2339 - val_loss: 7266.8042\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7509.0771 - val_loss: 6976.4526\n",
      "Epoch 79/300\n",
      "27/53 [==============>...............] - ETA: 0s - loss: 8246.1104\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7769.6133 - val_loss: 7565.7710\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7492.4692 - val_loss: 7094.8208\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7515.9009 - val_loss: 7082.5967\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7444.6597 - val_loss: 7301.5469\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7467.8940 - val_loss: 7408.5171\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7408.5825 - val_loss: 7208.1685\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7475.6118 - val_loss: 6862.7949\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7549.4990 - val_loss: 7130.2402\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7346.2788 - val_loss: 7295.2231\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7362.0249 - val_loss: 7049.0581\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7364.5312 - val_loss: 6896.2334\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7406.7188 - val_loss: 7144.9111\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7382.3604 - val_loss: 7340.1650\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7426.2456 - val_loss: 7129.3423\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7348.6040 - val_loss: 7122.4458\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7472.6582 - val_loss: 7097.3071\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7436.7886 - val_loss: 7158.0923\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7504.9268 - val_loss: 7397.6616\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7356.3071 - val_loss: 7044.0151\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7508.8853 - val_loss: 6972.1714\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7471.7939 - val_loss: 7138.0142\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7307.4663 - val_loss: 7260.3232\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7404.7715 - val_loss: 7153.6646\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7448.3340 - val_loss: 7182.5732\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7327.7456 - val_loss: 7373.9077\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7452.5693 - val_loss: 6826.4058\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7366.3687 - val_loss: 7354.9150\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7361.3652 - val_loss: 7499.8818\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7561.6992 - val_loss: 7272.0386\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7374.7939 - val_loss: 7349.6870\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7223.2100 - val_loss: 7214.0918\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7366.0684 - val_loss: 7077.7334\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7354.3574 - val_loss: 7053.5688\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7173.6396 - val_loss: 6862.7563\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7107.4136 - val_loss: 7129.5410\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7104.7778 - val_loss: 7270.7749\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7392.8599 - val_loss: 7217.5273\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7228.4219 - val_loss: 6820.8232\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7215.6670 - val_loss: 6893.2524\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7262.1924 - val_loss: 6867.0293\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7158.7773 - val_loss: 7043.6211\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7172.1187 - val_loss: 7089.9878\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7200.4980 - val_loss: 7158.3804\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7228.1699 - val_loss: 7549.3267\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7351.4355 - val_loss: 7035.5269\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7155.4141 - val_loss: 7139.5449\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7174.3042 - val_loss: 7222.3989\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - ETA: 0s - loss: 7174.43 - 0s 2ms/step - loss: 7169.9502 - val_loss: 7128.3481\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7285.8740 - val_loss: 7124.2764\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7222.4014 - val_loss: 6971.5674\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7156.5283 - val_loss: 7003.1724\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7127.0088 - val_loss: 7120.6680\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7237.6274 - val_loss: 6947.7656\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7144.0254 - val_loss: 7416.1631\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7222.7773 - val_loss: 7078.8022\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7047.0889 - val_loss: 7032.8076\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7196.7168 - val_loss: 7142.9814\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7157.1099 - val_loss: 7128.6558\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7111.3042 - val_loss: 7325.1123\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7067.0347 - val_loss: 6792.3999\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7131.0015 - val_loss: 6835.2510\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7090.9829 - val_loss: 7067.2671\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7105.9854 - val_loss: 7050.9922\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7192.9668 - val_loss: 7066.4980\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7301.2476 - val_loss: 7139.5991\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7007.0625 - val_loss: 7273.8740\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7310.3037 - val_loss: 7157.0801\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7098.1396 - val_loss: 7337.1357\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7206.7095 - val_loss: 7120.6406\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7278.2393 - val_loss: 6849.6860\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6978.8452 - val_loss: 7053.4575\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7172.9468 - val_loss: 7493.6128\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7038.7422 - val_loss: 6908.9922\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7129.3677 - val_loss: 6961.5654\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7079.7842 - val_loss: 7136.0249\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7059.0029 - val_loss: 7205.1523\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7252.9141 - val_loss: 7327.8022\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7019.9824 - val_loss: 7245.2715\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7021.2217 - val_loss: 6945.0347\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7164.2925 - val_loss: 6662.7480\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7124.2490 - val_loss: 7366.3984\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7205.7158 - val_loss: 6963.9380\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7031.8281 - val_loss: 7020.9111\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6975.6504 - val_loss: 7152.8135\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7114.9438 - val_loss: 7657.9717\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7307.1860 - val_loss: 7106.7920\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7016.9609 - val_loss: 6873.8945\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6951.9653 - val_loss: 6886.8354\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7084.1270 - val_loss: 7039.1792\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6963.9912 - val_loss: 6614.5469\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7071.2563 - val_loss: 7067.6899\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7107.1045 - val_loss: 6967.6431\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7118.3120 - val_loss: 6890.8740\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7074.8667 - val_loss: 6836.8320\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7022.3633 - val_loss: 7079.3643\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7063.1284 - val_loss: 6793.8853\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6993.4175 - val_loss: 7427.4009\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7122.9155 - val_loss: 6731.4946\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6821.0288 - val_loss: 6869.1055\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7032.3340 - val_loss: 6861.7676\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6977.9131 - val_loss: 6876.3042\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7086.0718 - val_loss: 7224.0430\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7066.5537 - val_loss: 6956.4468\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7039.5259 - val_loss: 7000.9473\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6967.2422 - val_loss: 6787.4766\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7072.9199 - val_loss: 7304.6689\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6953.1123 - val_loss: 7046.8242\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7196.1748 - val_loss: 7017.9351\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6843.3442 - val_loss: 6779.9438\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7010.8789 - val_loss: 6894.6626\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7025.4175 - val_loss: 6910.1138\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7196.7900 - val_loss: 6805.1040\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6926.6489 - val_loss: 7015.4067\n",
      "Epoch 192/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7184.3721 - val_loss: 7055.7622\n",
      "Epoch 193/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7057.5874 - val_loss: 6829.1782\n",
      "Epoch 194/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7054.1499 - val_loss: 6606.4204\n",
      "Epoch 195/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7061.3340 - val_loss: 6960.0898\n",
      "Epoch 196/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7182.1040 - val_loss: 7219.5669\n",
      "Epoch 197/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6973.2988 - val_loss: 7079.9731\n",
      "Epoch 198/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6903.3940 - val_loss: 6675.7368\n",
      "Epoch 199/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6835.7793 - val_loss: 6966.2842\n",
      "Epoch 200/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6985.1118 - val_loss: 6883.1104\n",
      "Epoch 201/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6948.0713 - val_loss: 6674.4033\n",
      "Epoch 202/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7027.1064 - val_loss: 6610.9517\n",
      "Epoch 203/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7084.2363 - val_loss: 6997.8970\n",
      "Epoch 204/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6915.5029 - val_loss: 6985.7524\n",
      "Epoch 205/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7158.3389 - val_loss: 6807.8354\n",
      "Epoch 206/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6838.6997 - val_loss: 7132.2603\n",
      "Epoch 207/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6867.9302 - val_loss: 6942.1816\n",
      "Epoch 208/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7024.0454 - val_loss: 6828.1582\n",
      "Epoch 209/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6965.3140 - val_loss: 7410.6167\n",
      "Epoch 210/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6940.4307 - val_loss: 6866.2964\n",
      "Epoch 211/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6803.6538 - val_loss: 7069.0625\n",
      "Epoch 212/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6882.5581 - val_loss: 7262.1069\n",
      "Epoch 213/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7078.3496 - val_loss: 7465.5825\n",
      "Epoch 214/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6904.1987 - val_loss: 6860.7231\n",
      "Epoch 215/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6906.3608 - val_loss: 6872.6313\n",
      "Epoch 216/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6993.4624 - val_loss: 7085.5522\n",
      "Epoch 217/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6891.0518 - val_loss: 7348.0400\n",
      "Epoch 218/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6944.9429 - val_loss: 6820.1821\n",
      "Epoch 219/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6934.0674 - val_loss: 6736.3716\n",
      "Epoch 220/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6893.0996 - val_loss: 7149.0728\n",
      "Epoch 221/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6932.5269 - val_loss: 6936.3013\n",
      "Epoch 222/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7009.7178 - val_loss: 7129.8076\n",
      "Epoch 223/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6900.7202 - val_loss: 7238.0298\n",
      "Epoch 224/300\n",
      "29/53 [===============>..............] - ETA: 0s - loss: 6645.1860\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6915.7876 - val_loss: 6906.0591\n",
      "Epoch 225/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6893.1650 - val_loss: 7224.3550\n",
      "Epoch 226/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6793.1011 - val_loss: 7028.2778\n",
      "Epoch 227/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6931.9438 - val_loss: 6976.3906\n",
      "Epoch 228/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6743.5854 - val_loss: 6965.2949\n",
      "Epoch 229/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6957.1279 - val_loss: 7324.2905\n",
      "Epoch 230/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6786.8228 - val_loss: 7026.0825\n",
      "Epoch 231/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6851.0835 - val_loss: 6919.7690\n",
      "Epoch 232/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6773.9414 - val_loss: 7218.7227\n",
      "Epoch 233/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6875.3975 - val_loss: 7201.7900\n",
      "Epoch 234/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6754.0835 - val_loss: 6645.0171\n",
      "Epoch 235/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6813.1230 - val_loss: 7333.9199\n",
      "Epoch 236/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6584.3599 - val_loss: 6732.2368\n",
      "Epoch 237/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6645.7686 - val_loss: 7100.4165\n",
      "Epoch 238/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6774.1670 - val_loss: 6788.1768\n",
      "Epoch 239/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6642.4131 - val_loss: 6968.4429\n",
      "Epoch 240/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6774.7354 - val_loss: 6998.3521\n",
      "Epoch 241/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6787.7363 - val_loss: 6725.2134\n",
      "Epoch 242/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6824.8623 - val_loss: 7002.2700\n",
      "Epoch 243/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6779.9810 - val_loss: 6802.3149\n",
      "Epoch 244/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6742.6323 - val_loss: 6955.9849\n",
      "6606.42041015625\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=False, group_number=3)\n",
    "deneyelim.get_raw_data()\n",
    "deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T8</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-04-30 15:10:00</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>35.633333</td>\n",
       "      <td>21.20</td>\n",
       "      <td>34.256667</td>\n",
       "      <td>24.493333</td>\n",
       "      <td>35.393333</td>\n",
       "      <td>20.10</td>\n",
       "      <td>37.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>38.090000</td>\n",
       "      <td>19.390000</td>\n",
       "      <td>38.363333</td>\n",
       "      <td>10.533333</td>\n",
       "      <td>759.650000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-15 18:30:00</th>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>37.590000</td>\n",
       "      <td>22.18</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>24.290000</td>\n",
       "      <td>35.564286</td>\n",
       "      <td>24.20</td>\n",
       "      <td>34.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>25.790000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>22.790000</td>\n",
       "      <td>33.718000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>760.950000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>34.5</td>\n",
       "      <td>4.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-10 19:50:00</th>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>39.693333</td>\n",
       "      <td>19.79</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>21.230000</td>\n",
       "      <td>39.363333</td>\n",
       "      <td>21.39</td>\n",
       "      <td>35.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.426667</td>\n",
       "      <td>36.730000</td>\n",
       "      <td>18.133333</td>\n",
       "      <td>39.933333</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>761.283333</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-18 04:10:00</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>20.633333</td>\n",
       "      <td>40.530000</td>\n",
       "      <td>19.00</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>40.290000</td>\n",
       "      <td>20.00</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.890000</td>\n",
       "      <td>47.590000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>40.933333</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>761.300000</td>\n",
       "      <td>85.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-6.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-25 11:40:00</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>20.066667</td>\n",
       "      <td>44.090000</td>\n",
       "      <td>19.39</td>\n",
       "      <td>43.326667</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>44.290000</td>\n",
       "      <td>18.20</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.088889</td>\n",
       "      <td>48.322778</td>\n",
       "      <td>16.633333</td>\n",
       "      <td>48.590000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>763.300000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1     T2  \\\n",
       "date                                                                   \n",
       "2016-04-30 15:10:00          90       0  21.890000  35.633333  21.20   \n",
       "2016-05-15 18:30:00         100      30  23.600000  37.590000  22.18   \n",
       "2016-03-10 19:50:00         150      20  22.000000  39.693333  19.79   \n",
       "2016-01-18 04:10:00          40       0  20.633333  40.530000  19.00   \n",
       "2016-01-25 11:40:00          30       0  20.066667  44.090000  19.39   \n",
       "\n",
       "                          RH_2         T3       RH_3     T4       RH_4  ...  \\\n",
       "date                                                                    ...   \n",
       "2016-04-30 15:10:00  34.256667  24.493333  35.393333  20.10  37.066667  ...   \n",
       "2016-05-15 18:30:00  37.090000  24.290000  35.564286  24.20  34.590000  ...   \n",
       "2016-03-10 19:50:00  40.400000  21.230000  39.363333  21.39  35.900000  ...   \n",
       "2016-01-18 04:10:00  40.400000  20.260000  40.290000  20.00  38.000000  ...   \n",
       "2016-01-25 11:40:00  43.326667  20.200000  44.290000  18.20  45.560000  ...   \n",
       "\n",
       "                            T8       RH_8         T9       RH_9      T_out  \\\n",
       "date                                                                         \n",
       "2016-04-30 15:10:00  22.700000  38.090000  19.390000  38.363333  10.533333   \n",
       "2016-05-15 18:30:00  25.790000  35.000000  22.790000  33.718000  11.700000   \n",
       "2016-03-10 19:50:00  22.426667  36.730000  18.133333  39.933333   6.083333   \n",
       "2016-01-18 04:10:00  20.890000  47.590000  17.600000  40.933333  -4.000000   \n",
       "2016-01-25 11:40:00  18.088889  48.322778  16.633333  48.590000  11.600000   \n",
       "\n",
       "                     Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint  \n",
       "date                                                                           \n",
       "2016-04-30 15:10:00   759.650000  59.500000   5.166667        40.0   2.933333  \n",
       "2016-05-15 18:30:00   760.950000  60.000000   3.000000        34.5   4.150000  \n",
       "2016-03-10 19:50:00   761.283333  67.333333   2.000000        65.0   0.450000  \n",
       "2016-01-18 04:10:00   761.300000  85.666667   3.000000        27.0  -6.133333  \n",
       "2016-01-25 11:40:00   763.300000  60.000000   4.000000        40.0   3.933333  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 19, 15, 0, 2, 10, 24, 9],\n",
       " [14, 11, 18, 13, 16, 6, 20, 4],\n",
       " [17, 12, 3, 1, 22, 23, 5, 7, 21]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups=deneyelim.get_grouped_feature_cols()\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  0.769623 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:10:00  0.762633 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:20:00  0.729854 -0.810232 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:30:00  0.739495 -0.834542 -1.983431  0.786817 -1.385479   \n",
       "  2016-01-11 17:40:00  0.792640 -0.834542 -1.983431  0.786817 -1.359694   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.294374 -0.569336 -0.106203  1.096424  0.110056   \n",
       "  2016-04-30 07:40:00 -0.287143 -0.609117 -0.106203  1.070624  0.058486   \n",
       "  2016-04-30 07:50:00 -0.281118 -0.569336 -0.159166  1.044823  0.110056   \n",
       "  2016-04-30 08:00:00 -0.300159 -0.569336 -0.159166  1.019022  0.161626   \n",
       "  2016-04-30 08:10:00 -0.329323 -0.569336 -0.159166  0.928720  0.257031   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-01-11 17:00:00   1.065608 -1.217218  1.554962  \n",
       "  2016-01-11 17:10:00   0.935169 -1.193835  1.654543  \n",
       "  2016-01-11 17:20:00   0.804731 -1.240601  1.630574  \n",
       "  2016-01-11 17:30:00   0.674293 -1.240601  1.591599  \n",
       "  2016-01-11 17:40:00   0.543854 -1.240601  1.546388  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-04-30 07:30:00  -1.282284  0.302667 -0.310373  \n",
       "  2016-04-30 07:40:00  -1.282284  0.302667 -0.279193  \n",
       "  2016-04-30 07:50:00  -1.282284  0.302667 -0.081980  \n",
       "  2016-04-30 08:00:00  -1.282284  0.302667 -0.050800  \n",
       "  2016-04-30 08:10:00  -1.282284  0.302667 -0.036769  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-01-11 17:00:00 -1.039517   0.729188  3.071295  0.180377  1.014203   \n",
       "  2016-01-11 17:10:00 -1.039517   0.700803  3.071295  0.152122  1.021582   \n",
       "  2016-01-11 17:20:00 -1.039517   0.672418  3.071295  0.123867  1.006824   \n",
       "  2016-01-11 17:30:00 -1.039517   0.644033  4.267005  0.095612  0.982227   \n",
       "  2016-01-11 17:40:00 -1.039517   0.615647  4.267005  0.067357  0.982227   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00  0.113503   0.487914 -0.515833 -0.182903  0.029511   \n",
       "  2016-04-30 07:40:00  0.168409   0.502106 -0.515833 -0.158684  0.014753   \n",
       "  2016-04-30 07:50:00  0.195862   0.516299 -0.515833 -0.134466 -0.001645   \n",
       "  2016-04-30 08:00:00  0.195862   0.530492 -0.515833 -0.110247 -0.059858   \n",
       "  2016-04-30 08:10:00  0.195862   0.521030 -0.515833 -0.077955 -0.092653   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-01-11 17:00:00  1.681976  2.103814 -1.200652  \n",
       "  2016-01-11 17:10:00  1.700697  1.852407 -1.200652  \n",
       "  2016-01-11 17:20:00  1.745419  1.742937 -1.200652  \n",
       "  2016-01-11 17:30:00  1.766219  1.677998 -1.200652  \n",
       "  2016-01-11 17:40:00  1.766219  1.752214 -1.200652  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-04-30 07:30:00 -0.667471 -0.771142  0.801833  \n",
       "  2016-04-30 07:40:00 -0.752754 -0.661673  0.761783  \n",
       "  2016-04-30 07:50:00 -0.906680 -0.580962  0.675009  \n",
       "  2016-04-30 08:00:00 -0.867158 -0.567047  0.675009  \n",
       "  2016-04-30 08:10:00 -0.885879 -0.457578  0.675009  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  1.311407 -1.494334  0.434884  1.246472  1.160518   \n",
       "  2016-01-11 17:10:00  1.298168 -1.494334  0.434884  1.226599  1.153506   \n",
       "  2016-01-11 17:20:00  1.273015 -1.494334  0.422695  1.198385  1.128006   \n",
       "  2016-01-11 17:30:00  1.244551 -1.536665  0.422695  1.187590  1.101232   \n",
       "  2016-01-11 17:40:00  1.232637 -1.494334  0.422695  1.169926  1.101232   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.284520 -0.287906 -0.894794 -0.045972  0.350278   \n",
       "  2016-04-30 07:40:00 -0.277239 -0.287906 -0.902920  0.009966  0.336253   \n",
       "  2016-04-30 07:50:00 -0.250761 -0.287906 -0.912246 -0.007699  0.305654   \n",
       "  2016-04-30 08:00:00 -0.237523 -0.287906 -0.961647 -0.078356  0.267405   \n",
       "  2016-04-30 08:10:00 -0.222960 -0.287906 -0.995628 -0.078356  0.210031   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-01-11 17:00:00    1.961544  0.188895 -0.284588    -2.815843  \n",
       "  2016-01-11 17:10:00    1.652462  0.147545 -0.284588    -2.802987  \n",
       "  2016-01-11 17:20:00    1.343380  0.089083 -0.284588    -2.790130  \n",
       "  2016-01-11 17:30:00    1.034298  0.061991 -0.284588    -2.777274  \n",
       "  2016-01-11 17:40:00    0.725216  0.047732 -0.284588    -2.764417  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-04-30 07:30:00    0.107051  0.131860 -0.554938     0.314740  \n",
       "  2016-04-30 07:40:00    0.107051  0.188895 -0.488999     0.325453  \n",
       "  2016-04-30 07:50:00    0.107051  0.311522 -0.440644     0.336167  \n",
       "  2016-04-30 08:00:00    0.107051  0.382817 -0.416466     0.346881  \n",
       "  2016-04-30 08:10:00    0.107051  0.445556 -0.416466     0.355452  \n",
       "  \n",
       "  [15788 rows x 9 columns],\n",
       "  date\n",
       "  2016-01-11 17:00:00     60\n",
       "  2016-01-11 17:10:00     60\n",
       "  2016-01-11 17:20:00     50\n",
       "  2016-01-11 17:30:00     50\n",
       "  2016-01-11 17:40:00     60\n",
       "                        ... \n",
       "  2016-04-30 07:30:00     80\n",
       "  2016-04-30 07:40:00     80\n",
       "  2016-04-30 07:50:00     50\n",
       "  2016-04-30 08:00:00     70\n",
       "  2016-04-30 08:10:00    300\n",
       "  Name: Appliances, Length: 15788, dtype: int64],\n",
       " [                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.365476 -0.569336 -0.159166  0.838418  0.210618   \n",
       "  2016-04-30 08:30:00 -0.430311 -0.525135 -0.159166  0.748116  0.187411   \n",
       "  2016-04-30 08:40:00 -0.534192 -0.503035 -0.198397  0.657814  0.187411   \n",
       "  2016-04-30 08:50:00 -0.740207 -0.454414 -0.159166  0.567512  0.187411   \n",
       "  2016-04-30 09:00:00 -0.789677 -0.430103 -0.159166  0.477210  0.187411   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00 -2.240395  3.017574  1.900489 -2.025449  3.281621   \n",
       "  2016-05-27 17:30:00 -2.240395  3.017574  1.900489 -1.999648  3.304827   \n",
       "  2016-05-27 17:40:00 -2.240395  3.017574  1.900489 -1.973847  3.304827   \n",
       "  2016-05-27 17:50:00 -2.240395  3.017574  1.878421 -1.948047  3.281621   \n",
       "  2016-05-27 18:00:00 -2.240395  3.017574  1.921674 -1.922246  3.281621   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-04-30 08:20:00  -1.282284  0.302667 -0.043785  \n",
       "  2016-04-30 08:30:00  -1.282284  0.302667  0.017016  \n",
       "  2016-04-30 08:40:00  -1.282284  0.302667  0.095745  \n",
       "  2016-04-30 08:50:00  -1.282284  0.302667  0.128484  \n",
       "  2016-04-30 09:00:00  -1.282284  0.365800  0.136279  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-05-27 17:20:00  -0.369215  3.108607  1.560419  \n",
       "  2016-05-27 17:30:00  -0.303996  3.108607  1.560419  \n",
       "  2016-05-27 17:40:00  -0.238776  3.108607  1.593158  \n",
       "  2016-05-27 17:50:00  -0.173557  3.108607  1.607189  \n",
       "  2016-05-27 18:00:00  -0.108338  3.108607  1.647723  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-04-30 08:20:00  0.195862   0.511568 -0.515833 -0.045664 -0.159065   \n",
       "  2016-04-30 08:30:00  0.195862   0.502106 -0.515833 -0.013372 -0.206618   \n",
       "  2016-04-30 08:40:00  0.195862   0.492645 -0.515833  0.018919 -0.256632   \n",
       "  2016-04-30 08:50:00  0.195862   0.483183  0.679876  0.051211 -0.274670   \n",
       "  2016-04-30 09:00:00  0.140956   0.473721 -0.515833  0.083502 -0.247613   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  3.635705   3.009470 -0.515833  4.087654  1.324122   \n",
       "  2016-05-27 17:30:00  3.580799   3.000008 -0.515833  4.055362  1.324122   \n",
       "  2016-05-27 17:40:00  3.580799   2.990547  0.679876  4.023071  1.324122   \n",
       "  2016-05-27 17:50:00  3.580799   2.981085  0.679876  3.990779  1.330886   \n",
       "  2016-05-27 18:00:00  3.580799   2.971623  0.679876  3.958488  1.337651   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-04-30 08:20:00 -0.885879 -0.494686  0.675009  \n",
       "  2016-04-30 08:30:00 -0.871838 -0.531794  0.675009  \n",
       "  2016-04-30 08:40:00 -0.857798 -0.541999  0.697259  \n",
       "  2016-04-30 08:50:00 -0.857798 -0.558697  0.741758  \n",
       "  2016-04-30 09:00:00 -0.857798 -0.594878  0.741758  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-05-27 17:20:00  0.569135  1.815298  3.745485  \n",
       "  2016-05-27 17:30:00  0.587856  1.798600  3.700986  \n",
       "  2016-05-27 17:40:00  0.733461  1.825503  3.645361  \n",
       "  2016-05-27 17:50:00  0.608657  1.934972  3.538562  \n",
       "  2016-05-27 18:00:00  0.567055  1.826431  3.494062  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.222960 -0.287906 -0.995628 -0.056767  0.171782   \n",
       "  2016-04-30 08:30:00 -0.222960 -0.287906 -0.958323 -0.096021  0.139271   \n",
       "  2016-04-30 08:40:00 -0.237523 -0.287906 -0.943549 -0.155883  0.127796   \n",
       "  2016-04-30 08:50:00 -0.244804 -0.287906 -0.928036 -0.232429  0.127796   \n",
       "  2016-04-30 09:00:00 -0.244804 -0.287906 -0.936162 -0.370800  0.139271   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  1.881996  3.140891  0.124626  0.432649  1.385039   \n",
       "  2016-05-27 17:30:00  1.864974  3.177175  0.116500  0.448631  1.330726   \n",
       "  2016-05-27 17:40:00  1.862138  3.166290  0.109851  0.651350  1.305864   \n",
       "  2016-05-27 17:50:00  1.841428  3.140891  0.102464  0.730083  1.278851   \n",
       "  2016-05-27 18:00:00  1.793429  3.140891  0.102464  0.711073  1.320398   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-04-30 08:20:00    0.107051  0.489759 -0.328547     0.364023  \n",
       "  2016-04-30 08:30:00    0.107051  0.595988 -0.284588     0.372594  \n",
       "  2016-04-30 08:40:00    0.107051  0.667283 -0.242827     0.381165  \n",
       "  2016-04-30 08:50:00    0.107051  0.749985 -0.066989     0.389736  \n",
       "  2016-04-30 09:00:00    0.107051  0.959591  0.064889     0.398307  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-05-27 17:20:00   -1.209907  3.989616  4.126733    -0.025960  \n",
       "  2016-05-27 17:30:00   -1.142716  3.861286  4.037056    -0.025960  \n",
       "  2016-05-27 17:40:00   -1.075524  3.739372  3.954349    -0.025960  \n",
       "  2016-05-27 17:50:00   -1.008332  3.484137  3.812863    -0.025960  \n",
       "  2016-05-27 18:00:00   -0.941140  3.183273  3.714143    -0.025960  \n",
       "  \n",
       "  [3947 rows x 9 columns],\n",
       "  date\n",
       "  2016-04-30 08:20:00    370\n",
       "  2016-04-30 08:30:00    590\n",
       "  2016-04-30 08:40:00    320\n",
       "  2016-04-30 08:50:00    310\n",
       "  2016-04-30 09:00:00    260\n",
       "                        ... \n",
       "  2016-05-27 17:20:00    100\n",
       "  2016-05-27 17:30:00     90\n",
       "  2016-05-27 17:40:00    270\n",
       "  2016-05-27 17:50:00    420\n",
       "  2016-05-27 18:00:00    430\n",
       "  Name: Appliances, Length: 3947, dtype: int64]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [None]*3\n",
    "b = [a,a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, 5, None], [None, 5, None]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][1] = 5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[None]*3]*2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, 5], [None, None, 5]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]=5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[None for _ in range(3)] for _ in range(2)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, 2]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][2]=2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,4,5]\n",
    "(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[2,4,6],[1,3,5]]\n",
    "c[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
