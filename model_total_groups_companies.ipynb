{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 2, company_number = 1):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.company_number = company_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self, index_col=0):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups, random_seed=41):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.7*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.7*row_num), 0]\n",
    "        valx_df = self.raw_data.iloc[int(0.7*row_num):int(0.85*row_num), 1:]\n",
    "        valy_df = self.raw_data.iloc[int(0.7*row_num):int(0.85*row_num), 0] \n",
    "        testx_df = self.raw_data.iloc[int(0.85*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.85*row_num):, 0]         \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        #trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        #textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(3)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_df.iloc[:,group]\n",
    "            train_temp_companies = self.transform_dataset(df=train_temp, random_seed=random_seed)\n",
    "            features_and_labels[0][index] = train_temp_companies.values\n",
    "            val_temp = valx_df.iloc[:,group]\n",
    "            val_temp_companies = self.transform_dataset(df=val_temp, random_seed=random_seed)\n",
    "            features_and_labels[1][index] = val_temp_companies.values            \n",
    "            test_temp = testx_df.iloc[:,group]\n",
    "            test_temp_companies = self.transform_dataset(df=test_temp, random_seed=random_seed)            \n",
    "            features_and_labels[2][index] = test_temp_companies.values\n",
    "        #trainy_df_companies = trainy_df.sample(frac=1, random_state=random_seed)\n",
    "        #testy_df_companies = testy_df.sample(frac=1, random_state=random_seed)        \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = valy_df.values   \n",
    "        features_and_labels[2][self.group_number] = testy_df.values   \n",
    "        \n",
    "        return features_and_labels\n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    \n",
    "    #For a 10k row telecommunication data, it splits data into #company_num row groups and transforms them independently, adds one-hot encoding.\n",
    "    #There is no label column here, all columns are features.\n",
    "    def transform_dataset(self, df, random_seed=41):\n",
    "        company_num = self.company_number\n",
    "        col_num = len(df.columns)\n",
    "        row_num = len(df.index)\n",
    "        dfs = [None]*company_num\n",
    "        dfs_features = [None]*company_num\n",
    "        dfs_scaleds = [None]*company_num\n",
    "        dfs_new = [None]*company_num\n",
    "        scaler = StandardScaler()\n",
    "        for i in range(company_num):\n",
    "            dfs[i] = df.iloc[int(i/company_num*row_num):int((i+1)/company_num*row_num), :]\n",
    "            dfs[i] = dfs[i].sample(frac=1, axis=1, random_state=random_seed)\n",
    "            df_features_scaled_temp = pd.DataFrame(scaler.fit_transform(dfs[i]), columns = dfs[i].columns, index = dfs[i].index)\n",
    "            dfs_new[i] = df_features_scaled_temp\n",
    "            dfs_new[i]['group'] = i\n",
    "            cols_num = len(dfs_new[i].columns)\n",
    "            col_names = [j for j in range(cols_num)]\n",
    "            dfs_new[i].columns = col_names\n",
    "        df_final = pd.concat(dfs_new, axis=0)\n",
    "        last_col_num = cols_num - 1\n",
    "        df_new = df_final.rename(columns={last_col_num: 'group'})\n",
    "        df_final_onehot = pd.concat([df_new.iloc[:,:-1], pd.get_dummies(df_new.group, prefix='group')], axis=1)\n",
    "        #df_final_onehot = df_final_onehot.sample(frac=1, random_state=random_seed)\n",
    "        return df_final_onehot\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size+self.company_number))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size+self.company_number))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32, activation=\"relu\"):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]+self.company_number), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=activation)(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "        \n",
    "    def default_exp_house(self, batch_size = 32):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes, activation=\"sigmoid\")\n",
    "        checkpoint_filepath = 'tmp/checkpoint'\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6),\n",
    "                keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)]  \n",
    "        \n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        \n",
    "        \n",
    "        training_acc = history.history[\"val_accuracy\"]\n",
    "        MICS_model.load_weights(checkpoint_filepath)\n",
    "        result = MICS_model.evaluate(x = features_and_labels[2][:-1], y = features_and_labels[2][-1])[1]\n",
    "        \n",
    "        print(\"best test accuracy is: \" + str(result))\n",
    "        return result\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>T8</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-03-03 21:50:00</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>22.421429</td>\n",
       "      <td>36.928571</td>\n",
       "      <td>21.00</td>\n",
       "      <td>37.457143</td>\n",
       "      <td>21.347143</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>20.1625</td>\n",
       "      <td>34.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.952857</td>\n",
       "      <td>39.510000</td>\n",
       "      <td>18.403750</td>\n",
       "      <td>36.733750</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>749.466667</td>\n",
       "      <td>91.833333</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>23.833333</td>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-24 05:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>20.390000</td>\n",
       "      <td>39.560000</td>\n",
       "      <td>18.20</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>40.590000</td>\n",
       "      <td>19.8900</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.290000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>18.290000</td>\n",
       "      <td>44.121429</td>\n",
       "      <td>-1.400000</td>\n",
       "      <td>760.000000</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>-1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-04 08:40:00</th>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>20.790000</td>\n",
       "      <td>43.663333</td>\n",
       "      <td>20.00</td>\n",
       "      <td>42.633333</td>\n",
       "      <td>21.676471</td>\n",
       "      <td>42.827059</td>\n",
       "      <td>20.1000</td>\n",
       "      <td>42.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>47.327778</td>\n",
       "      <td>18.390000</td>\n",
       "      <td>47.590000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>762.866667</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>3.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-17 15:30:00</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>34.566667</td>\n",
       "      <td>20.20</td>\n",
       "      <td>33.976000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>21.2000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.125000</td>\n",
       "      <td>39.290000</td>\n",
       "      <td>18.390000</td>\n",
       "      <td>39.126667</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>765.150000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-02 11:20:00</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>20.890000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>19.15</td>\n",
       "      <td>42.145000</td>\n",
       "      <td>21.790000</td>\n",
       "      <td>37.290000</td>\n",
       "      <td>19.7900</td>\n",
       "      <td>38.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>39.590000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>37.590000</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>758.000000</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30 15:10:00</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>35.633333</td>\n",
       "      <td>21.20</td>\n",
       "      <td>34.256667</td>\n",
       "      <td>24.493333</td>\n",
       "      <td>35.393333</td>\n",
       "      <td>20.1000</td>\n",
       "      <td>37.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>38.090000</td>\n",
       "      <td>19.390000</td>\n",
       "      <td>38.363333</td>\n",
       "      <td>10.533333</td>\n",
       "      <td>759.650000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-15 18:30:00</th>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>37.590000</td>\n",
       "      <td>22.18</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>24.290000</td>\n",
       "      <td>35.564286</td>\n",
       "      <td>24.2000</td>\n",
       "      <td>34.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>25.790000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>22.790000</td>\n",
       "      <td>33.718000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>760.950000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>4.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-10 19:50:00</th>\n",
       "      <td>150</td>\n",
       "      <td>20</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>39.693333</td>\n",
       "      <td>19.79</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>21.230000</td>\n",
       "      <td>39.363333</td>\n",
       "      <td>21.3900</td>\n",
       "      <td>35.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.426667</td>\n",
       "      <td>36.730000</td>\n",
       "      <td>18.133333</td>\n",
       "      <td>39.933333</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>761.283333</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-18 04:10:00</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>20.633333</td>\n",
       "      <td>40.530000</td>\n",
       "      <td>19.00</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>40.290000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.890000</td>\n",
       "      <td>47.590000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>40.933333</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>761.300000</td>\n",
       "      <td>85.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-6.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-25 11:40:00</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>20.066667</td>\n",
       "      <td>44.090000</td>\n",
       "      <td>19.39</td>\n",
       "      <td>43.326667</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>44.290000</td>\n",
       "      <td>18.2000</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.088889</td>\n",
       "      <td>48.322778</td>\n",
       "      <td>16.633333</td>\n",
       "      <td>48.590000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>763.300000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Appliances  lights         T1       RH_1     T2  \\\n",
       "date                                                                   \n",
       "2016-03-03 21:50:00          60       0  22.421429  36.928571  21.00   \n",
       "2016-02-24 05:20:00          50       0  20.390000  39.560000  18.20   \n",
       "2016-02-04 08:40:00          60      20  20.790000  43.663333  20.00   \n",
       "2016-01-17 15:30:00          70       0  21.700000  34.566667  20.20   \n",
       "2016-04-02 11:20:00          50       0  20.890000  39.400000  19.15   \n",
       "...                         ...     ...        ...        ...    ...   \n",
       "2016-04-30 15:10:00          90       0  21.890000  35.633333  21.20   \n",
       "2016-05-15 18:30:00         100      30  23.600000  37.590000  22.18   \n",
       "2016-03-10 19:50:00         150      20  22.000000  39.693333  19.79   \n",
       "2016-01-18 04:10:00          40       0  20.633333  40.530000  19.00   \n",
       "2016-01-25 11:40:00          30       0  20.066667  44.090000  19.39   \n",
       "\n",
       "                          RH_2         T3       RH_3       T4       RH_4  ...  \\\n",
       "date                                                                      ...   \n",
       "2016-03-03 21:50:00  37.457143  21.347143  37.500000  20.1625  34.590000  ...   \n",
       "2016-02-24 05:20:00  41.000000  21.200000  40.590000  19.8900  38.400000  ...   \n",
       "2016-02-04 08:40:00  42.633333  21.676471  42.827059  20.1000  42.530000  ...   \n",
       "2016-01-17 15:30:00  33.976000  21.000000  36.400000  21.2000  35.500000  ...   \n",
       "2016-04-02 11:20:00  42.145000  21.790000  37.290000  19.7900  38.290000  ...   \n",
       "...                        ...        ...        ...      ...        ...  ...   \n",
       "2016-04-30 15:10:00  34.256667  24.493333  35.393333  20.1000  37.066667  ...   \n",
       "2016-05-15 18:30:00  37.090000  24.290000  35.564286  24.2000  34.590000  ...   \n",
       "2016-03-10 19:50:00  40.400000  21.230000  39.363333  21.3900  35.900000  ...   \n",
       "2016-01-18 04:10:00  40.400000  20.260000  40.290000  20.0000  38.000000  ...   \n",
       "2016-01-25 11:40:00  43.326667  20.200000  44.290000  18.2000  45.560000  ...   \n",
       "\n",
       "                            T8       RH_8         T9       RH_9      T_out  \\\n",
       "date                                                                         \n",
       "2016-03-03 21:50:00  21.952857  39.510000  18.403750  36.733750   1.333333   \n",
       "2016-02-24 05:20:00  20.290000  45.790000  18.290000  44.121429  -1.400000   \n",
       "2016-02-04 08:40:00  21.100000  47.327778  18.390000  47.590000   4.000000   \n",
       "2016-01-17 15:30:00  22.125000  39.290000  18.390000  39.126667   2.900000   \n",
       "2016-04-02 11:20:00  21.100000  39.590000  19.600000  37.590000   8.333333   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2016-04-30 15:10:00  22.700000  38.090000  19.390000  38.363333  10.533333   \n",
       "2016-05-15 18:30:00  25.790000  35.000000  22.790000  33.718000  11.700000   \n",
       "2016-03-10 19:50:00  22.426667  36.730000  18.133333  39.933333   6.083333   \n",
       "2016-01-18 04:10:00  20.890000  47.590000  17.600000  40.933333  -4.000000   \n",
       "2016-01-25 11:40:00  18.088889  48.322778  16.633333  48.590000  11.600000   \n",
       "\n",
       "                     Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint  \n",
       "date                                                                           \n",
       "2016-03-03 21:50:00   749.466667  91.833333   2.833333   23.833333   0.116667  \n",
       "2016-02-24 05:20:00   760.000000  96.666667   1.333333   34.333333  -1.900000  \n",
       "2016-02-04 08:40:00   762.866667  94.000000   6.000000   55.333333   3.133333  \n",
       "2016-01-17 15:30:00   765.150000  59.500000   1.500000   40.000000  -4.300000  \n",
       "2016-04-02 11:20:00   758.000000  77.666667   2.000000   53.000000   4.633333  \n",
       "...                          ...        ...        ...         ...        ...  \n",
       "2016-04-30 15:10:00   759.650000  59.500000   5.166667   40.000000   2.933333  \n",
       "2016-05-15 18:30:00   760.950000  60.000000   3.000000   34.500000   4.150000  \n",
       "2016-03-10 19:50:00   761.283333  67.333333   2.000000   65.000000   0.450000  \n",
       "2016-01-18 04:10:00   761.300000  85.666667   3.000000   27.000000  -6.133333  \n",
       "2016-01-25 11:40:00   763.300000  60.000000   4.000000   40.000000   3.933333  \n",
       "\n",
       "[19735 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=True, group_number=2, company_number=1)\n",
    "deneyelim.get_raw_data()\n",
    "deneyelim.raw_data\n",
    "#deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=10, company_number=50)\n",
    "deneyelim2.get_raw_data()\n",
    "a = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_numbers = [2,4,6,8,10]\n",
    "company_numbers = [1,2,4,7,10,20,40]\n",
    "\n",
    "result_matrix = [[[] for _ in range(len(company_numbers))] for _ in range(len(group_numbers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "for group_index in range(len(group_numbers)):\n",
    "    for company_index in range(len(company_numbers)):\n",
    "        for loop in range(5):\n",
    "            deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=group_numbers[group_index], company_number=company_numbers[company_index])\n",
    "            deneyelim2.get_raw_data()\n",
    "            score = deneyelim2.default_exp_house(batch_size=32)\n",
    "            result_matrix[group_index][company_index].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "result_matrix_copy = copy.deepcopy(result_matrix)\n",
    "result_matrix_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_std_matrix = [[[] for _ in range(len(company_numbers))] for _ in range(len(group_numbers))]\n",
    "for group_index in range(len(group_numbers)):\n",
    "    for company_index in range(len(company_numbers)):\n",
    "        current_list = result_matrix_copy[group_index][company_index]\n",
    "        avg_std_matrix[group_index][company_index].append(statistics.mean(current_list)) \n",
    "        avg_std_matrix[group_index][company_index].append(statistics.stdev(current_list)) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.901369845867157, 0.008901180200105482]</td>\n",
       "      <td>[0.9013698697090149, 0.01000404993961545]</td>\n",
       "      <td>[0.8968036532402038, 0.002501039350504474]</td>\n",
       "      <td>[0.8904109597206116, 0.012081049554110651]</td>\n",
       "      <td>[0.8757990837097168, 0.011819354129907245]</td>\n",
       "      <td>[0.8657534122467041, 0.024926657367662853]</td>\n",
       "      <td>[0.8347031950950623, 0.010412549991255569]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.8995433688163758, 0.01070870269931029]</td>\n",
       "      <td>[0.9095890402793885, 0.006772776253837231]</td>\n",
       "      <td>[0.8949771761894226, 0.018826970403172494]</td>\n",
       "      <td>[0.8812785387039185, 0.009132415056238363]</td>\n",
       "      <td>[0.8876712322235107, 0.014654610225575277]</td>\n",
       "      <td>[0.8757990837097168, 0.01780236880875643]</td>\n",
       "      <td>[0.8365296721458435, 0.016901132849862725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.8986301422119141, 0.012670721870541872]</td>\n",
       "      <td>[0.9059360742568969, 0.016014328872232364]</td>\n",
       "      <td>[0.8986301302909852, 0.014221375875773275]</td>\n",
       "      <td>[0.877625572681427, 0.010901684161528903]</td>\n",
       "      <td>[0.8721461176872254, 0.018826959560969136]</td>\n",
       "      <td>[0.8602739691734314, 0.01190721358284922]</td>\n",
       "      <td>[0.8182648420333862, 0.01041255129817017]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.9132420063018799, 0.0055924453351836115]</td>\n",
       "      <td>[0.8885844826698304, 0.01190721358284922]</td>\n",
       "      <td>[0.8794520616531372, 0.011907223868657016]</td>\n",
       "      <td>[0.8931506752967835, 0.006925013290902036]</td>\n",
       "      <td>[0.8721461296081543, 0.028148012965071873]</td>\n",
       "      <td>[0.8648401856422424, 0.01665258945231131]</td>\n",
       "      <td>[0.8228310346603394, 0.01690114895334232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.8995433807373047, 0.007219812741103662]</td>\n",
       "      <td>[0.8995433807373047, 0.007219812741103662]</td>\n",
       "      <td>[0.8867579817771911, 0.015618848947038588]</td>\n",
       "      <td>[0.8767123222351074, 0.021172657396003998]</td>\n",
       "      <td>[0.8803653120994568, 0.052202777648679415]</td>\n",
       "      <td>[0.8584474921226501, 0.01677731978310403]</td>\n",
       "      <td>[0.8191780805587768, 0.013545567577167085]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             1   \\\n",
       "2     [0.901369845867157, 0.008901180200105482]   \n",
       "4     [0.8995433688163758, 0.01070870269931029]   \n",
       "6    [0.8986301422119141, 0.012670721870541872]   \n",
       "8   [0.9132420063018799, 0.0055924453351836115]   \n",
       "10   [0.8995433807373047, 0.007219812741103662]   \n",
       "\n",
       "                                            2   \\\n",
       "2    [0.9013698697090149, 0.01000404993961545]   \n",
       "4   [0.9095890402793885, 0.006772776253837231]   \n",
       "6   [0.9059360742568969, 0.016014328872232364]   \n",
       "8    [0.8885844826698304, 0.01190721358284922]   \n",
       "10  [0.8995433807373047, 0.007219812741103662]   \n",
       "\n",
       "                                            4   \\\n",
       "2   [0.8968036532402038, 0.002501039350504474]   \n",
       "4   [0.8949771761894226, 0.018826970403172494]   \n",
       "6   [0.8986301302909852, 0.014221375875773275]   \n",
       "8   [0.8794520616531372, 0.011907223868657016]   \n",
       "10  [0.8867579817771911, 0.015618848947038588]   \n",
       "\n",
       "                                            7   \\\n",
       "2   [0.8904109597206116, 0.012081049554110651]   \n",
       "4   [0.8812785387039185, 0.009132415056238363]   \n",
       "6    [0.877625572681427, 0.010901684161528903]   \n",
       "8   [0.8931506752967835, 0.006925013290902036]   \n",
       "10  [0.8767123222351074, 0.021172657396003998]   \n",
       "\n",
       "                                            10  \\\n",
       "2   [0.8757990837097168, 0.011819354129907245]   \n",
       "4   [0.8876712322235107, 0.014654610225575277]   \n",
       "6   [0.8721461176872254, 0.018826959560969136]   \n",
       "8   [0.8721461296081543, 0.028148012965071873]   \n",
       "10  [0.8803653120994568, 0.052202777648679415]   \n",
       "\n",
       "                                            20  \\\n",
       "2   [0.8657534122467041, 0.024926657367662853]   \n",
       "4    [0.8757990837097168, 0.01780236880875643]   \n",
       "6    [0.8602739691734314, 0.01190721358284922]   \n",
       "8    [0.8648401856422424, 0.01665258945231131]   \n",
       "10   [0.8584474921226501, 0.01677731978310403]   \n",
       "\n",
       "                                            40  \n",
       "2   [0.8347031950950623, 0.010412549991255569]  \n",
       "4   [0.8365296721458435, 0.016901132849862725]  \n",
       "6    [0.8182648420333862, 0.01041255129817017]  \n",
       "8    [0.8228310346603394, 0.01690114895334232]  \n",
       "10  [0.8191780805587768, 0.013545567577167085]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(avg_std_matrix, columns=company_numbers, index=group_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
