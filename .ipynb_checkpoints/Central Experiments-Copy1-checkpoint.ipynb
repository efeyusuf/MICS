{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Datasets/houseprices_ready.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AboveMedianPrice</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>14115</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>796</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>10084</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1686</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>10382</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7420</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AboveMedianPrice  LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  \\\n",
       "0                 1     8450            7            5          856         2   \n",
       "1                 1     9600            6            8         1262         2   \n",
       "2                 1    11250            7            5          920         2   \n",
       "3                 0     9550            7            5          756         1   \n",
       "4                 1    14260            8            5         1145         2   \n",
       "5                 0    14115            5            5          796         1   \n",
       "6                 1    10084            8            5         1686         2   \n",
       "7                 1    10382            7            6         1107         2   \n",
       "8                 0     6120            7            5          952         2   \n",
       "9                 0     7420            5            6          991         1   \n",
       "\n",
       "   HalfBath  BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "0         1             3             8           0         548  \n",
       "1         0             3             6           1         460  \n",
       "2         1             3             6           1         608  \n",
       "3         0             3             7           1         642  \n",
       "4         1             4             9           1         836  \n",
       "5         1             1             5           0         480  \n",
       "6         0             3             7           1         636  \n",
       "7         1             3             7           2         484  \n",
       "8         0             2             8           2         468  \n",
       "9         0             2             5           2         205  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=0)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(df, group_num):\n",
    "    df = df.sample(frac=1, random_state=41)\n",
    "    col_num = len(df.columns)\n",
    "    row_num = len(df.index)\n",
    "    dfs = [None]*group_num\n",
    "    dfs_features = [None]*group_num\n",
    "    dfs_scaleds = [None]*group_num\n",
    "    dfs_new = [None]*group_num\n",
    "    scaler = StandardScaler()\n",
    "    for i in range(group_num):\n",
    "        dfs[i] = df.iloc[int(i/group_num*row_num):int((i+1)/group_num*row_num), :]\n",
    "        dfs_features[i] = dfs[i].iloc[:,1:]\n",
    "        dfs_features[i] = dfs_features[i].sample(frac=1, axis=1)\n",
    "        df_features_scaled_temp = pd.DataFrame(scaler.fit_transform(dfs_features[i]), columns = dfs_features[i].columns, index = dfs_features[i].index)\n",
    "        dfs_new[i] = pd.concat([dfs[i].iloc[:,0], df_features_scaled_temp], axis=1)\n",
    "        dfs_new[i]['group'] = i\n",
    "        cols_num = len(dfs_new[i].columns)\n",
    "        col_names = [j for j in range(cols_num)]\n",
    "        dfs_new[i].columns = col_names\n",
    "    df_final = pd.concat(dfs_new, axis=0)\n",
    "    last_col_num = cols_num - 1\n",
    "    df_new = df_final.rename(columns={last_col_num: 'group'})\n",
    "    df_final_onehot = pd.concat([df_new.iloc[:,:-1], pd.get_dummies(df_new.group, prefix='group')], axis=1)\n",
    "    df_final_onehot = df_final_onehot.sample(frac=1)\n",
    "    return df_final_onehot\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>group_0</th>\n",
       "      <th>group_1</th>\n",
       "      <th>group_2</th>\n",
       "      <th>group_3</th>\n",
       "      <th>group_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>1</td>\n",
       "      <td>0.234417</td>\n",
       "      <td>1.420138</td>\n",
       "      <td>2.639801</td>\n",
       "      <td>-1.115404</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>1.117915</td>\n",
       "      <td>0.795705</td>\n",
       "      <td>1.783758</td>\n",
       "      <td>0.501544</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.944198</td>\n",
       "      <td>0.441210</td>\n",
       "      <td>-0.986454</td>\n",
       "      <td>-0.285914</td>\n",
       "      <td>-2.133841</td>\n",
       "      <td>-0.402592</td>\n",
       "      <td>-2.300060</td>\n",
       "      <td>-0.795293</td>\n",
       "      <td>-0.837060</td>\n",
       "      <td>-1.037554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>-0.326688</td>\n",
       "      <td>1.234705</td>\n",
       "      <td>-1.413946</td>\n",
       "      <td>-1.164017</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>1.286677</td>\n",
       "      <td>-0.788723</td>\n",
       "      <td>0.619102</td>\n",
       "      <td>1.459614</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>0.272152</td>\n",
       "      <td>-0.569052</td>\n",
       "      <td>0.860584</td>\n",
       "      <td>1.336320</td>\n",
       "      <td>-0.463731</td>\n",
       "      <td>-0.282364</td>\n",
       "      <td>0.187433</td>\n",
       "      <td>-0.079555</td>\n",
       "      <td>0.881489</td>\n",
       "      <td>-0.995635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.783109</td>\n",
       "      <td>-0.878084</td>\n",
       "      <td>-0.943535</td>\n",
       "      <td>-0.898186</td>\n",
       "      <td>-0.344260</td>\n",
       "      <td>-0.934691</td>\n",
       "      <td>-1.048466</td>\n",
       "      <td>-0.735808</td>\n",
       "      <td>-0.635547</td>\n",
       "      <td>1.434085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1</td>\n",
       "      <td>0.897888</td>\n",
       "      <td>-0.569052</td>\n",
       "      <td>-0.245485</td>\n",
       "      <td>1.336320</td>\n",
       "      <td>-0.503726</td>\n",
       "      <td>0.532135</td>\n",
       "      <td>1.490537</td>\n",
       "      <td>0.624383</td>\n",
       "      <td>0.881489</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>1</td>\n",
       "      <td>1.276961</td>\n",
       "      <td>0.694926</td>\n",
       "      <td>0.869046</td>\n",
       "      <td>0.384309</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>2.004183</td>\n",
       "      <td>0.191016</td>\n",
       "      <td>0.673083</td>\n",
       "      <td>0.429269</td>\n",
       "      <td>-0.439529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1</td>\n",
       "      <td>0.554203</td>\n",
       "      <td>-0.441210</td>\n",
       "      <td>1.281614</td>\n",
       "      <td>0.857743</td>\n",
       "      <td>0.201045</td>\n",
       "      <td>-0.071308</td>\n",
       "      <td>0.688227</td>\n",
       "      <td>1.189541</td>\n",
       "      <td>0.245050</td>\n",
       "      <td>0.703629</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.643145</td>\n",
       "      <td>1.420138</td>\n",
       "      <td>1.109040</td>\n",
       "      <td>-1.115404</td>\n",
       "      <td>-0.344399</td>\n",
       "      <td>-0.101130</td>\n",
       "      <td>1.117915</td>\n",
       "      <td>-1.005426</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.501544</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>1</td>\n",
       "      <td>2.775094</td>\n",
       "      <td>-0.569052</td>\n",
       "      <td>1.059583</td>\n",
       "      <td>1.336320</td>\n",
       "      <td>2.190064</td>\n",
       "      <td>1.360426</td>\n",
       "      <td>1.490537</td>\n",
       "      <td>2.032258</td>\n",
       "      <td>2.760284</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6         7  \\\n",
       "654   1  0.234417  1.420138  2.639801 -1.115404  0.300245 -0.045599  1.117915   \n",
       "968   0 -0.944198  0.441210 -0.986454 -0.285914 -2.133841 -0.402592 -2.300060   \n",
       "1266  0  0.708595 -0.326688  1.234705 -1.413946 -1.164017  0.015883  1.286677   \n",
       "256   1  0.272152 -0.569052  0.860584  1.336320 -0.463731 -0.282364  0.187433   \n",
       "274   0 -0.783109 -0.878084 -0.943535 -0.898186 -0.344260 -0.934691 -1.048466   \n",
       "...  ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "1237  1  0.897888 -0.569052 -0.245485  1.336320 -0.503726  0.532135  1.490537   \n",
       "517   1  1.276961  0.694926  0.869046  0.384309 -0.040887  2.004183  0.191016   \n",
       "279   1  0.554203 -0.441210  1.281614  0.857743  0.201045 -0.071308  0.688227   \n",
       "764   1 -0.643145  1.420138  1.109040 -1.115404 -0.344399 -0.101130  1.117915   \n",
       "1046  1  2.775094 -0.569052  1.059583  1.336320  2.190064  1.360426  1.490537   \n",
       "\n",
       "             8         9        10  group_0  group_1  group_2  group_3  \\\n",
       "654   0.795705  1.783758  0.501544        1        0        0        0   \n",
       "968  -0.795293 -0.837060 -1.037554        0        0        0        1   \n",
       "1266 -0.788723  0.619102  1.459614        0        1        0        0   \n",
       "256  -0.079555  0.881489 -0.995635        0        0        1        0   \n",
       "274  -0.735808 -0.635547  1.434085        0        0        0        0   \n",
       "...        ...       ...       ...      ...      ...      ...      ...   \n",
       "1237  0.624383  0.881489  0.567405        0        0        1        0   \n",
       "517   0.673083  0.429269 -0.439529        0        0        0        0   \n",
       "279   1.189541  0.245050  0.703629        0        0        0        1   \n",
       "764  -1.005426  0.034488  0.501544        1        0        0        0   \n",
       "1046  2.032258  2.760284  0.567405        0        0        1        0   \n",
       "\n",
       "      group_4  \n",
       "654         0  \n",
       "968         0  \n",
       "1266        0  \n",
       "256         0  \n",
       "274         1  \n",
       "...       ...  \n",
       "1237        0  \n",
       "517         1  \n",
       "279         0  \n",
       "764         0  \n",
       "1046        0  \n",
       "\n",
       "[1460 rows x 16 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_onehot = transform_dataset(df, 5)\n",
    "df_final_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 16\n"
     ]
    }
   ],
   "source": [
    "col_num_final = len(df_final_onehot.columns)\n",
    "row_num_final = len(df_final_onehot.index)\n",
    "print(row_num_final, col_num_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df_final_onehot.iloc[:int(0.7*row_num_final), 1:(col_num_final)]\n",
    "trainy = df_final_onehot.iloc[:int(0.7*row_num_final), 0]\n",
    "\n",
    "valx = df_final_onehot.iloc[int(0.7*row_num_final):int(0.85*row_num_final), 1:(col_num_final)]\n",
    "valy = df_final_onehot.iloc[int(0.7*row_num_final):int(0.85*row_num_final), 0]\n",
    "\n",
    "testx = df_final_onehot.iloc[int(0.85*row_num_final):, 1:(col_num_final)]\n",
    "testy = df_final_onehot.iloc[int(0.85*row_num_final):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726     1\n",
       "79      0\n",
       "330     0\n",
       "331     0\n",
       "1140    0\n",
       "       ..\n",
       "1237    1\n",
       "517     1\n",
       "279     1\n",
       "764     1\n",
       "1046    1\n",
       "Name: 0, Length: 219, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_rand_list(max_val, count):\n",
    "    randomlist = random.sample(range(0, max_val + 1), count)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_losses = {12:[], 14:[], 16:[], 18:[], 20:[], 22:[], 24:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7007 - accuracy: 0.7640 - val_loss: 0.4479 - val_accuracy: 0.8904\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.8619 - val_loss: 0.3979 - val_accuracy: 0.8904\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8786 - val_loss: 0.3515 - val_accuracy: 0.8904\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8727 - val_loss: 0.3343 - val_accuracy: 0.9132\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8678 - val_loss: 0.3026 - val_accuracy: 0.9178\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8776 - val_loss: 0.3046 - val_accuracy: 0.9132\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8815 - val_loss: 0.2896 - val_accuracy: 0.8995\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8923 - val_loss: 0.2734 - val_accuracy: 0.9087\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8923 - val_loss: 0.2683 - val_accuracy: 0.9041\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8991 - val_loss: 0.2971 - val_accuracy: 0.9132\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8815 - val_loss: 0.3332 - val_accuracy: 0.9315\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8883 - val_loss: 0.3066 - val_accuracy: 0.9132\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8942 - val_loss: 0.2739 - val_accuracy: 0.8904\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.9021 - val_loss: 0.2855 - val_accuracy: 0.9132\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.9021 - val_loss: 0.2580 - val_accuracy: 0.9224\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8805 - val_loss: 0.2938 - val_accuracy: 0.9087\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3074 - accuracy: 0.9119 - val_loss: 0.2840 - val_accuracy: 0.9087\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.9089 - val_loss: 0.3209 - val_accuracy: 0.8950\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3183 - accuracy: 0.8991 - val_loss: 0.2690 - val_accuracy: 0.9361\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.9030 - val_loss: 0.3363 - val_accuracy: 0.8904\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3120 - accuracy: 0.9050 - val_loss: 0.2719 - val_accuracy: 0.9224\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.9089 - val_loss: 0.3163 - val_accuracy: 0.9041\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.9070 - val_loss: 0.2857 - val_accuracy: 0.9132\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3023 - accuracy: 0.9050 - val_loss: 0.2791 - val_accuracy: 0.9132\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.9070 - val_loss: 0.2838 - val_accuracy: 0.8813\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8972 - val_loss: 0.2791 - val_accuracy: 0.9269\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.9070 - val_loss: 0.2827 - val_accuracy: 0.9224\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.9089 - val_loss: 0.2945 - val_accuracy: 0.9132\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.9148 - val_loss: 0.2541 - val_accuracy: 0.9361\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.9187 - val_loss: 0.3669 - val_accuracy: 0.8858\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.9148 - val_loss: 0.2778 - val_accuracy: 0.9132\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2922 - accuracy: 0.9216 - val_loss: 0.3162 - val_accuracy: 0.8950\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.9119 - val_loss: 0.2627 - val_accuracy: 0.9315\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.9060 - val_loss: 0.2948 - val_accuracy: 0.9178\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.9177 - val_loss: 0.3187 - val_accuracy: 0.8995\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.9070 - val_loss: 0.2616 - val_accuracy: 0.9132\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.9148 - val_loss: 0.2587 - val_accuracy: 0.9315\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.9030 - val_loss: 0.3029 - val_accuracy: 0.8995\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.9128 - val_loss: 0.2868 - val_accuracy: 0.8950\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.9158 - val_loss: 0.3102 - val_accuracy: 0.8904\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.9167 - val_loss: 0.3051 - val_accuracy: 0.9041\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.9138 - val_loss: 0.2745 - val_accuracy: 0.9315\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.9119 - val_loss: 0.2860 - val_accuracy: 0.9132\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.9148 - val_loss: 0.3050 - val_accuracy: 0.9132\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9197 - val_loss: 0.2770 - val_accuracy: 0.9224\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.9265 - val_loss: 0.3166 - val_accuracy: 0.9132\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.9148 - val_loss: 0.3667 - val_accuracy: 0.8676\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.9285 - val_loss: 0.3038 - val_accuracy: 0.9087\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2961 - accuracy: 0.9236 - val_loss: 0.2778 - val_accuracy: 0.9269\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.9295 - val_loss: 0.3142 - val_accuracy: 0.8995\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.9187 - val_loss: 0.3071 - val_accuracy: 0.9087\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.9207 - val_loss: 0.3134 - val_accuracy: 0.9132\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.9167 - val_loss: 0.3276 - val_accuracy: 0.9041\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.9236 - val_loss: 0.3099 - val_accuracy: 0.9178\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3187 - accuracy: 0.9099 - val_loss: 0.2942 - val_accuracy: 0.9132\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.9128 - val_loss: 0.3715 - val_accuracy: 0.8721\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.9119 - val_loss: 0.3031 - val_accuracy: 0.9132\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.9148 - val_loss: 0.3132 - val_accuracy: 0.9132\n",
      "Epoch 59/300\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2696 - accuracy: 0.9688\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.9197 - val_loss: 0.3309 - val_accuracy: 0.8904\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.9305 - val_loss: 0.3299 - val_accuracy: 0.9087\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9314 - val_loss: 0.3170 - val_accuracy: 0.9087\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.9177 - val_loss: 0.3235 - val_accuracy: 0.9087\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.9265 - val_loss: 0.3159 - val_accuracy: 0.8950\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.9256 - val_loss: 0.2946 - val_accuracy: 0.9041\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9275 - val_loss: 0.2859 - val_accuracy: 0.9041\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.9187 - val_loss: 0.3369 - val_accuracy: 0.8904\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.9373 - val_loss: 0.2979 - val_accuracy: 0.9087\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.9207 - val_loss: 0.3149 - val_accuracy: 0.8995\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.9256 - val_loss: 0.2987 - val_accuracy: 0.9087\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.9089 - val_loss: 0.2796 - val_accuracy: 0.9178\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9197 - val_loss: 0.3518 - val_accuracy: 0.9041\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.9236 - val_loss: 0.3406 - val_accuracy: 0.8904\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.9236 - val_loss: 0.2936 - val_accuracy: 0.9224\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.9275 - val_loss: 0.3495 - val_accuracy: 0.8995\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.9128 - val_loss: 0.3558 - val_accuracy: 0.8493\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.9158 - val_loss: 0.3681 - val_accuracy: 0.8904\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9285 - val_loss: 0.3094 - val_accuracy: 0.9224\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.9256 - val_loss: 0.3371 - val_accuracy: 0.9269\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.9403 - val_loss: 0.3907 - val_accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9360730648040771"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "checkpoint_filepath = 'tmp/checkpoint'\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-6),\n",
    "        keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)]\n",
    "\n",
    "\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx], y = trainy.values,  \n",
    "                         validation_data = ([testx], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy\n",
    "#min_losses3[c].append(best_val_loss)\n",
    "#i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 662us/step - loss: 0.6494 - accuracy: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6494320034980774, 0.8772727251052856]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MICS_model.evaluate(x = [valx], y = valy.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model.load_weights(checkpoint_filepath))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
