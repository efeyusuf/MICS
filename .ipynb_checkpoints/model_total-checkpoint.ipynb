{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 3):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self, index_col=0):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.8*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.8*row_num), 0]\n",
    "        testx_df = self.raw_data.iloc[int(0.8*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.8*row_num):, 0]        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(2)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_scaled.iloc[:,group]\n",
    "            features_and_labels[0][index] = train_temp.values\n",
    "            test_temp = textx_scaled.iloc[:,group]\n",
    "            features_and_labels[1][index] = test_temp.values            \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = testy_df.values   \n",
    "        return features_and_labels\n",
    "    \n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32, activation=\"relu\"):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=activation)(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "        \n",
    "    def default_exp_house(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes, activation=\"sigmoid\")\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        \n",
    "        training_acc = history.history[\"val_accuracy\"]\n",
    "        best_row_index_acc = np.argmax(training_acc)\n",
    "        best_val_acc = training_acc[best_row_index_acc]        \n",
    "        print(\"best val loss is: \" + str(best_val_loss))\n",
    "        print(\"best val accuracy is: \" + str(best_val_acc))\n",
    "        return best_val_acc\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 12611.9980 - val_loss: 8701.6113\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10123.4941 - val_loss: 8186.8301\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9637.7422 - val_loss: 7897.0996\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9496.2939 - val_loss: 8213.1562\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9385.5146 - val_loss: 8130.5259\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9067.1992 - val_loss: 7663.7197\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9007.0693 - val_loss: 7763.4619\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8859.5352 - val_loss: 7582.0825\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8911.4629 - val_loss: 7471.4004\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8857.0566 - val_loss: 7763.6646\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8769.6973 - val_loss: 7450.3481\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8854.7793 - val_loss: 7871.7690\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8819.0615 - val_loss: 7925.4194\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8629.9590 - val_loss: 7518.0498\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8647.8408 - val_loss: 7339.6426\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8660.6729 - val_loss: 7530.6670\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8478.0010 - val_loss: 7290.0234\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8505.4072 - val_loss: 7546.9272\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8571.7090 - val_loss: 7332.9092\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8570.3311 - val_loss: 7202.0591\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8562.5273 - val_loss: 7481.6216\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8470.6895 - val_loss: 7708.3208\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8382.8643 - val_loss: 7107.9795\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8463.2588 - val_loss: 7366.2383\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8551.0508 - val_loss: 7164.9678\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8406.8584 - val_loss: 7518.6118\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8275.1201 - val_loss: 7568.3491\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8276.9648 - val_loss: 7546.5190\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8229.2236 - val_loss: 7343.3921\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8368.0273 - val_loss: 7361.0698\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8320.6006 - val_loss: 7321.3662\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8247.0684 - val_loss: 7037.0547\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8272.1152 - val_loss: 7209.1431\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8169.9858 - val_loss: 7057.9849\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8140.5615 - val_loss: 7129.2397\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8155.0630 - val_loss: 7258.0454\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8092.8862 - val_loss: 7303.0386\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8076.2837 - val_loss: 7126.6978\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8148.0396 - val_loss: 7208.5186\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8095.9619 - val_loss: 7393.0762\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8083.7788 - val_loss: 7736.9097\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8081.2593 - val_loss: 7797.8486\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8003.1621 - val_loss: 7053.3486\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7990.7334 - val_loss: 7150.5986\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7893.1567 - val_loss: 7978.7075\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8022.9580 - val_loss: 7239.6743\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7899.4600 - val_loss: 7474.3320\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7976.0381 - val_loss: 7464.2568\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7955.3799 - val_loss: 6984.5269\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8078.0327 - val_loss: 7292.6309\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7962.9619 - val_loss: 7216.1841\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7873.6392 - val_loss: 7709.1709\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7942.3828 - val_loss: 7314.6587\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7818.1587 - val_loss: 7149.8467\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7730.5386 - val_loss: 7434.7764\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7979.5059 - val_loss: 7530.6929\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7938.8237 - val_loss: 7234.0923\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7861.2632 - val_loss: 7670.7588\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7805.0444 - val_loss: 8034.1069\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7922.7065 - val_loss: 7585.4937\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7873.5386 - val_loss: 6929.0112\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7839.8208 - val_loss: 7057.3926\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7829.4302 - val_loss: 7107.2559\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7760.1191 - val_loss: 8310.3027\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7892.8975 - val_loss: 7384.0635\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7771.5381 - val_loss: 7152.5049\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7924.0249 - val_loss: 7021.2129\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7807.1865 - val_loss: 7294.4932\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7666.6953 - val_loss: 7311.3789\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7805.0371 - val_loss: 7167.6060\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7630.3086 - val_loss: 7121.5034\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7663.6338 - val_loss: 7170.7231\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7594.0459 - val_loss: 7337.0083\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7616.0410 - val_loss: 7567.3350\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7498.0977 - val_loss: 6864.9053\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7656.6851 - val_loss: 7664.0938\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7631.6514 - val_loss: 7525.3931\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7613.1416 - val_loss: 7258.9756\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7631.6328 - val_loss: 7257.5708\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7619.8149 - val_loss: 7275.2627\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7557.3877 - val_loss: 7186.7109\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7550.7188 - val_loss: 7786.0977\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7723.4136 - val_loss: 7580.7979\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7514.4224 - val_loss: 7124.1895\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7644.8501 - val_loss: 7280.6431\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7585.7173 - val_loss: 7326.1548\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7670.1250 - val_loss: 7366.6206\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7471.1533 - val_loss: 7282.5981\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7437.4331 - val_loss: 7141.9297\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7685.4136 - val_loss: 7299.8716\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7490.5293 - val_loss: 7268.0059\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7512.4048 - val_loss: 7183.0630\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7479.6128 - val_loss: 7634.8896\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7567.1069 - val_loss: 7495.9458\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7547.3022 - val_loss: 7198.9404\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7581.4800 - val_loss: 7336.8867\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7336.3940 - val_loss: 7329.1948\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7473.0957 - val_loss: 7575.8203\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7418.1279 - val_loss: 7065.7861\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7491.0474 - val_loss: 7928.3853\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7519.0771 - val_loss: 6816.4087\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7564.2451 - val_loss: 7311.6021\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7456.9009 - val_loss: 7140.6465\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7349.1675 - val_loss: 7500.1255\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7484.9619 - val_loss: 7026.7734\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7531.7788 - val_loss: 7128.1631\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7316.8511 - val_loss: 7288.7295\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7503.8828 - val_loss: 6753.0103\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7423.5322 - val_loss: 7161.0723\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7475.8140 - val_loss: 7880.7358\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7403.6509 - val_loss: 7916.3276\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7553.7314 - val_loss: 6979.7646\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7390.0757 - val_loss: 7147.8428\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7316.7651 - val_loss: 7483.1367\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7538.5674 - val_loss: 7138.7944\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7433.4980 - val_loss: 7876.4976\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7409.8271 - val_loss: 7411.0737\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7263.1924 - val_loss: 8001.4233\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7318.4443 - val_loss: 7757.0425\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7408.1011 - val_loss: 7333.5347\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7261.2671 - val_loss: 7368.2402\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7483.2593 - val_loss: 8138.8931\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7525.7104 - val_loss: 7165.6201\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7335.4082 - val_loss: 7020.9966\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7174.6606 - val_loss: 7349.3105\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7395.6514 - val_loss: 7582.8174\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7492.6558 - val_loss: 7271.3569\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7149.6929 - val_loss: 7286.7837\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7219.3159 - val_loss: 7501.4556\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7292.0435 - val_loss: 7010.5479\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7231.9956 - val_loss: 7190.8125\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7243.7124 - val_loss: 7501.3506\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7230.0449 - val_loss: 7187.6953\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7317.3447 - val_loss: 7329.8613\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7476.5415 - val_loss: 7471.7334\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7293.3564 - val_loss: 7511.5356\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7185.5366 - val_loss: 7930.2876\n",
      "Epoch 138/300\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 7173.2559\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7298.1738 - val_loss: 7262.7485\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7199.5806 - val_loss: 7405.7046\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7110.5200 - val_loss: 7339.4971\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7056.6880 - val_loss: 7077.8569\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7296.5532 - val_loss: 7304.4136\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7045.4067 - val_loss: 7315.6895\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7226.3228 - val_loss: 7413.0742\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7226.2388 - val_loss: 7302.5146\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7158.3540 - val_loss: 7180.7930\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7274.9609 - val_loss: 7657.4604\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7077.2974 - val_loss: 7754.9634\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6990.8247 - val_loss: 7175.6147\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7013.6880 - val_loss: 7199.1792\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7142.5278 - val_loss: 7173.9795\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6960.7861 - val_loss: 7049.1982\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7138.9995 - val_loss: 6953.9150\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7135.5996 - val_loss: 6992.0923\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7292.4106 - val_loss: 7488.2993\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7099.1646 - val_loss: 6936.0840\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7078.9980 - val_loss: 7406.4082\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7152.6875 - val_loss: 7530.9849\n",
      "6753.01025390625\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=False, group_number=3)\n",
    "deneyelim.get_raw_data()\n",
    "deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, sampling_method=\"Vanilla\", group_number=5)\n",
    "    deneyelim2.get_raw_data()\n",
    "    acc = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.7697 - val_loss: 0.3884 - val_accuracy: 0.8938\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8733 - val_loss: 0.3383 - val_accuracy: 0.9007\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8887 - val_loss: 0.3667 - val_accuracy: 0.8973\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8801 - val_loss: 0.3067 - val_accuracy: 0.8938\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8896 - val_loss: 0.2846 - val_accuracy: 0.8904\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8904 - val_loss: 0.2721 - val_accuracy: 0.8973\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9050 - val_loss: 0.2583 - val_accuracy: 0.9041\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.9067 - val_loss: 0.2701 - val_accuracy: 0.8973\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.9058 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8998 - val_loss: 0.2983 - val_accuracy: 0.8973\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.9084 - val_loss: 0.2828 - val_accuracy: 0.8904\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9135 - val_loss: 0.3033 - val_accuracy: 0.8973\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.9075 - val_loss: 0.2979 - val_accuracy: 0.9007\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9067 - val_loss: 0.2783 - val_accuracy: 0.8870\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.9084 - val_loss: 0.2775 - val_accuracy: 0.9041\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8998 - val_loss: 0.2869 - val_accuracy: 0.9007\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9092 - val_loss: 0.2971 - val_accuracy: 0.9041\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9101 - val_loss: 0.2758 - val_accuracy: 0.8938\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.9075 - val_loss: 0.2806 - val_accuracy: 0.8870\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.9135 - val_loss: 0.2951 - val_accuracy: 0.8973\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9110 - val_loss: 0.2770 - val_accuracy: 0.8904\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9195 - val_loss: 0.3252 - val_accuracy: 0.8973\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9118 - val_loss: 0.2975 - val_accuracy: 0.8938\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.9041 - val_loss: 0.2956 - val_accuracy: 0.8904\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9135 - val_loss: 0.3207 - val_accuracy: 0.8904\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.9067 - val_loss: 0.2685 - val_accuracy: 0.9041\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9075 - val_loss: 0.2909 - val_accuracy: 0.8904\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9110 - val_loss: 0.2804 - val_accuracy: 0.8904\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.9092 - val_loss: 0.2729 - val_accuracy: 0.8973\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.9092 - val_loss: 0.2711 - val_accuracy: 0.8973\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9110 - val_loss: 0.2739 - val_accuracy: 0.8870\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.9187 - val_loss: 0.2804 - val_accuracy: 0.8870\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.9075 - val_loss: 0.2897 - val_accuracy: 0.8973\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.9118 - val_loss: 0.2806 - val_accuracy: 0.8904\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9187 - val_loss: 0.2916 - val_accuracy: 0.8904\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.9067 - val_loss: 0.2758 - val_accuracy: 0.8801\n",
      "Epoch 37/300\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9175\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9161 - val_loss: 0.3178 - val_accuracy: 0.8870\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9127 - val_loss: 0.2724 - val_accuracy: 0.8836\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9178 - val_loss: 0.2661 - val_accuracy: 0.9007\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9161 - val_loss: 0.2945 - val_accuracy: 0.8836\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.9187 - val_loss: 0.2762 - val_accuracy: 0.8836\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9221 - val_loss: 0.2755 - val_accuracy: 0.8836\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9264 - val_loss: 0.2771 - val_accuracy: 0.8836\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9229 - val_loss: 0.2602 - val_accuracy: 0.8836\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9272 - val_loss: 0.2816 - val_accuracy: 0.9007\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9170 - val_loss: 0.2646 - val_accuracy: 0.8904\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9238 - val_loss: 0.2738 - val_accuracy: 0.8904\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9315 - val_loss: 0.2778 - val_accuracy: 0.8973\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.9255 - val_loss: 0.2794 - val_accuracy: 0.8836\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9264 - val_loss: 0.3046 - val_accuracy: 0.8904\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9212 - val_loss: 0.2668 - val_accuracy: 0.8904\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9195 - val_loss: 0.2519 - val_accuracy: 0.8973\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9144 - val_loss: 0.3042 - val_accuracy: 0.8973\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.9101 - val_loss: 0.2684 - val_accuracy: 0.9007\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.9170 - val_loss: 0.2753 - val_accuracy: 0.8973\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9212 - val_loss: 0.2845 - val_accuracy: 0.8836\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9238 - val_loss: 0.2805 - val_accuracy: 0.8973\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.9272 - val_loss: 0.2706 - val_accuracy: 0.8938\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9255 - val_loss: 0.2906 - val_accuracy: 0.8904\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9247 - val_loss: 0.3026 - val_accuracy: 0.8973\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9324 - val_loss: 0.2930 - val_accuracy: 0.8973\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9238 - val_loss: 0.2976 - val_accuracy: 0.8870\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9229 - val_loss: 0.2564 - val_accuracy: 0.8870\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9272 - val_loss: 0.2794 - val_accuracy: 0.9007\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9264 - val_loss: 0.2583 - val_accuracy: 0.9110\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9255 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9289 - val_loss: 0.2593 - val_accuracy: 0.9007\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9272 - val_loss: 0.2805 - val_accuracy: 0.9041\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9281 - val_loss: 0.2737 - val_accuracy: 0.8938\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9247 - val_loss: 0.2855 - val_accuracy: 0.9007\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9264 - val_loss: 0.2709 - val_accuracy: 0.9110\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9298 - val_loss: 0.3050 - val_accuracy: 0.9007\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9135 - val_loss: 0.2627 - val_accuracy: 0.8973\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9281 - val_loss: 0.2856 - val_accuracy: 0.9075\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9247 - val_loss: 0.3255 - val_accuracy: 0.8938\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9058 - val_loss: 0.2671 - val_accuracy: 0.9041\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9238 - val_loss: 0.3048 - val_accuracy: 0.8973\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9281 - val_loss: 0.2967 - val_accuracy: 0.8973\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9178 - val_loss: 0.2964 - val_accuracy: 0.8973\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9315 - val_loss: 0.3316 - val_accuracy: 0.8973\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9212 - val_loss: 0.2860 - val_accuracy: 0.8973\n",
      "Epoch 82/300\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2433 - accuracy: 0.9304\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9298 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9298 - val_loss: 0.3124 - val_accuracy: 0.8938\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9238 - val_loss: 0.3067 - val_accuracy: 0.8904\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9324 - val_loss: 0.3308 - val_accuracy: 0.8938\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9332 - val_loss: 0.3049 - val_accuracy: 0.8904\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9358 - val_loss: 0.3226 - val_accuracy: 0.8870\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9324 - val_loss: 0.2741 - val_accuracy: 0.8904\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9298 - val_loss: 0.3048 - val_accuracy: 0.8870\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9238 - val_loss: 0.3301 - val_accuracy: 0.9007\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9221 - val_loss: 0.3131 - val_accuracy: 0.9007\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9247 - val_loss: 0.2832 - val_accuracy: 0.8870\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9238 - val_loss: 0.3121 - val_accuracy: 0.8904\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9212 - val_loss: 0.2892 - val_accuracy: 0.8870\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9281 - val_loss: 0.2692 - val_accuracy: 0.8938\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9195 - val_loss: 0.2798 - val_accuracy: 0.8870\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9255 - val_loss: 0.2958 - val_accuracy: 0.8938\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9375 - val_loss: 0.3294 - val_accuracy: 0.9007\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9229 - val_loss: 0.3181 - val_accuracy: 0.8938\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9264 - val_loss: 0.2994 - val_accuracy: 0.8904\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9366 - val_loss: 0.3187 - val_accuracy: 0.8904\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9307 - val_loss: 0.3049 - val_accuracy: 0.8870\n",
      "best val loss is: 0.2519095838069916\n",
      "best val accuracy is: 0.9109588861465454\n"
     ]
    }
   ],
   "source": [
    "deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=3)\n",
    "deneyelim2.get_raw_data()\n",
    "a = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  0.769623 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:10:00  0.762633 -0.761611 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:20:00  0.729854 -0.810232 -1.924584  0.786817 -1.385479   \n",
       "  2016-01-11 17:30:00  0.739495 -0.834542 -1.983431  0.786817 -1.385479   \n",
       "  2016-01-11 17:40:00  0.792640 -0.834542 -1.983431  0.786817 -1.359694   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.294374 -0.569336 -0.106203  1.096424  0.110056   \n",
       "  2016-04-30 07:40:00 -0.287143 -0.609117 -0.106203  1.070624  0.058486   \n",
       "  2016-04-30 07:50:00 -0.281118 -0.569336 -0.159166  1.044823  0.110056   \n",
       "  2016-04-30 08:00:00 -0.300159 -0.569336 -0.159166  1.019022  0.161626   \n",
       "  2016-04-30 08:10:00 -0.329323 -0.569336 -0.159166  0.928720  0.257031   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-01-11 17:00:00   1.065608 -1.217218  1.554962  \n",
       "  2016-01-11 17:10:00   0.935169 -1.193835  1.654543  \n",
       "  2016-01-11 17:20:00   0.804731 -1.240601  1.630574  \n",
       "  2016-01-11 17:30:00   0.674293 -1.240601  1.591599  \n",
       "  2016-01-11 17:40:00   0.543854 -1.240601  1.546388  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-04-30 07:30:00  -1.282284  0.302667 -0.310373  \n",
       "  2016-04-30 07:40:00  -1.282284  0.302667 -0.279193  \n",
       "  2016-04-30 07:50:00  -1.282284  0.302667 -0.081980  \n",
       "  2016-04-30 08:00:00  -1.282284  0.302667 -0.050800  \n",
       "  2016-04-30 08:10:00  -1.282284  0.302667 -0.036769  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-01-11 17:00:00 -1.039517   0.729188  3.071295  0.180377  1.014203   \n",
       "  2016-01-11 17:10:00 -1.039517   0.700803  3.071295  0.152122  1.021582   \n",
       "  2016-01-11 17:20:00 -1.039517   0.672418  3.071295  0.123867  1.006824   \n",
       "  2016-01-11 17:30:00 -1.039517   0.644033  4.267005  0.095612  0.982227   \n",
       "  2016-01-11 17:40:00 -1.039517   0.615647  4.267005  0.067357  0.982227   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00  0.113503   0.487914 -0.515833 -0.182903  0.029511   \n",
       "  2016-04-30 07:40:00  0.168409   0.502106 -0.515833 -0.158684  0.014753   \n",
       "  2016-04-30 07:50:00  0.195862   0.516299 -0.515833 -0.134466 -0.001645   \n",
       "  2016-04-30 08:00:00  0.195862   0.530492 -0.515833 -0.110247 -0.059858   \n",
       "  2016-04-30 08:10:00  0.195862   0.521030 -0.515833 -0.077955 -0.092653   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-01-11 17:00:00  1.681976  2.103814 -1.200652  \n",
       "  2016-01-11 17:10:00  1.700697  1.852407 -1.200652  \n",
       "  2016-01-11 17:20:00  1.745419  1.742937 -1.200652  \n",
       "  2016-01-11 17:30:00  1.766219  1.677998 -1.200652  \n",
       "  2016-01-11 17:40:00  1.766219  1.752214 -1.200652  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-04-30 07:30:00 -0.667471 -0.771142  0.801833  \n",
       "  2016-04-30 07:40:00 -0.752754 -0.661673  0.761783  \n",
       "  2016-04-30 07:50:00 -0.906680 -0.580962  0.675009  \n",
       "  2016-04-30 08:00:00 -0.867158 -0.567047  0.675009  \n",
       "  2016-04-30 08:10:00 -0.885879 -0.457578  0.675009  \n",
       "  \n",
       "  [15788 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-01-11 17:00:00  1.311407 -1.494334  0.434884  1.246472  1.160518   \n",
       "  2016-01-11 17:10:00  1.298168 -1.494334  0.434884  1.226599  1.153506   \n",
       "  2016-01-11 17:20:00  1.273015 -1.494334  0.422695  1.198385  1.128006   \n",
       "  2016-01-11 17:30:00  1.244551 -1.536665  0.422695  1.187590  1.101232   \n",
       "  2016-01-11 17:40:00  1.232637 -1.494334  0.422695  1.169926  1.101232   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-04-30 07:30:00 -0.284520 -0.287906 -0.894794 -0.045972  0.350278   \n",
       "  2016-04-30 07:40:00 -0.277239 -0.287906 -0.902920  0.009966  0.336253   \n",
       "  2016-04-30 07:50:00 -0.250761 -0.287906 -0.912246 -0.007699  0.305654   \n",
       "  2016-04-30 08:00:00 -0.237523 -0.287906 -0.961647 -0.078356  0.267405   \n",
       "  2016-04-30 08:10:00 -0.222960 -0.287906 -0.995628 -0.078356  0.210031   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-01-11 17:00:00    1.961544  0.188895 -0.284588    -2.815843  \n",
       "  2016-01-11 17:10:00    1.652462  0.147545 -0.284588    -2.802987  \n",
       "  2016-01-11 17:20:00    1.343380  0.089083 -0.284588    -2.790130  \n",
       "  2016-01-11 17:30:00    1.034298  0.061991 -0.284588    -2.777274  \n",
       "  2016-01-11 17:40:00    0.725216  0.047732 -0.284588    -2.764417  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-04-30 07:30:00    0.107051  0.131860 -0.554938     0.314740  \n",
       "  2016-04-30 07:40:00    0.107051  0.188895 -0.488999     0.325453  \n",
       "  2016-04-30 07:50:00    0.107051  0.311522 -0.440644     0.336167  \n",
       "  2016-04-30 08:00:00    0.107051  0.382817 -0.416466     0.346881  \n",
       "  2016-04-30 08:10:00    0.107051  0.445556 -0.416466     0.355452  \n",
       "  \n",
       "  [15788 rows x 9 columns],\n",
       "  date\n",
       "  2016-01-11 17:00:00     60\n",
       "  2016-01-11 17:10:00     60\n",
       "  2016-01-11 17:20:00     50\n",
       "  2016-01-11 17:30:00     50\n",
       "  2016-01-11 17:40:00     60\n",
       "                        ... \n",
       "  2016-04-30 07:30:00     80\n",
       "  2016-04-30 07:40:00     80\n",
       "  2016-04-30 07:50:00     50\n",
       "  2016-04-30 08:00:00     70\n",
       "  2016-04-30 08:10:00    300\n",
       "  Name: Appliances, Length: 15788, dtype: int64],\n",
       " [                         RH_6        T4        T8    RH_out        T5  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.365476 -0.569336 -0.159166  0.838418  0.210618   \n",
       "  2016-04-30 08:30:00 -0.430311 -0.525135 -0.159166  0.748116  0.187411   \n",
       "  2016-04-30 08:40:00 -0.534192 -0.503035 -0.198397  0.657814  0.187411   \n",
       "  2016-04-30 08:50:00 -0.740207 -0.454414 -0.159166  0.567512  0.187411   \n",
       "  2016-04-30 09:00:00 -0.789677 -0.430103 -0.159166  0.477210  0.187411   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00 -2.240395  3.017574  1.900489 -2.025449  3.281621   \n",
       "  2016-05-27 17:30:00 -2.240395  3.017574  1.900489 -1.999648  3.304827   \n",
       "  2016-05-27 17:40:00 -2.240395  3.017574  1.900489 -1.973847  3.304827   \n",
       "  2016-05-27 17:50:00 -2.240395  3.017574  1.878421 -1.948047  3.281621   \n",
       "  2016-05-27 18:00:00 -2.240395  3.017574  1.921674 -1.922246  3.281621   \n",
       "  \n",
       "                       Windspeed        T9      RH_4  \n",
       "  date                                                \n",
       "  2016-04-30 08:20:00  -1.282284  0.302667 -0.043785  \n",
       "  2016-04-30 08:30:00  -1.282284  0.302667  0.017016  \n",
       "  2016-04-30 08:40:00  -1.282284  0.302667  0.095745  \n",
       "  2016-04-30 08:50:00  -1.282284  0.302667  0.128484  \n",
       "  2016-04-30 09:00:00  -1.282284  0.365800  0.136279  \n",
       "  ...                        ...       ...       ...  \n",
       "  2016-05-27 17:20:00  -0.369215  3.108607  1.560419  \n",
       "  2016-05-27 17:30:00  -0.303996  3.108607  1.560419  \n",
       "  2016-05-27 17:40:00  -0.238776  3.108607  1.593158  \n",
       "  2016-05-27 17:50:00  -0.173557  3.108607  1.607189  \n",
       "  2016-05-27 18:00:00  -0.108338  3.108607  1.647723  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                             T1  Tdewpoint    lights     T_out      RH_9  \\\n",
       "  date                                                                     \n",
       "  2016-04-30 08:20:00  0.195862   0.511568 -0.515833 -0.045664 -0.159065   \n",
       "  2016-04-30 08:30:00  0.195862   0.502106 -0.515833 -0.013372 -0.206618   \n",
       "  2016-04-30 08:40:00  0.195862   0.492645 -0.515833  0.018919 -0.256632   \n",
       "  2016-04-30 08:50:00  0.195862   0.483183  0.679876  0.051211 -0.274670   \n",
       "  2016-04-30 09:00:00  0.140956   0.473721 -0.515833  0.083502 -0.247613   \n",
       "  ...                       ...        ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  3.635705   3.009470 -0.515833  4.087654  1.324122   \n",
       "  2016-05-27 17:30:00  3.580799   3.000008 -0.515833  4.055362  1.324122   \n",
       "  2016-05-27 17:40:00  3.580799   2.990547  0.679876  4.023071  1.324122   \n",
       "  2016-05-27 17:50:00  3.580799   2.981085  0.679876  3.990779  1.330886   \n",
       "  2016-05-27 18:00:00  3.580799   2.971623  0.679876  3.958488  1.337651   \n",
       "  \n",
       "                           RH_3      RH_1        T3  \n",
       "  date                                               \n",
       "  2016-04-30 08:20:00 -0.885879 -0.494686  0.675009  \n",
       "  2016-04-30 08:30:00 -0.871838 -0.531794  0.675009  \n",
       "  2016-04-30 08:40:00 -0.857798 -0.541999  0.697259  \n",
       "  2016-04-30 08:50:00 -0.857798 -0.558697  0.741758  \n",
       "  2016-04-30 09:00:00 -0.857798 -0.594878  0.741758  \n",
       "  ...                       ...       ...       ...  \n",
       "  2016-05-27 17:20:00  0.569135  1.815298  3.745485  \n",
       "  2016-05-27 17:30:00  0.587856  1.798600  3.700986  \n",
       "  2016-05-27 17:40:00  0.733461  1.825503  3.645361  \n",
       "  2016-05-27 17:50:00  0.608657  1.934972  3.538562  \n",
       "  2016-05-27 18:00:00  0.567055  1.826431  3.494062  \n",
       "  \n",
       "  [3947 rows x 8 columns],\n",
       "                           RH_7        T7      RH_5      RH_2      RH_8  \\\n",
       "  date                                                                    \n",
       "  2016-04-30 08:20:00 -0.222960 -0.287906 -0.995628 -0.056767  0.171782   \n",
       "  2016-04-30 08:30:00 -0.222960 -0.287906 -0.958323 -0.096021  0.139271   \n",
       "  2016-04-30 08:40:00 -0.237523 -0.287906 -0.943549 -0.155883  0.127796   \n",
       "  2016-04-30 08:50:00 -0.244804 -0.287906 -0.928036 -0.232429  0.127796   \n",
       "  2016-04-30 09:00:00 -0.244804 -0.287906 -0.936162 -0.370800  0.139271   \n",
       "  ...                       ...       ...       ...       ...       ...   \n",
       "  2016-05-27 17:20:00  1.881996  3.140891  0.124626  0.432649  1.385039   \n",
       "  2016-05-27 17:30:00  1.864974  3.177175  0.116500  0.448631  1.330726   \n",
       "  2016-05-27 17:40:00  1.862138  3.166290  0.109851  0.651350  1.305864   \n",
       "  2016-05-27 17:50:00  1.841428  3.140891  0.102464  0.730083  1.278851   \n",
       "  2016-05-27 18:00:00  1.793429  3.140891  0.102464  0.711073  1.320398   \n",
       "  \n",
       "                       Visibility        T6        T2  Press_mm_hg  \n",
       "  date                                                              \n",
       "  2016-04-30 08:20:00    0.107051  0.489759 -0.328547     0.364023  \n",
       "  2016-04-30 08:30:00    0.107051  0.595988 -0.284588     0.372594  \n",
       "  2016-04-30 08:40:00    0.107051  0.667283 -0.242827     0.381165  \n",
       "  2016-04-30 08:50:00    0.107051  0.749985 -0.066989     0.389736  \n",
       "  2016-04-30 09:00:00    0.107051  0.959591  0.064889     0.398307  \n",
       "  ...                         ...       ...       ...          ...  \n",
       "  2016-05-27 17:20:00   -1.209907  3.989616  4.126733    -0.025960  \n",
       "  2016-05-27 17:30:00   -1.142716  3.861286  4.037056    -0.025960  \n",
       "  2016-05-27 17:40:00   -1.075524  3.739372  3.954349    -0.025960  \n",
       "  2016-05-27 17:50:00   -1.008332  3.484137  3.812863    -0.025960  \n",
       "  2016-05-27 18:00:00   -0.941140  3.183273  3.714143    -0.025960  \n",
       "  \n",
       "  [3947 rows x 9 columns],\n",
       "  date\n",
       "  2016-04-30 08:20:00    370\n",
       "  2016-04-30 08:30:00    590\n",
       "  2016-04-30 08:40:00    320\n",
       "  2016-04-30 08:50:00    310\n",
       "  2016-04-30 09:00:00    260\n",
       "                        ... \n",
       "  2016-05-27 17:20:00    100\n",
       "  2016-05-27 17:30:00     90\n",
       "  2016-05-27 17:40:00    270\n",
       "  2016-05-27 17:50:00    420\n",
       "  2016-05-27 18:00:00    430\n",
       "  Name: Appliances, Length: 3947, dtype: int64]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneyelim.get_features_and_labels(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [None]*3\n",
    "b = [a,a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, 5, None], [None, 5, None]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][1] = 5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[None]*3]*2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, 5], [None, None, 5]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]=5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, None]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[None for _ in range(3)] for _ in range(2)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None, None], [None, None, 2]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][2]=2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,4,5]\n",
    "(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[2,4,6],[1,3,5]]\n",
    "c[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
