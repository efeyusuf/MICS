{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Datasets/first-order-theorem-proving_csv.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.1168</td>\n",
       "      <td>1.7836</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.30543</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.59356</td>\n",
       "      <td>-0.23316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.6676</td>\n",
       "      <td>1.68030</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.46300</td>\n",
       "      <td>0.27015</td>\n",
       "      <td>-1.55590</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.1168</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>1.54030</td>\n",
       "      <td>-0.31850</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.59356</td>\n",
       "      <td>-0.23316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.6676</td>\n",
       "      <td>1.69740</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.48010</td>\n",
       "      <td>0.21361</td>\n",
       "      <td>-1.55590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.1168</td>\n",
       "      <td>1.7836</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.30543</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.59356</td>\n",
       "      <td>-0.23316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.6676</td>\n",
       "      <td>1.69170</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.47440</td>\n",
       "      <td>0.23245</td>\n",
       "      <td>-1.55590</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.1168</td>\n",
       "      <td>1.7836</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.30543</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.59356</td>\n",
       "      <td>-0.23316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.6676</td>\n",
       "      <td>1.65750</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.44010</td>\n",
       "      <td>0.34553</td>\n",
       "      <td>-1.55590</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.1168</td>\n",
       "      <td>1.7836</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.30543</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.59356</td>\n",
       "      <td>-0.23316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.6676</td>\n",
       "      <td>1.66320</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.44580</td>\n",
       "      <td>0.32669</td>\n",
       "      <td>-1.55590</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.97422</td>\n",
       "      <td>-2.82510</td>\n",
       "      <td>2.7381</td>\n",
       "      <td>-1.0652</td>\n",
       "      <td>-1.09820</td>\n",
       "      <td>-0.32988</td>\n",
       "      <td>1.10730</td>\n",
       "      <td>-0.28157</td>\n",
       "      <td>0.76331</td>\n",
       "      <td>-0.50533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.56363</td>\n",
       "      <td>-1.79950</td>\n",
       "      <td>3.4576</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.66229</td>\n",
       "      <td>-0.16876</td>\n",
       "      <td>0.71854</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.72640</td>\n",
       "      <td>-2.80740</td>\n",
       "      <td>2.7381</td>\n",
       "      <td>-1.0652</td>\n",
       "      <td>-0.81894</td>\n",
       "      <td>0.58234</td>\n",
       "      <td>0.50195</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>0.62074</td>\n",
       "      <td>-0.50533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.57834</td>\n",
       "      <td>-1.88710</td>\n",
       "      <td>3.4576</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.47584</td>\n",
       "      <td>0.10746</td>\n",
       "      <td>0.44636</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.98776</td>\n",
       "      <td>-2.92290</td>\n",
       "      <td>2.7381</td>\n",
       "      <td>-1.0652</td>\n",
       "      <td>-1.11050</td>\n",
       "      <td>-0.38680</td>\n",
       "      <td>1.14050</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>0.79415</td>\n",
       "      <td>-0.50533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.56520</td>\n",
       "      <td>-1.86910</td>\n",
       "      <td>3.4576</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.67537</td>\n",
       "      <td>-0.20043</td>\n",
       "      <td>0.74140</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.98421</td>\n",
       "      <td>-3.41720</td>\n",
       "      <td>2.7381</td>\n",
       "      <td>-1.0652</td>\n",
       "      <td>-1.10650</td>\n",
       "      <td>-0.41329</td>\n",
       "      <td>1.14730</td>\n",
       "      <td>-0.28157</td>\n",
       "      <td>0.77468</td>\n",
       "      <td>-0.64141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.56491</td>\n",
       "      <td>-2.20540</td>\n",
       "      <td>3.4576</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.67292</td>\n",
       "      <td>-0.19449</td>\n",
       "      <td>0.73711</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.78667</td>\n",
       "      <td>-2.58590</td>\n",
       "      <td>2.7381</td>\n",
       "      <td>-1.0652</td>\n",
       "      <td>-0.70168</td>\n",
       "      <td>0.45861</td>\n",
       "      <td>0.44585</td>\n",
       "      <td>-0.28157</td>\n",
       "      <td>0.42516</td>\n",
       "      <td>-0.50533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.57834</td>\n",
       "      <td>-1.85600</td>\n",
       "      <td>3.4576</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.30657</td>\n",
       "      <td>0.21633</td>\n",
       "      <td>0.24263</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1       V2      V3      V4       V5       V6       V7       V8  \\\n",
       "0   1.48950  0.81688  2.1168  1.7836  1.53460 -0.30543 -1.24770 -0.14537   \n",
       "1   1.48950  0.81688  2.1168  1.7895  1.54030 -0.31850 -1.24770 -0.14537   \n",
       "2   1.48950  0.81688  2.1168  1.7836  1.53460 -0.30543 -1.24770 -0.14537   \n",
       "3   1.48950  0.81688  2.1168  1.7836  1.53460 -0.30543 -1.24770 -0.14537   \n",
       "4   1.48950  0.81688  2.1168  1.7836  1.53460 -0.30543 -1.24770 -0.14537   \n",
       "..      ...      ...     ...     ...      ...      ...      ...      ...   \n",
       "95 -0.97422 -2.82510  2.7381 -1.0652 -1.09820 -0.32988  1.10730 -0.28157   \n",
       "96 -0.72640 -2.80740  2.7381 -1.0652 -0.81894  0.58234  0.50195 -0.14537   \n",
       "97 -0.98776 -2.92290  2.7381 -1.0652 -1.11050 -0.38680  1.14050 -0.14537   \n",
       "98 -0.98421 -3.41720  2.7381 -1.0652 -1.10650 -0.41329  1.14730 -0.28157   \n",
       "99 -0.78667 -2.58590  2.7381 -1.0652 -0.70168  0.45861  0.44585 -0.28157   \n",
       "\n",
       "         V9      V10  ...       V43      V44      V45     V46      V47  \\\n",
       "0  -0.59356 -0.23316  ...  0.026975  1.83230  0.97743  2.6676  1.68030   \n",
       "1  -0.59356 -0.23316  ...  0.026975  1.83230  0.97743  2.6676  1.69740   \n",
       "2  -0.59356 -0.23316  ...  0.026975  1.83230  0.97743  2.6676  1.69170   \n",
       "3  -0.59356 -0.23316  ...  0.026975  1.83230  0.97743  2.6676  1.65750   \n",
       "4  -0.59356 -0.23316  ...  0.026975  1.83230  0.97743  2.6676  1.66320   \n",
       "..      ...      ...  ...       ...      ...      ...     ...      ...   \n",
       "95  0.76331 -0.50533  ... -0.112290 -0.56363 -1.79950  3.4576 -0.55678   \n",
       "96  0.62074 -0.50533  ... -0.112290 -0.57834 -1.88710  3.4576 -0.55678   \n",
       "97  0.79415 -0.50533  ... -0.112290 -0.56520 -1.86910  3.4576 -0.55678   \n",
       "98  0.77468 -0.64141  ... -0.112290 -0.56491 -2.20540  3.4576 -0.55678   \n",
       "99  0.42516 -0.50533  ... -0.112290 -0.57834 -1.85600  3.4576 -0.55678   \n",
       "\n",
       "        V48      V49      V50      V51  Class  \n",
       "0  -0.48278  1.46300  0.27015 -1.55590      6  \n",
       "1  -0.48278  1.48010  0.21361 -1.55590      1  \n",
       "2  -0.48278  1.47440  0.23245 -1.55590      6  \n",
       "3  -0.48278  1.44010  0.34553 -1.55590      6  \n",
       "4  -0.48278  1.44580  0.32669 -1.55590      6  \n",
       "..      ...      ...      ...      ...    ...  \n",
       "95 -0.48990 -0.66229 -0.16876  0.71854      3  \n",
       "96 -0.48990 -0.47584  0.10746  0.44636      5  \n",
       "97 -0.48990 -0.67537 -0.20043  0.74140      5  \n",
       "98 -0.48990 -0.67292 -0.19449  0.73711      3  \n",
       "99 -0.48990 -0.30657  0.21633  0.24263      6  \n",
       "\n",
       "[100 rows x 52 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=None)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V42</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.11680</td>\n",
       "      <td>1.78360</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.305430</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.593560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.66760</td>\n",
       "      <td>1.68030</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.46300</td>\n",
       "      <td>0.27015</td>\n",
       "      <td>-1.55590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.11680</td>\n",
       "      <td>1.78950</td>\n",
       "      <td>1.54030</td>\n",
       "      <td>-0.318500</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.593560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.66760</td>\n",
       "      <td>1.69740</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.48010</td>\n",
       "      <td>0.21361</td>\n",
       "      <td>-1.55590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.11680</td>\n",
       "      <td>1.78360</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.305430</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.593560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.66760</td>\n",
       "      <td>1.69170</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.47440</td>\n",
       "      <td>0.23245</td>\n",
       "      <td>-1.55590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.11680</td>\n",
       "      <td>1.78360</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.305430</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.593560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.66760</td>\n",
       "      <td>1.65750</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.44010</td>\n",
       "      <td>0.34553</td>\n",
       "      <td>-1.55590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.81688</td>\n",
       "      <td>2.11680</td>\n",
       "      <td>1.78360</td>\n",
       "      <td>1.53460</td>\n",
       "      <td>-0.305430</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>-0.14537</td>\n",
       "      <td>-0.593560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>1.83230</td>\n",
       "      <td>0.97743</td>\n",
       "      <td>2.66760</td>\n",
       "      <td>1.66320</td>\n",
       "      <td>-0.48278</td>\n",
       "      <td>1.44580</td>\n",
       "      <td>0.32669</td>\n",
       "      <td>-1.55590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>5</td>\n",
       "      <td>1.38650</td>\n",
       "      <td>0.83152</td>\n",
       "      <td>-0.23966</td>\n",
       "      <td>1.16080</td>\n",
       "      <td>0.92316</td>\n",
       "      <td>0.763810</td>\n",
       "      <td>-1.12100</td>\n",
       "      <td>-0.55397</td>\n",
       "      <td>-0.539690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.36328</td>\n",
       "      <td>0.99018</td>\n",
       "      <td>-0.54541</td>\n",
       "      <td>-0.33836</td>\n",
       "      <td>-0.21738</td>\n",
       "      <td>-0.56571</td>\n",
       "      <td>-0.46486</td>\n",
       "      <td>0.71176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.50624</td>\n",
       "      <td>-0.58481</td>\n",
       "      <td>-0.33987</td>\n",
       "      <td>-0.49442</td>\n",
       "      <td>-0.40892</td>\n",
       "      <td>-0.090004</td>\n",
       "      <td>0.39949</td>\n",
       "      <td>0.67183</td>\n",
       "      <td>0.059521</td>\n",
       "      <td>...</td>\n",
       "      <td>1.88690</td>\n",
       "      <td>0.305510</td>\n",
       "      <td>-0.57834</td>\n",
       "      <td>-1.34880</td>\n",
       "      <td>-0.54541</td>\n",
       "      <td>-0.55678</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.53790</td>\n",
       "      <td>-0.46486</td>\n",
       "      <td>0.68376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6115</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.59253</td>\n",
       "      <td>0.73765</td>\n",
       "      <td>-0.62354</td>\n",
       "      <td>-0.56530</td>\n",
       "      <td>-0.73434</td>\n",
       "      <td>-0.599120</td>\n",
       "      <td>0.88843</td>\n",
       "      <td>-0.41777</td>\n",
       "      <td>-0.334510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.28034</td>\n",
       "      <td>0.91559</td>\n",
       "      <td>-0.43858</td>\n",
       "      <td>-0.33992</td>\n",
       "      <td>-0.48990</td>\n",
       "      <td>-0.54816</td>\n",
       "      <td>-0.13433</td>\n",
       "      <td>0.59309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6116</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.51839</td>\n",
       "      <td>0.43125</td>\n",
       "      <td>-0.49860</td>\n",
       "      <td>-0.39309</td>\n",
       "      <td>-0.48333</td>\n",
       "      <td>-0.642260</td>\n",
       "      <td>0.68166</td>\n",
       "      <td>-0.28157</td>\n",
       "      <td>-0.204090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>-0.43072</td>\n",
       "      <td>0.34074</td>\n",
       "      <td>-0.19947</td>\n",
       "      <td>-0.40686</td>\n",
       "      <td>-0.34960</td>\n",
       "      <td>-0.37141</td>\n",
       "      <td>-0.27919</td>\n",
       "      <td>0.45935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6117</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.43252</td>\n",
       "      <td>0.16421</td>\n",
       "      <td>2.73810</td>\n",
       "      <td>-0.94444</td>\n",
       "      <td>-0.60105</td>\n",
       "      <td>2.379000</td>\n",
       "      <td>-0.39452</td>\n",
       "      <td>1.76140</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16374</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>-0.12816</td>\n",
       "      <td>0.42741</td>\n",
       "      <td>3.45760</td>\n",
       "      <td>-0.50028</td>\n",
       "      <td>-0.47708</td>\n",
       "      <td>-0.22345</td>\n",
       "      <td>3.18260</td>\n",
       "      <td>-0.74757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6118 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label       V1       V2       V3       V4       V5        V6       V7  \\\n",
       "0         5  1.48950  0.81688  2.11680  1.78360  1.53460 -0.305430 -1.24770   \n",
       "1         0  1.48950  0.81688  2.11680  1.78950  1.54030 -0.318500 -1.24770   \n",
       "2         5  1.48950  0.81688  2.11680  1.78360  1.53460 -0.305430 -1.24770   \n",
       "3         5  1.48950  0.81688  2.11680  1.78360  1.53460 -0.305430 -1.24770   \n",
       "4         5  1.48950  0.81688  2.11680  1.78360  1.53460 -0.305430 -1.24770   \n",
       "...     ...      ...      ...      ...      ...      ...       ...      ...   \n",
       "6113      5  1.38650  0.83152 -0.23966  1.16080  0.92316  0.763810 -1.12100   \n",
       "6114      5 -0.50624 -0.58481 -0.33987 -0.49442 -0.40892 -0.090004  0.39949   \n",
       "6115      4 -0.59253  0.73765 -0.62354 -0.56530 -0.73434 -0.599120  0.88843   \n",
       "6116      5 -0.51839  0.43125 -0.49860 -0.39309 -0.48333 -0.642260  0.68166   \n",
       "6117      3 -0.43252  0.16421  2.73810 -0.94444 -0.60105  2.379000 -0.39452   \n",
       "\n",
       "           V8        V9  ...      V42       V43      V44      V45      V46  \\\n",
       "0    -0.14537 -0.593560  ... -0.16374  0.026975  1.83230  0.97743  2.66760   \n",
       "1    -0.14537 -0.593560  ... -0.16374  0.026975  1.83230  0.97743  2.66760   \n",
       "2    -0.14537 -0.593560  ... -0.16374  0.026975  1.83230  0.97743  2.66760   \n",
       "3    -0.14537 -0.593560  ... -0.16374  0.026975  1.83230  0.97743  2.66760   \n",
       "4    -0.14537 -0.593560  ... -0.16374  0.026975  1.83230  0.97743  2.66760   \n",
       "...       ...       ...  ...      ...       ...      ...      ...      ...   \n",
       "6113 -0.55397 -0.539690  ... -0.16374 -0.112290 -0.36328  0.99018 -0.54541   \n",
       "6114  0.67183  0.059521  ...  1.88690  0.305510 -0.57834 -1.34880 -0.54541   \n",
       "6115 -0.41777 -0.334510  ... -0.16374 -0.112290 -0.28034  0.91559 -0.43858   \n",
       "6116 -0.28157 -0.204090  ... -0.16374  0.026975 -0.43072  0.34074 -0.19947   \n",
       "6117  1.76140  2.810000  ... -0.16374 -0.112290 -0.12816  0.42741  3.45760   \n",
       "\n",
       "          V47      V48      V49      V50      V51  \n",
       "0     1.68030 -0.48278  1.46300  0.27015 -1.55590  \n",
       "1     1.69740 -0.48278  1.48010  0.21361 -1.55590  \n",
       "2     1.69170 -0.48278  1.47440  0.23245 -1.55590  \n",
       "3     1.65750 -0.48278  1.44010  0.34553 -1.55590  \n",
       "4     1.66320 -0.48278  1.44580  0.32669 -1.55590  \n",
       "...       ...      ...      ...      ...      ...  \n",
       "6113 -0.33836 -0.21738 -0.56571 -0.46486  0.71176  \n",
       "6114 -0.55678 -0.48990 -0.53790 -0.46486  0.68376  \n",
       "6115 -0.33992 -0.48990 -0.54816 -0.13433  0.59309  \n",
       "6116 -0.40686 -0.34960 -0.37141 -0.27919  0.45935  \n",
       "6117 -0.50028 -0.47708 -0.22345  3.18260 -0.74757  \n",
       "\n",
       "[6118 rows x 52 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_column = df.pop('Class')\n",
    "label_column = np.asarray(label_column)\n",
    "label_column = label_column - 1\n",
    "df.insert(0, 'label', label_column)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 52 and number of rows is: 6118\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010    5\n",
       "4888    2\n",
       "3508    0\n",
       "2485    4\n",
       "2375    5\n",
       "       ..\n",
       "5851    4\n",
       "2330    2\n",
       "1902    5\n",
       "1289    0\n",
       "1662    1\n",
       "Name: label, Length: 4894, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V42</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>-1.049193</td>\n",
       "      <td>-2.646142</td>\n",
       "      <td>-0.978325</td>\n",
       "      <td>-0.992785</td>\n",
       "      <td>-1.188927</td>\n",
       "      <td>-0.243668</td>\n",
       "      <td>1.158136</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>1.403881</td>\n",
       "      <td>-0.642189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.127991</td>\n",
       "      <td>-0.578185</td>\n",
       "      <td>-1.943720</td>\n",
       "      <td>-0.532895</td>\n",
       "      <td>-0.556443</td>\n",
       "      <td>-0.488727</td>\n",
       "      <td>-0.649132</td>\n",
       "      <td>-0.456475</td>\n",
       "      <td>0.794834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>-0.033691</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.180647</td>\n",
       "      <td>0.122679</td>\n",
       "      <td>-0.095518</td>\n",
       "      <td>-0.687557</td>\n",
       "      <td>0.357672</td>\n",
       "      <td>-0.155612</td>\n",
       "      <td>-0.114726</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.127991</td>\n",
       "      <td>-0.578185</td>\n",
       "      <td>0.932467</td>\n",
       "      <td>0.030919</td>\n",
       "      <td>-0.556443</td>\n",
       "      <td>-0.488727</td>\n",
       "      <td>-0.843227</td>\n",
       "      <td>-0.456475</td>\n",
       "      <td>0.989027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>-0.827254</td>\n",
       "      <td>-3.278007</td>\n",
       "      <td>2.341687</td>\n",
       "      <td>-0.729316</td>\n",
       "      <td>2.276311</td>\n",
       "      <td>-0.660671</td>\n",
       "      <td>-1.770774</td>\n",
       "      <td>0.868795</td>\n",
       "      <td>-0.200183</td>\n",
       "      <td>-0.642189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.127991</td>\n",
       "      <td>-0.423461</td>\n",
       "      <td>-2.113639</td>\n",
       "      <td>1.753357</td>\n",
       "      <td>-0.399262</td>\n",
       "      <td>-0.291596</td>\n",
       "      <td>1.744763</td>\n",
       "      <td>-0.456475</td>\n",
       "      <td>-1.600159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>-0.801216</td>\n",
       "      <td>-0.977780</td>\n",
       "      <td>-0.372338</td>\n",
       "      <td>-0.733599</td>\n",
       "      <td>-0.762407</td>\n",
       "      <td>-0.381209</td>\n",
       "      <td>0.831765</td>\n",
       "      <td>0.429763</td>\n",
       "      <td>0.387124</td>\n",
       "      <td>-0.238591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.040767</td>\n",
       "      <td>-0.517134</td>\n",
       "      <td>-0.504527</td>\n",
       "      <td>0.224826</td>\n",
       "      <td>-0.515104</td>\n",
       "      <td>-0.436871</td>\n",
       "      <td>-0.800978</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.798799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>-0.793144</td>\n",
       "      <td>-0.882079</td>\n",
       "      <td>-0.517617</td>\n",
       "      <td>-0.726277</td>\n",
       "      <td>-0.909346</td>\n",
       "      <td>-0.307321</td>\n",
       "      <td>0.933698</td>\n",
       "      <td>0.429763</td>\n",
       "      <td>0.421518</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>0.046463</td>\n",
       "      <td>-0.525240</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>-0.356880</td>\n",
       "      <td>-0.502662</td>\n",
       "      <td>-0.421276</td>\n",
       "      <td>-0.788260</td>\n",
       "      <td>0.234066</td>\n",
       "      <td>0.714065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>-0.136939</td>\n",
       "      <td>-0.003997</td>\n",
       "      <td>-0.575371</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>-0.184713</td>\n",
       "      <td>-0.283433</td>\n",
       "      <td>0.277217</td>\n",
       "      <td>0.429763</td>\n",
       "      <td>0.232387</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.040767</td>\n",
       "      <td>-0.532515</td>\n",
       "      <td>-0.457692</td>\n",
       "      <td>-0.441797</td>\n",
       "      <td>-0.533251</td>\n",
       "      <td>-0.459640</td>\n",
       "      <td>-0.155724</td>\n",
       "      <td>0.362558</td>\n",
       "      <td>0.040302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>-0.846590</td>\n",
       "      <td>-0.909454</td>\n",
       "      <td>-0.598472</td>\n",
       "      <td>-0.829477</td>\n",
       "      <td>-1.028852</td>\n",
       "      <td>-0.246942</td>\n",
       "      <td>1.016491</td>\n",
       "      <td>0.429763</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.040767</td>\n",
       "      <td>-0.549426</td>\n",
       "      <td>0.070240</td>\n",
       "      <td>-0.418145</td>\n",
       "      <td>-0.527225</td>\n",
       "      <td>-0.452084</td>\n",
       "      <td>-0.813370</td>\n",
       "      <td>0.200036</td>\n",
       "      <td>0.750023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>-0.474686</td>\n",
       "      <td>0.404901</td>\n",
       "      <td>-0.435462</td>\n",
       "      <td>-0.350177</td>\n",
       "      <td>-0.443151</td>\n",
       "      <td>-0.625379</td>\n",
       "      <td>0.643445</td>\n",
       "      <td>-0.301956</td>\n",
       "      <td>-0.238084</td>\n",
       "      <td>2.317601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>0.046463</td>\n",
       "      <td>-0.508078</td>\n",
       "      <td>0.062118</td>\n",
       "      <td>-0.222117</td>\n",
       "      <td>-0.485225</td>\n",
       "      <td>-0.399403</td>\n",
       "      <td>-0.552079</td>\n",
       "      <td>-0.304070</td>\n",
       "      <td>0.649195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-0.436391</td>\n",
       "      <td>-0.652378</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.265311</td>\n",
       "      <td>-0.380762</td>\n",
       "      <td>-0.029373</td>\n",
       "      <td>0.351604</td>\n",
       "      <td>-0.301956</td>\n",
       "      <td>-0.010245</td>\n",
       "      <td>-0.507659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185452</td>\n",
       "      <td>0.046463</td>\n",
       "      <td>-0.578185</td>\n",
       "      <td>-1.883398</td>\n",
       "      <td>-0.045789</td>\n",
       "      <td>-0.556443</td>\n",
       "      <td>-0.488727</td>\n",
       "      <td>0.107476</td>\n",
       "      <td>-0.456475</td>\n",
       "      <td>0.037884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>-0.375108</td>\n",
       "      <td>0.208544</td>\n",
       "      <td>-0.513017</td>\n",
       "      <td>-0.507796</td>\n",
       "      <td>-0.404525</td>\n",
       "      <td>0.158138</td>\n",
       "      <td>0.298552</td>\n",
       "      <td>-0.594643</td>\n",
       "      <td>-0.284508</td>\n",
       "      <td>-0.642189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175921</td>\n",
       "      <td>-0.127991</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>0.660358</td>\n",
       "      <td>-0.477652</td>\n",
       "      <td>-0.260989</td>\n",
       "      <td>-0.118190</td>\n",
       "      <td>-0.217785</td>\n",
       "      <td>0.153145</td>\n",
       "      <td>0.169110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4894 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3        V4        V5        V6        V7  \\\n",
       "3010 -1.049193 -2.646142 -0.978325 -0.992785 -1.188927 -0.243668  1.158136   \n",
       "4888 -0.033691  0.012195 -0.180647  0.122679 -0.095518 -0.687557  0.357672   \n",
       "3508 -0.827254 -3.278007  2.341687 -0.729316  2.276311 -0.660671 -1.770774   \n",
       "2485 -0.801216 -0.977780 -0.372338 -0.733599 -0.762407 -0.381209  0.831765   \n",
       "2375 -0.793144 -0.882079 -0.517617 -0.726277 -0.909346 -0.307321  0.933698   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5851 -0.136939 -0.003997 -0.575371  0.031681 -0.184713 -0.283433  0.277217   \n",
       "2330 -0.846590 -0.909454 -0.598472 -0.829477 -1.028852 -0.246942  1.016491   \n",
       "1902 -0.474686  0.404901 -0.435462 -0.350177 -0.443151 -0.625379  0.643445   \n",
       "1289 -0.436391 -0.652378 -0.023843 -0.265311 -0.380762 -0.029373  0.351604   \n",
       "1662 -0.375108  0.208544 -0.513017 -0.507796 -0.404525  0.158138  0.298552   \n",
       "\n",
       "            V8        V9       V10  ...       V42       V43       V44  \\\n",
       "3010 -0.009270  1.403881 -0.642189  ... -0.175921 -0.127991 -0.578185   \n",
       "4888 -0.155612 -0.114726 -0.373120  ... -0.175921 -0.127991 -0.578185   \n",
       "3508  0.868795 -0.200183 -0.642189  ... -0.175921 -0.127991 -0.423461   \n",
       "2485  0.429763  0.387124 -0.238591  ... -0.175921 -0.040767 -0.517134   \n",
       "2375  0.429763  0.421518 -0.373120  ... -0.175921  0.046463 -0.525240   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5851  0.429763  0.232387 -0.373120  ... -0.175921 -0.040767 -0.532515   \n",
       "2330  0.429763  0.445400 -0.373120  ... -0.175921 -0.040767 -0.549426   \n",
       "1902 -0.301956 -0.238084  2.317601  ... -0.175921  0.046463 -0.508078   \n",
       "1289 -0.301956 -0.010245 -0.507659  ...  0.185452  0.046463 -0.578185   \n",
       "1662 -0.594643 -0.284508 -0.642189  ... -0.175921 -0.127991 -0.141943   \n",
       "\n",
       "           V45       V46       V47       V48       V49       V50       V51  \n",
       "3010 -1.943720 -0.532895 -0.556443 -0.488727 -0.649132 -0.456475  0.794834  \n",
       "4888  0.932467  0.030919 -0.556443 -0.488727 -0.843227 -0.456475  0.989027  \n",
       "3508 -2.113639  1.753357 -0.399262 -0.291596  1.744763 -0.456475 -1.600159  \n",
       "2485 -0.504527  0.224826 -0.515104 -0.436871 -0.800978  0.007996  0.798799  \n",
       "2375 -0.004216 -0.356880 -0.502662 -0.421276 -0.788260  0.234066  0.714065  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5851 -0.457692 -0.441797 -0.533251 -0.459640 -0.155724  0.362558  0.040302  \n",
       "2330  0.070240 -0.418145 -0.527225 -0.452084 -0.813370  0.200036  0.750023  \n",
       "1902  0.062118 -0.222117 -0.485225 -0.399403 -0.552079 -0.304070  0.649195  \n",
       "1289 -1.883398 -0.045789 -0.556443 -0.488727  0.107476 -0.456475  0.037884  \n",
       "1662  0.660358 -0.477652 -0.260989 -0.118190 -0.217785  0.153145  0.169110  \n",
       "\n",
       "[4894 rows x 51 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_rand_list(max_val, count):\n",
    "    randomlist = random.sample(range(0, max_val + 1), count)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 6, hidden_size=64):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(6, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2554\n",
       "0    1089\n",
       "2     748\n",
       "4     624\n",
       "3     617\n",
       "1     486\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.8548 - accuracy: 0.4172 - val_loss: 1.6425 - val_accuracy: 0.4028\n",
      "Epoch 2/300\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 1.6199 - accuracy: 0.4211 - val_loss: 1.5809 - val_accuracy: 0.4028\n",
      "Epoch 3/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5863 - accuracy: 0.4217 - val_loss: 1.5795 - val_accuracy: 0.4028\n",
      "Epoch 4/300\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 1.5857 - accuracy: 0.4195 - val_loss: 1.5825 - val_accuracy: 0.4028\n",
      "Epoch 5/300\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 1.5835 - accuracy: 0.4211 - val_loss: 1.5637 - val_accuracy: 0.4028\n",
      "Epoch 6/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5672 - accuracy: 0.4211 - val_loss: 1.5606 - val_accuracy: 0.4028\n",
      "Epoch 7/300\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 1.5644 - accuracy: 0.4205 - val_loss: 1.5832 - val_accuracy: 0.4028\n",
      "Epoch 8/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5823 - accuracy: 0.4211 - val_loss: 1.6035 - val_accuracy: 0.4028\n",
      "Epoch 9/300\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 1.5757 - accuracy: 0.4205 - val_loss: 1.5510 - val_accuracy: 0.4028\n",
      "Epoch 10/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5730 - accuracy: 0.4217 - val_loss: 1.5680 - val_accuracy: 0.4028\n",
      "Epoch 11/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5731 - accuracy: 0.4211 - val_loss: 1.5789 - val_accuracy: 0.4028\n",
      "Epoch 12/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5923 - accuracy: 0.4211 - val_loss: 1.5932 - val_accuracy: 0.4028\n",
      "Epoch 13/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5900 - accuracy: 0.4211 - val_loss: 1.5809 - val_accuracy: 0.4028\n",
      "Epoch 14/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5801 - accuracy: 0.4211 - val_loss: 1.5759 - val_accuracy: 0.4028\n",
      "Epoch 15/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5817 - accuracy: 0.4211 - val_loss: 1.5784 - val_accuracy: 0.4028\n",
      "Epoch 16/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5921 - accuracy: 0.4211 - val_loss: 1.5929 - val_accuracy: 0.4028\n",
      "Epoch 17/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5883 - accuracy: 0.4211 - val_loss: 1.5720 - val_accuracy: 0.4028\n",
      "Epoch 18/300\n",
      "153/153 [==============================] - 1s 3ms/step - loss: 1.5973 - accuracy: 0.4211 - val_loss: 1.6164 - val_accuracy: 0.4028\n",
      "Epoch 19/300\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 1.5883 - accuracy: 0.4223\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5901 - accuracy: 0.4211 - val_loss: 1.5777 - val_accuracy: 0.4028\n",
      "Epoch 20/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5742 - accuracy: 0.4211 - val_loss: 1.5750 - val_accuracy: 0.4028\n",
      "Epoch 21/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5659 - accuracy: 0.4211 - val_loss: 1.5683 - val_accuracy: 0.4028\n",
      "Epoch 22/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5531 - accuracy: 0.4211 - val_loss: 1.5467 - val_accuracy: 0.4028\n",
      "Epoch 23/300\n",
      "153/153 [==============================] - 1s 3ms/step - loss: 1.5576 - accuracy: 0.4211 - val_loss: 1.5619 - val_accuracy: 0.4028\n",
      "Epoch 24/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5603 - accuracy: 0.4211 - val_loss: 1.5658 - val_accuracy: 0.4028\n",
      "Epoch 25/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5521 - accuracy: 0.4211 - val_loss: 1.5600 - val_accuracy: 0.4028\n",
      "Epoch 26/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5519 - accuracy: 0.4211 - val_loss: 1.5508 - val_accuracy: 0.4028\n",
      "Epoch 27/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5452 - accuracy: 0.4211 - val_loss: 1.5403 - val_accuracy: 0.4028\n",
      "Epoch 28/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5432 - accuracy: 0.4211 - val_loss: 1.5409 - val_accuracy: 0.4028\n",
      "Epoch 29/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5425 - accuracy: 0.4211 - val_loss: 1.5484 - val_accuracy: 0.4028\n",
      "Epoch 30/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5495 - accuracy: 0.4211 - val_loss: 1.5570 - val_accuracy: 0.4028\n",
      "Epoch 31/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5371 - accuracy: 0.4211 - val_loss: 1.5497 - val_accuracy: 0.4028\n",
      "Epoch 32/300\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 1.5377 - accuracy: 0.4211 - val_loss: 1.5316 - val_accuracy: 0.4028\n",
      "Epoch 33/300\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 1.5324 - accuracy: 0.4211 - val_loss: 1.5333 - val_accuracy: 0.4028\n",
      "Epoch 34/300\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 1.5304 - accuracy: 0.4211 - val_loss: 1.5303 - val_accuracy: 0.4306\n",
      "Epoch 35/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5388 - accuracy: 0.4181 - val_loss: 1.5406 - val_accuracy: 0.4028\n",
      "Epoch 36/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5286 - accuracy: 0.4177 - val_loss: 1.5184 - val_accuracy: 0.4028\n",
      "Epoch 37/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5283 - accuracy: 0.4168 - val_loss: 1.5442 - val_accuracy: 0.4028\n",
      "Epoch 38/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5328 - accuracy: 0.4226 - val_loss: 1.5284 - val_accuracy: 0.4028\n",
      "Epoch 39/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5298 - accuracy: 0.4240 - val_loss: 1.5228 - val_accuracy: 0.4028\n",
      "Epoch 40/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5409 - accuracy: 0.4221 - val_loss: 1.5552 - val_accuracy: 0.4436\n",
      "Epoch 41/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5359 - accuracy: 0.4281 - val_loss: 1.5323 - val_accuracy: 0.4028\n",
      "Epoch 42/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5333 - accuracy: 0.4191 - val_loss: 1.5444 - val_accuracy: 0.4028\n",
      "Epoch 43/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5285 - accuracy: 0.4156 - val_loss: 1.5328 - val_accuracy: 0.4028\n",
      "Epoch 44/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5299 - accuracy: 0.4156 - val_loss: 1.5327 - val_accuracy: 0.4028\n",
      "Epoch 45/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5344 - accuracy: 0.4318 - val_loss: 1.5462 - val_accuracy: 0.4518\n",
      "Epoch 46/300\n",
      "153/153 [==============================] - ETA: 0s - loss: 1.5266 - accuracy: 0.4293\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5266 - accuracy: 0.4293 - val_loss: 1.5226 - val_accuracy: 0.4404\n",
      "Epoch 47/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5150 - accuracy: 0.4332 - val_loss: 1.5341 - val_accuracy: 0.4453\n",
      "Epoch 48/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5163 - accuracy: 0.4362 - val_loss: 1.5265 - val_accuracy: 0.4502\n",
      "Epoch 49/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5073 - accuracy: 0.4283 - val_loss: 1.5346 - val_accuracy: 0.4518\n",
      "Epoch 50/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5114 - accuracy: 0.4307 - val_loss: 1.5286 - val_accuracy: 0.4379\n",
      "Epoch 51/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5073 - accuracy: 0.4287 - val_loss: 1.5129 - val_accuracy: 0.4469\n",
      "Epoch 52/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5040 - accuracy: 0.4303 - val_loss: 1.5114 - val_accuracy: 0.4428\n",
      "Epoch 53/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5014 - accuracy: 0.4307 - val_loss: 1.5248 - val_accuracy: 0.4518\n",
      "Epoch 54/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5077 - accuracy: 0.4354 - val_loss: 1.5177 - val_accuracy: 0.4518\n",
      "Epoch 55/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5071 - accuracy: 0.4277 - val_loss: 1.5150 - val_accuracy: 0.4477\n",
      "Epoch 56/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5055 - accuracy: 0.4389 - val_loss: 1.5165 - val_accuracy: 0.4485\n",
      "Epoch 57/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5031 - accuracy: 0.4238 - val_loss: 1.5170 - val_accuracy: 0.4477\n",
      "Epoch 58/300\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.5065 - accuracy: 0.4252 - val_loss: 1.5137 - val_accuracy: 0.4485\n",
      "Epoch 59/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5025 - accuracy: 0.4244 - val_loss: 1.5183 - val_accuracy: 0.4395\n",
      "Epoch 60/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5061 - accuracy: 0.4297 - val_loss: 1.5140 - val_accuracy: 0.4428\n",
      "Epoch 61/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4970 - accuracy: 0.4330 - val_loss: 1.5222 - val_accuracy: 0.4461\n",
      "Epoch 62/300\n",
      "153/153 [==============================] - ETA: 0s - loss: 1.5048 - accuracy: 0.4277\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.5048 - accuracy: 0.4277 - val_loss: 1.5124 - val_accuracy: 0.4526\n",
      "Epoch 63/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4976 - accuracy: 0.4369 - val_loss: 1.5121 - val_accuracy: 0.4477\n",
      "Epoch 64/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4888 - accuracy: 0.4381 - val_loss: 1.5068 - val_accuracy: 0.4502\n",
      "Epoch 65/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4931 - accuracy: 0.4418 - val_loss: 1.5085 - val_accuracy: 0.4542\n",
      "Epoch 66/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4822 - accuracy: 0.4444 - val_loss: 1.5104 - val_accuracy: 0.4469\n",
      "Epoch 67/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4899 - accuracy: 0.4401 - val_loss: 1.5066 - val_accuracy: 0.4567\n",
      "Epoch 68/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4810 - accuracy: 0.4432 - val_loss: 1.5077 - val_accuracy: 0.4526\n",
      "Epoch 69/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4885 - accuracy: 0.4403 - val_loss: 1.5048 - val_accuracy: 0.4526\n",
      "Epoch 70/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4819 - accuracy: 0.4412 - val_loss: 1.5028 - val_accuracy: 0.4526\n",
      "Epoch 71/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4888 - accuracy: 0.4387 - val_loss: 1.5042 - val_accuracy: 0.4534\n",
      "Epoch 72/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4855 - accuracy: 0.4418 - val_loss: 1.4984 - val_accuracy: 0.4559\n",
      "Epoch 73/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4845 - accuracy: 0.4405 - val_loss: 1.4960 - val_accuracy: 0.4583\n",
      "Epoch 74/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4855 - accuracy: 0.4397 - val_loss: 1.5042 - val_accuracy: 0.4542\n",
      "Epoch 75/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4823 - accuracy: 0.4399 - val_loss: 1.4942 - val_accuracy: 0.4542\n",
      "Epoch 76/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4849 - accuracy: 0.4407 - val_loss: 1.4983 - val_accuracy: 0.4567\n",
      "Epoch 77/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4869 - accuracy: 0.4379 - val_loss: 1.5005 - val_accuracy: 0.4485\n",
      "Epoch 78/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4800 - accuracy: 0.4383 - val_loss: 1.4995 - val_accuracy: 0.4567\n",
      "Epoch 79/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4780 - accuracy: 0.4434 - val_loss: 1.5003 - val_accuracy: 0.4493\n",
      "Epoch 80/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4797 - accuracy: 0.4422 - val_loss: 1.5036 - val_accuracy: 0.4559\n",
      "Epoch 81/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4859 - accuracy: 0.4373 - val_loss: 1.5025 - val_accuracy: 0.4542\n",
      "Epoch 82/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4745 - accuracy: 0.4459 - val_loss: 1.5100 - val_accuracy: 0.4583\n",
      "Epoch 83/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4711 - accuracy: 0.4495 - val_loss: 1.5031 - val_accuracy: 0.4542\n",
      "Epoch 84/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4899 - accuracy: 0.4409 - val_loss: 1.5011 - val_accuracy: 0.4518\n",
      "Epoch 85/300\n",
      "135/153 [=========================>....] - ETA: 0s - loss: 1.4734 - accuracy: 0.4563\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4805 - accuracy: 0.4501 - val_loss: 1.4977 - val_accuracy: 0.4526\n",
      "Epoch 86/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4774 - accuracy: 0.4432 - val_loss: 1.4994 - val_accuracy: 0.4526\n",
      "Epoch 87/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4773 - accuracy: 0.4452 - val_loss: 1.5006 - val_accuracy: 0.4518\n",
      "Epoch 88/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4784 - accuracy: 0.4432 - val_loss: 1.5047 - val_accuracy: 0.4551\n",
      "Epoch 89/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4777 - accuracy: 0.4424 - val_loss: 1.4974 - val_accuracy: 0.4583\n",
      "Epoch 90/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4736 - accuracy: 0.4483 - val_loss: 1.4959 - val_accuracy: 0.4608\n",
      "Epoch 91/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4787 - accuracy: 0.4452 - val_loss: 1.4957 - val_accuracy: 0.4592\n",
      "Epoch 92/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4742 - accuracy: 0.4485 - val_loss: 1.4957 - val_accuracy: 0.4575\n",
      "Epoch 93/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4715 - accuracy: 0.4426 - val_loss: 1.4978 - val_accuracy: 0.4583\n",
      "Epoch 94/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4745 - accuracy: 0.4424 - val_loss: 1.4968 - val_accuracy: 0.4542\n",
      "Epoch 95/300\n",
      "152/153 [============================>.] - ETA: 0s - loss: 1.4702 - accuracy: 0.4422\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4710 - accuracy: 0.4420 - val_loss: 1.4961 - val_accuracy: 0.4575\n",
      "Epoch 96/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4725 - accuracy: 0.4459 - val_loss: 1.4954 - val_accuracy: 0.4567\n",
      "Epoch 97/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4752 - accuracy: 0.4450 - val_loss: 1.4970 - val_accuracy: 0.4551\n",
      "Epoch 98/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4667 - accuracy: 0.4473 - val_loss: 1.4954 - val_accuracy: 0.4583\n",
      "Epoch 99/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4741 - accuracy: 0.4481 - val_loss: 1.5001 - val_accuracy: 0.4583\n",
      "Epoch 100/300\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 1.4684 - accuracy: 0.4532 - val_loss: 1.4983 - val_accuracy: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46078431606292725"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.5, patience=10,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-6)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx_scaled], y = trainy.values,  \n",
    "                         validation_data = ([textx_scaled], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy\n",
    "#min_losses3[c].append(best_val_loss)\n",
    "#i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = MICS_model.predict(trainx_scaled)\n",
    "k = np.argmax(k, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2593\n",
       "5    2301\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = pd.DataFrame(k)\n",
    "k[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
