{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Datasets/winequality-white.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0         1            7.0              0.27         0.36            20.7   \n",
       "1         1            6.3              0.30         0.34             1.6   \n",
       "2         1            8.1              0.28         0.40             6.9   \n",
       "3         1            7.2              0.23         0.32             8.5   \n",
       "4         1            7.2              0.23         0.32             8.5   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "4893      1            6.2              0.21         0.29             1.6   \n",
       "4894      0            6.6              0.32         0.36             8.0   \n",
       "4895      1            6.5              0.24         0.19             1.2   \n",
       "4896      1            5.5              0.29         0.30             1.1   \n",
       "4897      1            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0         0.045                 45.0                 170.0  1.00100  3.00   \n",
       "1         0.049                 14.0                 132.0  0.99400  3.30   \n",
       "2         0.050                 30.0                  97.0  0.99510  3.26   \n",
       "3         0.058                 47.0                 186.0  0.99560  3.19   \n",
       "4         0.058                 47.0                 186.0  0.99560  3.19   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "4893      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "4894      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "4895      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "4896      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "4897      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulphates  alcohol  \n",
       "0          0.45      8.8  \n",
       "1          0.49      9.5  \n",
       "2          0.44     10.1  \n",
       "3          0.40      9.9  \n",
       "4          0.40      9.9  \n",
       "...         ...      ...  \n",
       "4893       0.50     11.2  \n",
       "4894       0.46      9.6  \n",
       "4895       0.46      9.4  \n",
       "4896       0.38     12.8  \n",
       "4897       0.32     11.8  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, sep = \";\", index_col=None)\n",
    "df = df.fillna(df.mean())\n",
    "df['quality'] = (df['quality'] >= 6).astype(int)\n",
    "label_column = df.pop('quality')\n",
    "label_column = np.asarray(label_column)\n",
    "df.insert(0, 'label', label_column)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 12 and number of rows is: 4898\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 3, hidden_size=256):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "        h = keras.layers.BatchNormalization()(h)\n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 3, hidden_size=256):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    h = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "    h = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h)    \n",
    "    h = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h) \n",
    "    h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "    h = keras.layers.Dropout(drop_out)(h) \n",
    "    h = keras.layers.BatchNormalization()(h) \n",
    "    \n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3258\n",
       "0    1640\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.9397 - accuracy: 0.7070 - val_loss: 1.3242 - val_accuracy: 0.7102\n",
      "Epoch 2/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.0498 - accuracy: 0.7356 - val_loss: 0.8814 - val_accuracy: 0.7398\n",
      "Epoch 3/300\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.9284 - accuracy: 0.7399 - val_loss: 0.9427 - val_accuracy: 0.6786\n",
      "Epoch 4/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8877 - accuracy: 0.7210 - val_loss: 0.8989 - val_accuracy: 0.7265\n",
      "Epoch 5/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.8762 - accuracy: 0.7340 - val_loss: 0.8130 - val_accuracy: 0.7561\n",
      "Epoch 6/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8547 - accuracy: 0.7305 - val_loss: 0.8808 - val_accuracy: 0.7480\n",
      "Epoch 7/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8480 - accuracy: 0.7437 - val_loss: 0.8303 - val_accuracy: 0.7786\n",
      "Epoch 8/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8703 - accuracy: 0.7430 - val_loss: 0.8362 - val_accuracy: 0.7704\n",
      "Epoch 9/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8605 - accuracy: 0.7412 - val_loss: 0.7727 - val_accuracy: 0.7653\n",
      "Epoch 10/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8666 - accuracy: 0.7430 - val_loss: 0.8138 - val_accuracy: 0.7051\n",
      "Epoch 11/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8221 - accuracy: 0.7417 - val_loss: 0.8434 - val_accuracy: 0.7296\n",
      "Epoch 12/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8210 - accuracy: 0.7392 - val_loss: 0.7620 - val_accuracy: 0.7602\n",
      "Epoch 13/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8159 - accuracy: 0.7300 - val_loss: 0.8530 - val_accuracy: 0.7500\n",
      "Epoch 14/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7919 - accuracy: 0.7343 - val_loss: 0.8411 - val_accuracy: 0.7673\n",
      "Epoch 15/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8131 - accuracy: 0.7417 - val_loss: 0.7671 - val_accuracy: 0.7816\n",
      "Epoch 16/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7730 - accuracy: 0.7425 - val_loss: 0.7885 - val_accuracy: 0.7673\n",
      "Epoch 17/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7810 - accuracy: 0.7397 - val_loss: 0.8140 - val_accuracy: 0.7388\n",
      "Epoch 18/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7691 - accuracy: 0.7292 - val_loss: 0.7036 - val_accuracy: 0.7561\n",
      "Epoch 19/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7556 - accuracy: 0.7392 - val_loss: 0.7473 - val_accuracy: 0.7561\n",
      "Epoch 20/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7628 - accuracy: 0.7333 - val_loss: 0.7928 - val_accuracy: 0.7673\n",
      "Epoch 21/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7687 - accuracy: 0.7369 - val_loss: 0.6829 - val_accuracy: 0.7694\n",
      "Epoch 22/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7609 - accuracy: 0.7343 - val_loss: 0.6908 - val_accuracy: 0.7643\n",
      "Epoch 23/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7540 - accuracy: 0.7427 - val_loss: 0.6781 - val_accuracy: 0.7755\n",
      "Epoch 24/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7390 - accuracy: 0.7376 - val_loss: 0.7383 - val_accuracy: 0.7612\n",
      "Epoch 25/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7263 - accuracy: 0.7371 - val_loss: 0.6721 - val_accuracy: 0.7694\n",
      "Epoch 26/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7165 - accuracy: 0.7399 - val_loss: 0.7223 - val_accuracy: 0.7714\n",
      "Epoch 27/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7034 - accuracy: 0.7315 - val_loss: 0.6868 - val_accuracy: 0.7592\n",
      "Epoch 28/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.7394 - val_loss: 0.6880 - val_accuracy: 0.7714\n",
      "Epoch 29/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.7389 - val_loss: 0.6983 - val_accuracy: 0.7327\n",
      "Epoch 30/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6908 - accuracy: 0.7379 - val_loss: 0.6606 - val_accuracy: 0.7633\n",
      "Epoch 31/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.7325 - val_loss: 0.6361 - val_accuracy: 0.7684\n",
      "Epoch 32/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.7389 - val_loss: 0.6794 - val_accuracy: 0.7327\n",
      "Epoch 33/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6876 - accuracy: 0.7340 - val_loss: 0.6373 - val_accuracy: 0.7592\n",
      "Epoch 34/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6721 - accuracy: 0.7374 - val_loss: 0.6504 - val_accuracy: 0.7541\n",
      "Epoch 35/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6744 - accuracy: 0.7363 - val_loss: 0.7061 - val_accuracy: 0.7153\n",
      "Epoch 36/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6861 - accuracy: 0.7310 - val_loss: 0.6491 - val_accuracy: 0.7296\n",
      "Epoch 37/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.7371 - val_loss: 0.6547 - val_accuracy: 0.7663\n",
      "Epoch 38/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6693 - accuracy: 0.7471 - val_loss: 0.6652 - val_accuracy: 0.7276\n",
      "Epoch 39/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.7307 - val_loss: 0.6333 - val_accuracy: 0.7806\n",
      "Epoch 40/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.7343 - val_loss: 0.6244 - val_accuracy: 0.7653\n",
      "Epoch 41/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.7386 - val_loss: 0.6131 - val_accuracy: 0.7765\n",
      "Epoch 42/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6569 - accuracy: 0.7409 - val_loss: 0.6402 - val_accuracy: 0.7449\n",
      "Epoch 43/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6597 - accuracy: 0.7379 - val_loss: 0.6528 - val_accuracy: 0.7765\n",
      "Epoch 44/300\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6649 - accuracy: 0.7450 - val_loss: 0.6313 - val_accuracy: 0.7735\n",
      "Epoch 45/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.7333 - val_loss: 0.6666 - val_accuracy: 0.7571\n",
      "Epoch 46/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6593 - accuracy: 0.7371 - val_loss: 0.6207 - val_accuracy: 0.7602\n",
      "Epoch 47/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.7315 - val_loss: 0.6446 - val_accuracy: 0.7551\n",
      "Epoch 48/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6489 - accuracy: 0.7363 - val_loss: 0.6273 - val_accuracy: 0.7735\n",
      "Epoch 49/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6630 - accuracy: 0.7404 - val_loss: 0.6556 - val_accuracy: 0.7622\n",
      "Epoch 50/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.7397 - val_loss: 0.6359 - val_accuracy: 0.7776\n",
      "Epoch 51/300\n",
      "115/123 [===========================>..] - ETA: 0s - loss: 0.6611 - accuracy: 0.7459\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.7443 - val_loss: 0.6783 - val_accuracy: 0.7500\n",
      "Epoch 52/300\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.7414 - val_loss: 0.5898 - val_accuracy: 0.7755\n",
      "Epoch 53/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7542 - val_loss: 0.5686 - val_accuracy: 0.7633\n",
      "Epoch 54/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5690 - accuracy: 0.7550 - val_loss: 0.5607 - val_accuracy: 0.7653\n",
      "Epoch 55/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.7560 - val_loss: 0.5499 - val_accuracy: 0.7582\n",
      "Epoch 56/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5479 - accuracy: 0.7557 - val_loss: 0.5494 - val_accuracy: 0.7663\n",
      "Epoch 57/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5471 - accuracy: 0.7545 - val_loss: 0.5370 - val_accuracy: 0.7684\n",
      "Epoch 58/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5374 - accuracy: 0.7603 - val_loss: 0.5371 - val_accuracy: 0.7724\n",
      "Epoch 59/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7606 - val_loss: 0.5382 - val_accuracy: 0.7745\n",
      "Epoch 60/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5369 - accuracy: 0.7534 - val_loss: 0.5302 - val_accuracy: 0.7724\n",
      "Epoch 61/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7583 - val_loss: 0.5359 - val_accuracy: 0.7806\n",
      "Epoch 62/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5344 - accuracy: 0.7555 - val_loss: 0.5243 - val_accuracy: 0.7796\n",
      "Epoch 63/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5321 - accuracy: 0.7588 - val_loss: 0.5257 - val_accuracy: 0.7857\n",
      "Epoch 64/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5345 - accuracy: 0.7660 - val_loss: 0.5239 - val_accuracy: 0.7786\n",
      "Epoch 65/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5195 - accuracy: 0.7695 - val_loss: 0.5247 - val_accuracy: 0.7806\n",
      "Epoch 66/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5220 - accuracy: 0.7667 - val_loss: 0.5341 - val_accuracy: 0.7837\n",
      "Epoch 67/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5255 - accuracy: 0.7675 - val_loss: 0.5200 - val_accuracy: 0.7878\n",
      "Epoch 68/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5286 - accuracy: 0.7654 - val_loss: 0.5229 - val_accuracy: 0.7837\n",
      "Epoch 69/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5201 - accuracy: 0.7726 - val_loss: 0.5190 - val_accuracy: 0.7816\n",
      "Epoch 70/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5230 - accuracy: 0.7591 - val_loss: 0.5225 - val_accuracy: 0.7929\n",
      "Epoch 71/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5357 - accuracy: 0.7688 - val_loss: 0.5228 - val_accuracy: 0.7765\n",
      "Epoch 72/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5237 - accuracy: 0.7690 - val_loss: 0.5258 - val_accuracy: 0.7878\n",
      "Epoch 73/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5240 - accuracy: 0.7657 - val_loss: 0.5199 - val_accuracy: 0.7827\n",
      "Epoch 74/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5245 - accuracy: 0.7695 - val_loss: 0.5161 - val_accuracy: 0.7837\n",
      "Epoch 75/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5230 - accuracy: 0.7614 - val_loss: 0.5125 - val_accuracy: 0.7857\n",
      "Epoch 76/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5204 - accuracy: 0.7586 - val_loss: 0.5160 - val_accuracy: 0.7908\n",
      "Epoch 77/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5144 - accuracy: 0.7703 - val_loss: 0.5230 - val_accuracy: 0.7867\n",
      "Epoch 78/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5246 - accuracy: 0.7703 - val_loss: 0.5137 - val_accuracy: 0.7888\n",
      "Epoch 79/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5170 - accuracy: 0.7624 - val_loss: 0.5204 - val_accuracy: 0.7908\n",
      "Epoch 80/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5123 - accuracy: 0.7705 - val_loss: 0.5140 - val_accuracy: 0.7908\n",
      "Epoch 81/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5142 - accuracy: 0.7705 - val_loss: 0.5098 - val_accuracy: 0.7878\n",
      "Epoch 82/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5094 - accuracy: 0.7723 - val_loss: 0.5140 - val_accuracy: 0.7929\n",
      "Epoch 83/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5081 - accuracy: 0.7774 - val_loss: 0.5077 - val_accuracy: 0.7908\n",
      "Epoch 84/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5109 - accuracy: 0.7688 - val_loss: 0.5151 - val_accuracy: 0.7959\n",
      "Epoch 85/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5180 - accuracy: 0.7734 - val_loss: 0.5073 - val_accuracy: 0.7929\n",
      "Epoch 86/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5140 - accuracy: 0.7690 - val_loss: 0.5153 - val_accuracy: 0.7857\n",
      "Epoch 87/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5213 - accuracy: 0.7703 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
      "Epoch 88/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5089 - accuracy: 0.7652 - val_loss: 0.5137 - val_accuracy: 0.7939\n",
      "Epoch 89/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5124 - accuracy: 0.7700 - val_loss: 0.5040 - val_accuracy: 0.7969\n",
      "Epoch 90/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5162 - accuracy: 0.7703 - val_loss: 0.5043 - val_accuracy: 0.7867\n",
      "Epoch 91/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5107 - accuracy: 0.7713 - val_loss: 0.5087 - val_accuracy: 0.7878\n",
      "Epoch 92/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5173 - accuracy: 0.7657 - val_loss: 0.5046 - val_accuracy: 0.7878\n",
      "Epoch 93/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5185 - accuracy: 0.7695 - val_loss: 0.5174 - val_accuracy: 0.7704\n",
      "Epoch 94/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5080 - accuracy: 0.7751 - val_loss: 0.5045 - val_accuracy: 0.7908\n",
      "Epoch 95/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5114 - accuracy: 0.7675 - val_loss: 0.5080 - val_accuracy: 0.7888\n",
      "Epoch 96/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5147 - accuracy: 0.7670 - val_loss: 0.5062 - val_accuracy: 0.7878\n",
      "Epoch 97/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5095 - accuracy: 0.7675 - val_loss: 0.5149 - val_accuracy: 0.7888\n",
      "Epoch 98/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5177 - accuracy: 0.7751 - val_loss: 0.5064 - val_accuracy: 0.7929\n",
      "Epoch 99/300\n",
      "119/123 [============================>.] - ETA: 0s - loss: 0.5120 - accuracy: 0.7718\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5116 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7908\n",
      "Epoch 100/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5054 - accuracy: 0.7741 - val_loss: 0.5076 - val_accuracy: 0.7939\n",
      "Epoch 101/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4965 - accuracy: 0.7751 - val_loss: 0.5058 - val_accuracy: 0.7949\n",
      "Epoch 102/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4898 - accuracy: 0.7810 - val_loss: 0.5045 - val_accuracy: 0.7969\n",
      "Epoch 103/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.7795 - val_loss: 0.5025 - val_accuracy: 0.7918\n",
      "Epoch 104/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4951 - accuracy: 0.7790 - val_loss: 0.5027 - val_accuracy: 0.7949\n",
      "Epoch 105/300\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.4935 - accuracy: 0.7731 - val_loss: 0.5017 - val_accuracy: 0.7929\n",
      "Epoch 106/300\n",
      " 37/123 [========>.....................] - ETA: 0s - loss: 0.4832 - accuracy: 0.7779"
     ]
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.1, patience=10,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-5)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics = [\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx_scaled], y = trainy.values,  \n",
    "                         validation_data = ([textx_scaled], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy\n",
    "#min_losses3[c].append(best_val_loss)\n",
    "#i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
