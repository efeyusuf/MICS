{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class Mics_Model:\n",
    "    def __init__(self, dataset_dir, use_encoder=True, sampling_method=\"Vanilla\", global_model=\"NN\", group_number = 2, company_number = 1):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.use_encoder = use_encoder\n",
    "        self.sampling_method = sampling_method\n",
    "        self.global_model = global_model\n",
    "        self.group_number = group_number\n",
    "        self.company_number = company_number\n",
    "        self.raw_data = None\n",
    "            \n",
    "    #ASSUMPTION: column 0: index, column 1: labels, remaining columns are features. \n",
    "    def get_raw_data(self, index_col=0):\n",
    "        raw_data = pd.read_csv(self.dataset_dir, index_col=0)\n",
    "        raw_data = raw_data.fillna(raw_data.mean())\n",
    "        raw_data = raw_data.sample(frac=1, random_state=41)\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    #This method assigns the feature number = column number - 1 (exclude label column). After that, it returns a list of\n",
    "    #input feature numbers according to group count. Ex: for 28 cols, 27 features, 4 group_num: returns [7,7,6,7] \n",
    "    #Output of this function can be fed to get_model methods as inp_sizes input.\n",
    "    def get_input_group_lenthgs(self):\n",
    "        count = self.group_number\n",
    "        input_sizes = [None]*count\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        for i in range(count):\n",
    "            group_size = round(feature_num/(count-i))\n",
    "            input_sizes[i] = group_size\n",
    "            feature_num = feature_num - group_size\n",
    "        return input_sizes\n",
    "    \n",
    "    #This method returns grouped column numbers\n",
    "    #[[1,4,5],[2,3,6]]\n",
    "    def get_grouped_feature_cols(self):\n",
    "        grouped_feature_cols = [None]*self.group_number\n",
    "        feature_num = len(self.raw_data.columns) - 1\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        total_nums = [i for i in range(feature_num)]\n",
    "        for j in range(len(inp_sizes)):\n",
    "            size = inp_sizes[j]\n",
    "            temp_list = random.sample(total_nums, size)\n",
    "            grouped_feature_cols[j] = temp_list\n",
    "            for k in temp_list:\n",
    "                total_nums.remove(k)\n",
    "        return grouped_feature_cols\n",
    "    \n",
    "    #groups is a list of lists [[1,4,5], [2,3,6]] which is output of get_grouped_feature_cols method\n",
    "    #returns: [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #          [test_x1, test_x2..., test_xn, test_y]]\n",
    "    def get_features_and_labels(self, groups, random_seed=41):\n",
    "        row_num = len(self.raw_data.index)\n",
    "        \n",
    "        trainx_df = self.raw_data.iloc[:int(0.7*row_num), 1:]\n",
    "        trainy_df = self.raw_data.iloc[:int(0.7*row_num), 0]\n",
    "        valx_df = self.raw_data.iloc[int(0.7*row_num):int(0.85*row_num), 1:]\n",
    "        valy_df = self.raw_data.iloc[int(0.7*row_num):int(0.85*row_num), 0] \n",
    "        testx_df = self.raw_data.iloc[int(0.85*row_num):, 1:]\n",
    "        testy_df = self.raw_data.iloc[int(0.85*row_num):, 0]         \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        #trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx_df), columns = trainx_df.columns, index = trainx_df.index)\n",
    "        #textx_scaled = pd.DataFrame(scaler.transform(testx_df), columns = testx_df.columns, index = testx_df.index)\n",
    "        \n",
    "        features_and_labels = [[None for _ in range(self.group_number + 1)] for _ in range(3)]\n",
    "        \n",
    "        for index, group in enumerate(groups):\n",
    "            train_temp = trainx_df.iloc[:,group]\n",
    "            train_temp_companies = self.transform_dataset(df=train_temp, random_seed=random_seed)\n",
    "            features_and_labels[0][index] = train_temp_companies.values\n",
    "            val_temp = valx_df.iloc[:,group]\n",
    "            val_temp_companies = self.transform_dataset(df=val_temp, random_seed=random_seed)\n",
    "            features_and_labels[1][index] = val_temp_companies.values            \n",
    "            test_temp = testx_df.iloc[:,group]\n",
    "            test_temp_companies = self.transform_dataset(df=test_temp, random_seed=random_seed)            \n",
    "            features_and_labels[2][index] = test_temp_companies.values\n",
    "        #trainy_df_companies = trainy_df.sample(frac=1, random_state=random_seed)\n",
    "        #testy_df_companies = testy_df.sample(frac=1, random_state=random_seed)        \n",
    "        features_and_labels[0][self.group_number] = trainy_df.values\n",
    "        features_and_labels[1][self.group_number] = valy_df.values   \n",
    "        features_and_labels[2][self.group_number] = testy_df.values   \n",
    "        \n",
    "        return features_and_labels\n",
    "    #returns [[train_x1, train_x2..., train_xn, train_y],\n",
    "    #         [test_x1, test_x2..., test_xn, test_y]]\n",
    "    \n",
    "    \n",
    "    #For a 10k row telecommunication data, it splits data into #company_num row groups and transforms them independently, adds one-hot encoding.\n",
    "    #There is no label column here, all columns are features.\n",
    "    def transform_dataset(self, df, random_seed=41):\n",
    "        company_num = self.company_number\n",
    "        col_num = len(df.columns)\n",
    "        row_num = len(df.index)\n",
    "        dfs = [None]*company_num\n",
    "        dfs_features = [None]*company_num\n",
    "        dfs_scaleds = [None]*company_num\n",
    "        dfs_new = [None]*company_num\n",
    "        scaler = StandardScaler()\n",
    "        for i in range(company_num):\n",
    "            dfs[i] = df.iloc[int(i/company_num*row_num):int((i+1)/company_num*row_num), :]\n",
    "            dfs[i] = dfs[i].sample(frac=1, axis=1, random_state=random_seed)\n",
    "            df_features_scaled_temp = pd.DataFrame(scaler.fit_transform(dfs[i]), columns = dfs[i].columns, index = dfs[i].index)\n",
    "            dfs_new[i] = df_features_scaled_temp\n",
    "            dfs_new[i]['group'] = i\n",
    "            cols_num = len(dfs_new[i].columns)\n",
    "            col_names = [j for j in range(cols_num)]\n",
    "            dfs_new[i].columns = col_names\n",
    "        df_final = pd.concat(dfs_new, axis=0)\n",
    "        last_col_num = cols_num - 1\n",
    "        df_new = df_final.rename(columns={last_col_num: 'group'})\n",
    "        df_final_onehot = pd.concat([df_new.iloc[:,:-1], pd.get_dummies(df_new.group, prefix='group')], axis=1)\n",
    "        #df_final_onehot = df_final_onehot.sample(frac=1, random_state=random_seed)\n",
    "        return df_final_onehot\n",
    "    \n",
    "    def get_vanilla_encoder_model(self, inp_size):\n",
    "        inputs = keras.layers.Input(shape=(inp_size+self.company_number))\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        h1 = keras.layers.Dense(10, activation=\"relu\")(inputs)        \n",
    "        outputs = keras.layers.Dense(inp_size, activation=\"relu\")(h1)\n",
    "        return keras.Model(inputs,outputs)\n",
    "    \n",
    "    #This subclass is created for sampling for a given mean and log_variance.\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon #multiplies with std\n",
    "    \n",
    "    def get_variatonal_encoder_model(self, inp_size):\n",
    "        inputs = layers.Input(shape=(inp_size+self.company_number))\n",
    "        h1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "        z_mean = layers.Dense(inp_size, name=\"z_mean\")(h1)\n",
    "        z_log_var = layers.Dense(inp_size, name=\"z_log_var\")(h1)\n",
    "        outputs = self.Sampling()([z_mean, z_log_var])\n",
    "        return keras.Model(inputs,outputs)\n",
    "    #New sampling methods can be added here \n",
    "    \n",
    "    def get_nn_model(self, inp_sizes, drop_out=0.25, hidden_num = 4, hidden_size=32, activation=\"relu\"):\n",
    "        inp_group_count = len(inp_sizes)\n",
    "        inputs = [None]*inp_group_count\n",
    "        for i in range(inp_group_count):\n",
    "            inputs[i] = keras.layers.Input(shape=(inp_sizes[i]+self.company_number), name=\"input_\"+str(i))\n",
    "        if self.use_encoder == True:\n",
    "            encoders = [None]*inp_group_count\n",
    "            if self.sampling_method == \"Vanilla\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_vanilla_encoder_model(inp_sizes[j])\n",
    "            elif self.sampling_method == \"Variational\":\n",
    "                for j in range(inp_group_count):\n",
    "                    encoders[j] = self.get_variatonal_encoder_model(inp_sizes[j])\n",
    "            #This place can be extended if new sampling methods are added.\n",
    "            global_inputs = [None]*inp_group_count\n",
    "            for k in range(inp_group_count):\n",
    "                global_inputs[k] = encoders[k](inputs[k])\n",
    "            global_input = keras.layers.concatenate(global_inputs)\n",
    "        else:\n",
    "            global_input = keras.layers.concatenate(inputs)\n",
    "            \n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(global_input)\n",
    "        h = keras.layers.Dropout(drop_out)(h)\n",
    "        for hidden in range(hidden_num):\n",
    "            h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "            h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation=activation)(h)    \n",
    "        return keras.Model(inputs=inputs, outputs = outputs) \n",
    "    \n",
    "    def default_exp(self, batch_size = 300):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes)\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6)]\n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.MeanSquaredError())\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        training_val_loss = history.history[\"val_loss\"]\n",
    "        best_row_index = np.argmin(training_val_loss)\n",
    "        best_val_loss = training_val_loss[best_row_index]\n",
    "        print(best_val_loss)\n",
    "        \n",
    "    def default_exp_house(self, batch_size = 32):\n",
    "        inp_sizes = self.get_input_group_lenthgs()\n",
    "        groups = self.get_grouped_feature_cols()\n",
    "        features_and_labels = self.get_features_and_labels(groups)\n",
    "        MICS_model = self.get_nn_model(inp_sizes=inp_sizes, activation=\"sigmoid\")\n",
    "        checkpoint_filepath = 'tmp/checkpoint'\n",
    "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "                keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                                 verbose = 2, mode = \"auto\", \n",
    "                                                  min_lr = 1e-6),\n",
    "                keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)]  \n",
    "        \n",
    "        MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "        history = MICS_model.fit(x = features_and_labels[0][:-1], y = features_and_labels[0][-1],  \n",
    "                                 validation_data = (features_and_labels[1][:-1], features_and_labels[1][-1]),\n",
    "                                 epochs=300, batch_size = batch_size, callbacks=callback)\n",
    "        \n",
    "        \n",
    "        training_acc = history.history[\"val_accuracy\"]\n",
    "        MICS_model.load_weights(checkpoint_filepath)\n",
    "        result = MICS_model.evaluate(x = features_and_labels[2][:-1], y = features_and_labels[2][-1])[1]\n",
    "        \n",
    "        print(\"best test accuracy is: \" + str(result))\n",
    "        return result\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./Datasets/energydata_use.csv\"\n",
    "deneyelim = Mics_Model(dataset_dir, use_encoder=True, group_number=2, company_number=1)\n",
    "deneyelim.get_raw_data()\n",
    "#deneyelim.raw_data\n",
    "deneyelim.default_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=10, company_number=50)\n",
    "deneyelim2.get_raw_data()\n",
    "a = deneyelim2.default_exp_house(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_numbers = [2,4,6,8,10]\n",
    "company_numbers = [1,2,4,7,10,20,40]\n",
    "\n",
    "result_matrix = [[[] for _ in range(len(company_numbers))] for _ in range(len(group_numbers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir2 = \"./Datasets/houseprices_ready.csv\"\n",
    "for group_index in range(len(group_numbers)):\n",
    "    for company_index in range(len(company_numbers)):\n",
    "        for loop in range(5):\n",
    "            deneyelim2 = Mics_Model(dataset_dir2, use_encoder=True, group_number=group_numbers[group_index], company_number=company_numbers[company_index])\n",
    "            deneyelim2.get_raw_data()\n",
    "            score = deneyelim2.default_exp_house(batch_size=32)\n",
    "            result_matrix[group_index][company_index].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "result_matrix_copy = copy.deepcopy(result_matrix)\n",
    "result_matrix_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_std_matrix = [[[] for _ in range(len(company_numbers))] for _ in range(len(group_numbers))]\n",
    "for group_index in range(len(group_numbers)):\n",
    "    for company_index in range(len(company_numbers)):\n",
    "        current_list = result_matrix_copy[group_index][company_index]\n",
    "        avg_std_matrix[group_index][company_index].append(statistics.mean(current_list)) \n",
    "        avg_std_matrix[group_index][company_index].append(statistics.stdev(current_list)) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.901369845867157, 0.008901180200105482]</td>\n",
       "      <td>[0.9013698697090149, 0.01000404993961545]</td>\n",
       "      <td>[0.8968036532402038, 0.002501039350504474]</td>\n",
       "      <td>[0.8904109597206116, 0.012081049554110651]</td>\n",
       "      <td>[0.8757990837097168, 0.011819354129907245]</td>\n",
       "      <td>[0.8657534122467041, 0.024926657367662853]</td>\n",
       "      <td>[0.8347031950950623, 0.010412549991255569]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.8995433688163758, 0.01070870269931029]</td>\n",
       "      <td>[0.9095890402793885, 0.006772776253837231]</td>\n",
       "      <td>[0.8949771761894226, 0.018826970403172494]</td>\n",
       "      <td>[0.8812785387039185, 0.009132415056238363]</td>\n",
       "      <td>[0.8876712322235107, 0.014654610225575277]</td>\n",
       "      <td>[0.8757990837097168, 0.01780236880875643]</td>\n",
       "      <td>[0.8365296721458435, 0.016901132849862725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.8986301422119141, 0.012670721870541872]</td>\n",
       "      <td>[0.9059360742568969, 0.016014328872232364]</td>\n",
       "      <td>[0.8986301302909852, 0.014221375875773275]</td>\n",
       "      <td>[0.877625572681427, 0.010901684161528903]</td>\n",
       "      <td>[0.8721461176872254, 0.018826959560969136]</td>\n",
       "      <td>[0.8602739691734314, 0.01190721358284922]</td>\n",
       "      <td>[0.8182648420333862, 0.01041255129817017]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.9132420063018799, 0.0055924453351836115]</td>\n",
       "      <td>[0.8885844826698304, 0.01190721358284922]</td>\n",
       "      <td>[0.8794520616531372, 0.011907223868657016]</td>\n",
       "      <td>[0.8931506752967835, 0.006925013290902036]</td>\n",
       "      <td>[0.8721461296081543, 0.028148012965071873]</td>\n",
       "      <td>[0.8648401856422424, 0.01665258945231131]</td>\n",
       "      <td>[0.8228310346603394, 0.01690114895334232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.8995433807373047, 0.007219812741103662]</td>\n",
       "      <td>[0.8995433807373047, 0.007219812741103662]</td>\n",
       "      <td>[0.8867579817771911, 0.015618848947038588]</td>\n",
       "      <td>[0.8767123222351074, 0.021172657396003998]</td>\n",
       "      <td>[0.8803653120994568, 0.052202777648679415]</td>\n",
       "      <td>[0.8584474921226501, 0.01677731978310403]</td>\n",
       "      <td>[0.8191780805587768, 0.013545567577167085]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             1   \\\n",
       "2     [0.901369845867157, 0.008901180200105482]   \n",
       "4     [0.8995433688163758, 0.01070870269931029]   \n",
       "6    [0.8986301422119141, 0.012670721870541872]   \n",
       "8   [0.9132420063018799, 0.0055924453351836115]   \n",
       "10   [0.8995433807373047, 0.007219812741103662]   \n",
       "\n",
       "                                            2   \\\n",
       "2    [0.9013698697090149, 0.01000404993961545]   \n",
       "4   [0.9095890402793885, 0.006772776253837231]   \n",
       "6   [0.9059360742568969, 0.016014328872232364]   \n",
       "8    [0.8885844826698304, 0.01190721358284922]   \n",
       "10  [0.8995433807373047, 0.007219812741103662]   \n",
       "\n",
       "                                            4   \\\n",
       "2   [0.8968036532402038, 0.002501039350504474]   \n",
       "4   [0.8949771761894226, 0.018826970403172494]   \n",
       "6   [0.8986301302909852, 0.014221375875773275]   \n",
       "8   [0.8794520616531372, 0.011907223868657016]   \n",
       "10  [0.8867579817771911, 0.015618848947038588]   \n",
       "\n",
       "                                            7   \\\n",
       "2   [0.8904109597206116, 0.012081049554110651]   \n",
       "4   [0.8812785387039185, 0.009132415056238363]   \n",
       "6    [0.877625572681427, 0.010901684161528903]   \n",
       "8   [0.8931506752967835, 0.006925013290902036]   \n",
       "10  [0.8767123222351074, 0.021172657396003998]   \n",
       "\n",
       "                                            10  \\\n",
       "2   [0.8757990837097168, 0.011819354129907245]   \n",
       "4   [0.8876712322235107, 0.014654610225575277]   \n",
       "6   [0.8721461176872254, 0.018826959560969136]   \n",
       "8   [0.8721461296081543, 0.028148012965071873]   \n",
       "10  [0.8803653120994568, 0.052202777648679415]   \n",
       "\n",
       "                                            20  \\\n",
       "2   [0.8657534122467041, 0.024926657367662853]   \n",
       "4    [0.8757990837097168, 0.01780236880875643]   \n",
       "6    [0.8602739691734314, 0.01190721358284922]   \n",
       "8    [0.8648401856422424, 0.01665258945231131]   \n",
       "10   [0.8584474921226501, 0.01677731978310403]   \n",
       "\n",
       "                                            40  \n",
       "2   [0.8347031950950623, 0.010412549991255569]  \n",
       "4   [0.8365296721458435, 0.016901132849862725]  \n",
       "6    [0.8182648420333862, 0.01041255129817017]  \n",
       "8    [0.8228310346603394, 0.01690114895334232]  \n",
       "10  [0.8191780805587768, 0.013545567577167085]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(avg_std_matrix, columns=company_numbers, index=group_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
