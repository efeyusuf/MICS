{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import tensorflow.keras.utils as utils\n",
    "import pydot\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Datasets/houseprices_ready.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AboveMedianPrice</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>14115</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>796</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>10084</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1686</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>10382</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7420</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AboveMedianPrice  LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  \\\n",
       "0                 1     8450            7            5          856         2   \n",
       "1                 1     9600            6            8         1262         2   \n",
       "2                 1    11250            7            5          920         2   \n",
       "3                 0     9550            7            5          756         1   \n",
       "4                 1    14260            8            5         1145         2   \n",
       "5                 0    14115            5            5          796         1   \n",
       "6                 1    10084            8            5         1686         2   \n",
       "7                 1    10382            7            6         1107         2   \n",
       "8                 0     6120            7            5          952         2   \n",
       "9                 0     7420            5            6          991         1   \n",
       "\n",
       "   HalfBath  BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "0         1             3             8           0         548  \n",
       "1         0             3             6           1         460  \n",
       "2         1             3             6           1         608  \n",
       "3         0             3             7           1         642  \n",
       "4         1             4             9           1         836  \n",
       "5         1             1             5           0         480  \n",
       "6         0             3             7           1         636  \n",
       "7         1             3             7           2         484  \n",
       "8         0             2             8           2         468  \n",
       "9         0             2             5           2         205  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_dir, index_col=0)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns is: 11 and number of rows is: 1460\n"
     ]
    }
   ],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)\n",
    "print(\"Number of columns is: {} and number of rows is: {}\".format(col_num, row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335     1\n",
       "1074    1\n",
       "933     1\n",
       "982     0\n",
       "921     0\n",
       "       ..\n",
       "1018    0\n",
       "1016    1\n",
       "307     0\n",
       "1450    0\n",
       "17      0\n",
       "Name: AboveMedianPrice, Length: 1168, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "textx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>14.391355</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>0.354888</td>\n",
       "      <td>1.012200</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>2.126222</td>\n",
       "      <td>0.249948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>-0.184336</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.427102</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.943654</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>1.638311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>-0.190779</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.014459</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.441608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-0.686115</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.485838</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>-0.163701</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>0.499392</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>1.496164</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-1.506656</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>1.215994</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.343729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0.126311</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.023495</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>0.011542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.243721</td>\n",
       "      <td>-0.082326</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>-1.653495</td>\n",
       "      <td>-1.047785</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.142879</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-0.350016</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.024350</td>\n",
       "      <td>-1.539237</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-2.374136</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.189178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
       "335   14.391355    -0.810781     0.354888     1.012200  0.770659 -0.756619   \n",
       "1074  -0.184336     0.646130    -0.536529     0.427102  0.770659 -0.756619   \n",
       "933   -0.190779     0.646130    -0.536529     1.014459  0.770659 -0.756619   \n",
       "982   -0.686115     0.646130    -0.536529     0.485838  0.770659 -0.756619   \n",
       "921   -0.163701    -0.810781     1.246305     0.499392  0.770659  3.188607   \n",
       "...         ...          ...          ...          ...       ...       ...   \n",
       "1018   0.023696     0.646130    -0.536529    -1.506656  0.770659  1.215994   \n",
       "1016   0.126311     0.646130    -0.536529     1.023495  0.770659 -0.756619   \n",
       "307   -0.243721    -0.082326     1.246305    -1.653495 -1.047785 -0.756619   \n",
       "1450  -0.142879    -0.810781    -0.536529    -0.350016  0.770659  3.188607   \n",
       "17     0.024350    -1.539237    -0.536529    -2.374136  0.770659 -0.756619   \n",
       "\n",
       "      BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "335       0.157079      0.276255    2.126222    0.249948  \n",
       "1074     -1.066042     -0.943654   -0.970321    1.638311  \n",
       "933       0.157079     -0.333699   -0.970321    0.441608  \n",
       "982      -1.066042     -0.333699    0.577951   -0.409173  \n",
       "921       1.380199      1.496164   -0.970321   -2.222927  \n",
       "...            ...           ...         ...         ...  \n",
       "1018      0.157079      0.276255    0.577951   -0.343729  \n",
       "1016      0.157079     -0.333699    0.577951    0.011542  \n",
       "307       0.157079     -0.333699   -0.970321   -2.222927  \n",
       "1450      1.380199      0.886209   -0.970321   -2.222927  \n",
       "17       -1.066042     -0.333699   -0.970321    0.189178  \n",
       "\n",
       "[1168 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_rand_list(max_val, count):\n",
    "    randomlist = random.sample(range(0, max_val + 1), count)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "        \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>14.391355</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>0.354888</td>\n",
       "      <td>1.012200</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>2.126222</td>\n",
       "      <td>0.249948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>-0.184336</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.427102</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.943654</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>1.638311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>-0.190779</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.014459</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.441608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-0.686115</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.485838</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>-0.163701</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>0.499392</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>1.496164</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-1.506656</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>1.215994</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.343729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0.126311</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.023495</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>0.011542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.243721</td>\n",
       "      <td>-0.082326</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>-1.653495</td>\n",
       "      <td>-1.047785</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.142879</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-0.350016</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.024350</td>\n",
       "      <td>-1.539237</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-2.374136</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.189178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
       "335   14.391355    -0.810781     0.354888     1.012200  0.770659 -0.756619   \n",
       "1074  -0.184336     0.646130    -0.536529     0.427102  0.770659 -0.756619   \n",
       "933   -0.190779     0.646130    -0.536529     1.014459  0.770659 -0.756619   \n",
       "982   -0.686115     0.646130    -0.536529     0.485838  0.770659 -0.756619   \n",
       "921   -0.163701    -0.810781     1.246305     0.499392  0.770659  3.188607   \n",
       "...         ...          ...          ...          ...       ...       ...   \n",
       "1018   0.023696     0.646130    -0.536529    -1.506656  0.770659  1.215994   \n",
       "1016   0.126311     0.646130    -0.536529     1.023495  0.770659 -0.756619   \n",
       "307   -0.243721    -0.082326     1.246305    -1.653495 -1.047785 -0.756619   \n",
       "1450  -0.142879    -0.810781    -0.536529    -0.350016  0.770659  3.188607   \n",
       "17     0.024350    -1.539237    -0.536529    -2.374136  0.770659 -0.756619   \n",
       "\n",
       "      BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "335       0.157079      0.276255    2.126222    0.249948  \n",
       "1074     -1.066042     -0.943654   -0.970321    1.638311  \n",
       "933       0.157079     -0.333699   -0.970321    0.441608  \n",
       "982      -1.066042     -0.333699    0.577951   -0.409173  \n",
       "921       1.380199      1.496164   -0.970321   -2.222927  \n",
       "...            ...           ...         ...         ...  \n",
       "1018      0.157079      0.276255    0.577951   -0.343729  \n",
       "1016      0.157079     -0.333699    0.577951    0.011542  \n",
       "307       0.157079     -0.333699   -0.970321   -2.222927  \n",
       "1450      1.380199      0.886209   -0.970321   -2.222927  \n",
       "17       -1.066042     -0.333699   -0.970321    0.189178  \n",
       "\n",
       "[1168 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x_rand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d758e9971430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minp_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_rand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x_rand' is not defined"
     ]
    }
   ],
   "source": [
    "inp_size = len(test_x_rand.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_losses = {12:[], 14:[], 16:[], 18:[], 20:[], 22:[], 24:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.6307 - accuracy: 0.7748 - val_loss: 0.4052 - val_accuracy: 0.8938\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4487 - accuracy: 0.8904 - val_loss: 0.3264 - val_accuracy: 0.9007\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8973 - val_loss: 0.3258 - val_accuracy: 0.9007\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8964 - val_loss: 0.3451 - val_accuracy: 0.8973\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.9033 - val_loss: 0.2986 - val_accuracy: 0.9041\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8998 - val_loss: 0.2894 - val_accuracy: 0.8973\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.9033 - val_loss: 0.2980 - val_accuracy: 0.9007\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.9050 - val_loss: 0.2974 - val_accuracy: 0.8973\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.9101 - val_loss: 0.2995 - val_accuracy: 0.9075\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8990 - val_loss: 0.2878 - val_accuracy: 0.9144\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.9041 - val_loss: 0.2942 - val_accuracy: 0.8870\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.9084 - val_loss: 0.2921 - val_accuracy: 0.9110\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8981 - val_loss: 0.2926 - val_accuracy: 0.9075\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.9084 - val_loss: 0.2879 - val_accuracy: 0.9075\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.9084 - val_loss: 0.3004 - val_accuracy: 0.8973\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.9092 - val_loss: 0.3047 - val_accuracy: 0.9110\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.9092 - val_loss: 0.2966 - val_accuracy: 0.8904\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3089 - accuracy: 0.9092 - val_loss: 0.3100 - val_accuracy: 0.9178\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.9050 - val_loss: 0.2966 - val_accuracy: 0.9178\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.9075 - val_loss: 0.2940 - val_accuracy: 0.9075\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.9084 - val_loss: 0.2905 - val_accuracy: 0.9110\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.9075 - val_loss: 0.3064 - val_accuracy: 0.9212\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.9127 - val_loss: 0.2982 - val_accuracy: 0.9075\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.9075 - val_loss: 0.3275 - val_accuracy: 0.9075\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.9075 - val_loss: 0.3061 - val_accuracy: 0.9007\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.9075 - val_loss: 0.3126 - val_accuracy: 0.9110\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3044 - accuracy: 0.9110 - val_loss: 0.2885 - val_accuracy: 0.9041\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.9067 - val_loss: 0.3176 - val_accuracy: 0.9041\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.9033 - val_loss: 0.2995 - val_accuracy: 0.9007\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.9187 - val_loss: 0.2943 - val_accuracy: 0.8904\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.9127 - val_loss: 0.3186 - val_accuracy: 0.9110\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2937 - accuracy: 0.9067 - val_loss: 0.3273 - val_accuracy: 0.9075\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2856 - accuracy: 0.9135 - val_loss: 0.2964 - val_accuracy: 0.9178\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3104 - accuracy: 0.9110 - val_loss: 0.3465 - val_accuracy: 0.8836\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8921 - val_loss: 0.2905 - val_accuracy: 0.9144\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.9084 - val_loss: 0.2943 - val_accuracy: 0.9144\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.9092 - val_loss: 0.3189 - val_accuracy: 0.9144\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.9178 - val_loss: 0.3078 - val_accuracy: 0.9178\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9101 - val_loss: 0.2981 - val_accuracy: 0.9110\n",
      "Epoch 40/300\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1926 - accuracy: 0.9688\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.9067 - val_loss: 0.2939 - val_accuracy: 0.9110\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.9152 - val_loss: 0.2960 - val_accuracy: 0.9144\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.9152 - val_loss: 0.3114 - val_accuracy: 0.9110\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9178 - val_loss: 0.2880 - val_accuracy: 0.9178\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.9092 - val_loss: 0.3019 - val_accuracy: 0.9075\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.9212 - val_loss: 0.3000 - val_accuracy: 0.9110\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.9229 - val_loss: 0.2916 - val_accuracy: 0.9075\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.9212 - val_loss: 0.2916 - val_accuracy: 0.9110\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.9229 - val_loss: 0.2966 - val_accuracy: 0.9144\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.9170 - val_loss: 0.2828 - val_accuracy: 0.9212\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2820 - accuracy: 0.9187 - val_loss: 0.2895 - val_accuracy: 0.9075\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2888 - accuracy: 0.9161 - val_loss: 0.2966 - val_accuracy: 0.9110\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.9127 - val_loss: 0.3019 - val_accuracy: 0.9075\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9101 - val_loss: 0.3084 - val_accuracy: 0.8973\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.9178 - val_loss: 0.3136 - val_accuracy: 0.9110\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.9067 - val_loss: 0.3174 - val_accuracy: 0.9007\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.9127 - val_loss: 0.3179 - val_accuracy: 0.9007\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.9135 - val_loss: 0.2924 - val_accuracy: 0.9144\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.9195 - val_loss: 0.2804 - val_accuracy: 0.9110\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.9204 - val_loss: 0.2770 - val_accuracy: 0.9178\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.9144 - val_loss: 0.2857 - val_accuracy: 0.9110\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2914 - accuracy: 0.9161 - val_loss: 0.2833 - val_accuracy: 0.9075\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.9161 - val_loss: 0.2858 - val_accuracy: 0.9212\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.9212 - val_loss: 0.3129 - val_accuracy: 0.9144\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2892 - accuracy: 0.9024 - val_loss: 0.2986 - val_accuracy: 0.9178\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2796 - accuracy: 0.9118 - val_loss: 0.2942 - val_accuracy: 0.9212\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.9135 - val_loss: 0.2923 - val_accuracy: 0.9144\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.9178 - val_loss: 0.2912 - val_accuracy: 0.9075\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.9127 - val_loss: 0.2843 - val_accuracy: 0.9212\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2661 - accuracy: 0.9195 - val_loss: 0.3015 - val_accuracy: 0.9212\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9152 - val_loss: 0.2881 - val_accuracy: 0.9144\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.9204 - val_loss: 0.3108 - val_accuracy: 0.9075\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.9221 - val_loss: 0.2972 - val_accuracy: 0.9007\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.9127 - val_loss: 0.2965 - val_accuracy: 0.9178\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.9135 - val_loss: 0.2883 - val_accuracy: 0.9144\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.9170 - val_loss: 0.2896 - val_accuracy: 0.9178\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.9315 - val_loss: 0.3134 - val_accuracy: 0.9075\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.9187 - val_loss: 0.2906 - val_accuracy: 0.9144\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2767 - accuracy: 0.9195 - val_loss: 0.2893 - val_accuracy: 0.9178\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2646 - accuracy: 0.9152 - val_loss: 0.3082 - val_accuracy: 0.9110\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2808 - accuracy: 0.9161 - val_loss: 0.2861 - val_accuracy: 0.9144\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2706 - accuracy: 0.9212 - val_loss: 0.2950 - val_accuracy: 0.9041\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.9135 - val_loss: 0.2869 - val_accuracy: 0.9110\n",
      "Epoch 83/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.9110 - val_loss: 0.2749 - val_accuracy: 0.9212\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.9170 - val_loss: 0.2900 - val_accuracy: 0.9110\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.9058 - val_loss: 0.2975 - val_accuracy: 0.9212\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2848 - accuracy: 0.9170 - val_loss: 0.2866 - val_accuracy: 0.9212\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.9247 - val_loss: 0.2981 - val_accuracy: 0.9110\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2743 - accuracy: 0.9204 - val_loss: 0.3028 - val_accuracy: 0.9007\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.9221 - val_loss: 0.2735 - val_accuracy: 0.9110\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.9170 - val_loss: 0.2755 - val_accuracy: 0.9110\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.9110 - val_loss: 0.2966 - val_accuracy: 0.9144\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2694 - accuracy: 0.9101 - val_loss: 0.2967 - val_accuracy: 0.9212\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.9204 - val_loss: 0.3139 - val_accuracy: 0.9041\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2761 - accuracy: 0.9238 - val_loss: 0.2842 - val_accuracy: 0.9007\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2709 - accuracy: 0.9221 - val_loss: 0.2997 - val_accuracy: 0.9110\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.9152 - val_loss: 0.2858 - val_accuracy: 0.8973\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.9178 - val_loss: 0.2781 - val_accuracy: 0.9144\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2862 - accuracy: 0.9033 - val_loss: 0.3009 - val_accuracy: 0.9212\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9255 - val_loss: 0.2881 - val_accuracy: 0.9178\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.9152 - val_loss: 0.2935 - val_accuracy: 0.9075\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.9050 - val_loss: 0.3010 - val_accuracy: 0.9144\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.9152 - val_loss: 0.2853 - val_accuracy: 0.9212\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2801 - accuracy: 0.9127 - val_loss: 0.2937 - val_accuracy: 0.9144\n",
      "Epoch 104/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.8990 - val_loss: 0.3099 - val_accuracy: 0.9110\n",
      "Epoch 105/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2880 - accuracy: 0.9075 - val_loss: 0.3069 - val_accuracy: 0.9144\n",
      "Epoch 106/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2658 - accuracy: 0.9307 - val_loss: 0.2962 - val_accuracy: 0.9075\n",
      "Epoch 107/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.9178 - val_loss: 0.2768 - val_accuracy: 0.9178\n",
      "Epoch 108/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.9195 - val_loss: 0.2939 - val_accuracy: 0.9110\n",
      "Epoch 109/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2945 - accuracy: 0.9041 - val_loss: 0.2939 - val_accuracy: 0.9144\n",
      "Epoch 110/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.9255 - val_loss: 0.2855 - val_accuracy: 0.9212\n",
      "Epoch 111/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9161 - val_loss: 0.3144 - val_accuracy: 0.9041\n",
      "Epoch 112/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.9135 - val_loss: 0.2781 - val_accuracy: 0.9110\n",
      "Epoch 113/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.9212 - val_loss: 0.2747 - val_accuracy: 0.9110\n",
      "Epoch 114/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.9024 - val_loss: 0.2901 - val_accuracy: 0.9178\n",
      "Epoch 115/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2771 - accuracy: 0.9170 - val_loss: 0.2931 - val_accuracy: 0.9144\n",
      "Epoch 116/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.9110 - val_loss: 0.2948 - val_accuracy: 0.9247\n",
      "Epoch 117/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2651 - accuracy: 0.9075 - val_loss: 0.2905 - val_accuracy: 0.9178\n",
      "Epoch 118/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2717 - accuracy: 0.9195 - val_loss: 0.3087 - val_accuracy: 0.9041\n",
      "Epoch 119/300\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2714 - accuracy: 0.9204\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.9195 - val_loss: 0.2958 - val_accuracy: 0.9041\n",
      "Epoch 120/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2894 - accuracy: 0.9101 - val_loss: 0.2896 - val_accuracy: 0.9178\n",
      "Epoch 121/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.9238 - val_loss: 0.3045 - val_accuracy: 0.9144\n",
      "Epoch 122/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.9289 - val_loss: 0.2751 - val_accuracy: 0.9212\n",
      "Epoch 123/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2723 - accuracy: 0.9178 - val_loss: 0.2832 - val_accuracy: 0.9144\n",
      "Epoch 124/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9229 - val_loss: 0.2773 - val_accuracy: 0.9178\n",
      "Epoch 125/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2688 - accuracy: 0.9118 - val_loss: 0.2942 - val_accuracy: 0.9144\n",
      "Epoch 126/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.9238 - val_loss: 0.2841 - val_accuracy: 0.9144\n",
      "Epoch 127/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9264 - val_loss: 0.2856 - val_accuracy: 0.9110\n",
      "Epoch 128/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.9221 - val_loss: 0.3084 - val_accuracy: 0.9212\n",
      "Epoch 129/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2810 - accuracy: 0.9127 - val_loss: 0.2844 - val_accuracy: 0.9144\n",
      "Epoch 130/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.9161 - val_loss: 0.2758 - val_accuracy: 0.9144\n",
      "Epoch 131/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.9221 - val_loss: 0.2958 - val_accuracy: 0.9075\n",
      "Epoch 132/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9135 - val_loss: 0.2782 - val_accuracy: 0.9178\n",
      "Epoch 133/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.9135 - val_loss: 0.2744 - val_accuracy: 0.9144\n",
      "Epoch 134/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.9204 - val_loss: 0.2930 - val_accuracy: 0.9178\n",
      "Epoch 135/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.9187 - val_loss: 0.2818 - val_accuracy: 0.9144\n",
      "Epoch 136/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.9264 - val_loss: 0.2773 - val_accuracy: 0.9178\n",
      "Epoch 137/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2622 - accuracy: 0.9255 - val_loss: 0.2910 - val_accuracy: 0.9041\n",
      "Epoch 138/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.9178 - val_loss: 0.2692 - val_accuracy: 0.9144\n",
      "Epoch 139/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.9272 - val_loss: 0.2790 - val_accuracy: 0.9178\n",
      "Epoch 140/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.9170 - val_loss: 0.2831 - val_accuracy: 0.9212\n",
      "Epoch 141/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.9238 - val_loss: 0.2805 - val_accuracy: 0.9110\n",
      "Epoch 142/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.9161 - val_loss: 0.2736 - val_accuracy: 0.9144\n",
      "Epoch 143/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.9187 - val_loss: 0.2738 - val_accuracy: 0.9212\n",
      "Epoch 144/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9221 - val_loss: 0.2697 - val_accuracy: 0.9178\n",
      "Epoch 145/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9332 - val_loss: 0.2969 - val_accuracy: 0.9178\n",
      "Epoch 146/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.9187 - val_loss: 0.2814 - val_accuracy: 0.9110\n",
      "Epoch 147/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9349 - val_loss: 0.2996 - val_accuracy: 0.9212\n",
      "Epoch 148/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9221 - val_loss: 0.3342 - val_accuracy: 0.8938\n",
      "Epoch 149/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.9204 - val_loss: 0.2784 - val_accuracy: 0.9178\n",
      "Epoch 150/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2465 - accuracy: 0.9349 - val_loss: 0.3016 - val_accuracy: 0.9144\n",
      "Epoch 151/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2506 - accuracy: 0.9349 - val_loss: 0.2709 - val_accuracy: 0.9178\n",
      "Epoch 152/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9307 - val_loss: 0.2894 - val_accuracy: 0.9144\n",
      "Epoch 153/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2408 - accuracy: 0.9264 - val_loss: 0.3332 - val_accuracy: 0.8938\n",
      "Epoch 154/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2671 - accuracy: 0.9238 - val_loss: 0.2922 - val_accuracy: 0.9075\n",
      "Epoch 155/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.9298 - val_loss: 0.2876 - val_accuracy: 0.9212\n",
      "Epoch 156/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.9195 - val_loss: 0.2853 - val_accuracy: 0.9144\n",
      "Epoch 157/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.9255 - val_loss: 0.3081 - val_accuracy: 0.9007\n",
      "Epoch 158/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2478 - accuracy: 0.9307 - val_loss: 0.2978 - val_accuracy: 0.9144\n",
      "Epoch 159/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2504 - accuracy: 0.9264 - val_loss: 0.3101 - val_accuracy: 0.9110\n",
      "Epoch 160/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2563 - accuracy: 0.9255 - val_loss: 0.3051 - val_accuracy: 0.9075\n",
      "Epoch 161/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9298 - val_loss: 0.3231 - val_accuracy: 0.9007\n",
      "Epoch 162/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.9289 - val_loss: 0.2802 - val_accuracy: 0.9144\n",
      "Epoch 163/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9281 - val_loss: 0.2945 - val_accuracy: 0.9110\n",
      "Epoch 164/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2581 - accuracy: 0.9178 - val_loss: 0.2871 - val_accuracy: 0.9144\n",
      "Epoch 165/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9212 - val_loss: 0.2920 - val_accuracy: 0.9075\n",
      "Epoch 166/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2476 - accuracy: 0.9238 - val_loss: 0.2883 - val_accuracy: 0.9110\n",
      "Epoch 167/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2574 - accuracy: 0.9135 - val_loss: 0.2993 - val_accuracy: 0.8973\n",
      "Epoch 168/300\n",
      "19/37 [==============>...............] - ETA: 0s - loss: 0.2687 - accuracy: 0.9194\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2773 - accuracy: 0.9152 - val_loss: 0.3015 - val_accuracy: 0.9007\n",
      "Epoch 169/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.9255 - val_loss: 0.2885 - val_accuracy: 0.9144\n",
      "Epoch 170/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2467 - accuracy: 0.9204 - val_loss: 0.2964 - val_accuracy: 0.9247\n",
      "Epoch 171/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9307 - val_loss: 0.2954 - val_accuracy: 0.9110\n",
      "Epoch 172/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.9212 - val_loss: 0.3022 - val_accuracy: 0.9144\n",
      "Epoch 173/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9281 - val_loss: 0.2909 - val_accuracy: 0.9075\n",
      "Epoch 174/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.9221 - val_loss: 0.3038 - val_accuracy: 0.9041\n",
      "Epoch 175/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.9264 - val_loss: 0.2946 - val_accuracy: 0.9075\n",
      "Epoch 176/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9289 - val_loss: 0.3102 - val_accuracy: 0.9178\n",
      "Epoch 177/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.9315 - val_loss: 0.2970 - val_accuracy: 0.9178\n",
      "Epoch 178/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.9255 - val_loss: 0.2948 - val_accuracy: 0.9041\n",
      "Epoch 179/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.9315 - val_loss: 0.3104 - val_accuracy: 0.9075\n",
      "Epoch 180/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9255 - val_loss: 0.2864 - val_accuracy: 0.9144\n",
      "Epoch 181/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9315 - val_loss: 0.3163 - val_accuracy: 0.9178\n",
      "Epoch 182/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9264 - val_loss: 0.3103 - val_accuracy: 0.9110\n",
      "Epoch 183/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9298 - val_loss: 0.3373 - val_accuracy: 0.9007\n",
      "Epoch 184/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.9229 - val_loss: 0.2901 - val_accuracy: 0.9041\n",
      "Epoch 185/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2232 - accuracy: 0.9315 - val_loss: 0.3253 - val_accuracy: 0.9110\n",
      "Epoch 186/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.9289 - val_loss: 0.2930 - val_accuracy: 0.9110\n",
      "Epoch 187/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.9255 - val_loss: 0.2943 - val_accuracy: 0.9041\n",
      "Epoch 188/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9341 - val_loss: 0.3078 - val_accuracy: 0.8973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9246575236320496"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = len(trainx.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-6)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx_scaled], y = trainy.values,  \n",
    "                         validation_data = ([textx_scaled], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy\n",
    "#min_losses3[c].append(best_val_loss)\n",
    "#i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
