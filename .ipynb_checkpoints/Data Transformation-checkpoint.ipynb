{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#GaussianRandomProjection(n_components='auto', *, eps=0.1, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AboveMedianPrice</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>14115</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>796</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>10084</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1686</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>10382</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7420</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AboveMedianPrice  LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  \\\n",
       "0                 1     8450            7            5          856         2   \n",
       "1                 1     9600            6            8         1262         2   \n",
       "2                 1    11250            7            5          920         2   \n",
       "3                 0     9550            7            5          756         1   \n",
       "4                 1    14260            8            5         1145         2   \n",
       "5                 0    14115            5            5          796         1   \n",
       "6                 1    10084            8            5         1686         2   \n",
       "7                 1    10382            7            6         1107         2   \n",
       "8                 0     6120            7            5          952         2   \n",
       "9                 0     7420            5            6          991         1   \n",
       "\n",
       "   HalfBath  BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "0         1             3             8           0         548  \n",
       "1         0             3             6           1         460  \n",
       "2         1             3             6           1         608  \n",
       "3         0             3             7           1         642  \n",
       "4         1             4             9           1         836  \n",
       "5         1             1             5           0         480  \n",
       "6         0             3             7           1         636  \n",
       "7         1             3             7           2         484  \n",
       "8         0             2             8           2         468  \n",
       "9         0             2             5           2         205  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"./Datasets/houseprices_ready.csv\"\n",
    "df = pd.read_csv(dataset_dir, index_col=0)\n",
    "df = df.fillna(df.mean())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = len(df.columns)\n",
    "row_num = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = df.iloc[:int(0.8*row_num), 1:(col_num)]\n",
    "trainy = df.iloc[:int(0.8*row_num), 0]\n",
    "\n",
    "testx = df.iloc[int(0.8*row_num):, 1:(col_num)]\n",
    "testy = df.iloc[int(0.8*row_num):, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainx_scaled = pd.DataFrame(scaler.fit_transform(trainx), columns = trainx.columns, index = trainx.index)\n",
    "testx_scaled = pd.DataFrame(scaler.transform(testx), columns = testx.columns, index = testx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>14.391355</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>0.354888</td>\n",
       "      <td>1.012200</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>2.126222</td>\n",
       "      <td>0.249948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>-0.184336</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.427102</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.943654</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>1.638311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>-0.190779</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.014459</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.441608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-0.686115</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>0.485838</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>-0.163701</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>0.499392</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>1.496164</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-1.506656</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>1.215994</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.276255</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>-0.343729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0.126311</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>1.023495</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>0.011542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.243721</td>\n",
       "      <td>-0.082326</td>\n",
       "      <td>1.246305</td>\n",
       "      <td>-1.653495</td>\n",
       "      <td>-1.047785</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.142879</td>\n",
       "      <td>-0.810781</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-0.350016</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>3.188607</td>\n",
       "      <td>1.380199</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>-2.222927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.024350</td>\n",
       "      <td>-1.539237</td>\n",
       "      <td>-0.536529</td>\n",
       "      <td>-2.374136</td>\n",
       "      <td>0.770659</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-1.066042</td>\n",
       "      <td>-0.333699</td>\n",
       "      <td>-0.970321</td>\n",
       "      <td>0.189178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
       "335   14.391355    -0.810781     0.354888     1.012200  0.770659 -0.756619   \n",
       "1074  -0.184336     0.646130    -0.536529     0.427102  0.770659 -0.756619   \n",
       "933   -0.190779     0.646130    -0.536529     1.014459  0.770659 -0.756619   \n",
       "982   -0.686115     0.646130    -0.536529     0.485838  0.770659 -0.756619   \n",
       "921   -0.163701    -0.810781     1.246305     0.499392  0.770659  3.188607   \n",
       "...         ...          ...          ...          ...       ...       ...   \n",
       "1018   0.023696     0.646130    -0.536529    -1.506656  0.770659  1.215994   \n",
       "1016   0.126311     0.646130    -0.536529     1.023495  0.770659 -0.756619   \n",
       "307   -0.243721    -0.082326     1.246305    -1.653495 -1.047785 -0.756619   \n",
       "1450  -0.142879    -0.810781    -0.536529    -0.350016  0.770659  3.188607   \n",
       "17     0.024350    -1.539237    -0.536529    -2.374136  0.770659 -0.756619   \n",
       "\n",
       "      BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  \n",
       "335       0.157079      0.276255    2.126222    0.249948  \n",
       "1074     -1.066042     -0.943654   -0.970321    1.638311  \n",
       "933       0.157079     -0.333699   -0.970321    0.441608  \n",
       "982      -1.066042     -0.333699    0.577951   -0.409173  \n",
       "921       1.380199      1.496164   -0.970321   -2.222927  \n",
       "...            ...           ...         ...         ...  \n",
       "1018      0.157079      0.276255    0.577951   -0.343729  \n",
       "1016      0.157079     -0.333699    0.577951    0.011542  \n",
       "307       0.157079     -0.333699   -0.970321   -2.222927  \n",
       "1450      1.380199      0.886209   -0.970321   -2.222927  \n",
       "17       -1.066042     -0.333699   -0.970321    0.189178  \n",
       "\n",
       "[1168 rows x 10 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GaussianRandomProjection(n_components=9, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.084836</td>\n",
       "      <td>-1.904383</td>\n",
       "      <td>-7.679614</td>\n",
       "      <td>4.631098</td>\n",
       "      <td>-0.114246</td>\n",
       "      <td>0.952415</td>\n",
       "      <td>9.226800</td>\n",
       "      <td>4.946266</td>\n",
       "      <td>3.855080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.044219</td>\n",
       "      <td>-0.938037</td>\n",
       "      <td>-0.639518</td>\n",
       "      <td>1.358543</td>\n",
       "      <td>-0.829323</td>\n",
       "      <td>1.034943</td>\n",
       "      <td>-1.095426</td>\n",
       "      <td>1.511421</td>\n",
       "      <td>0.209625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.251319</td>\n",
       "      <td>-1.175339</td>\n",
       "      <td>-0.362331</td>\n",
       "      <td>1.208742</td>\n",
       "      <td>0.244125</td>\n",
       "      <td>0.614165</td>\n",
       "      <td>-1.533636</td>\n",
       "      <td>0.944224</td>\n",
       "      <td>-0.281345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157709</td>\n",
       "      <td>-0.256910</td>\n",
       "      <td>1.068377</td>\n",
       "      <td>1.006826</td>\n",
       "      <td>-0.543655</td>\n",
       "      <td>-0.200589</td>\n",
       "      <td>-1.948781</td>\n",
       "      <td>0.180073</td>\n",
       "      <td>0.200504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.398890</td>\n",
       "      <td>0.363869</td>\n",
       "      <td>-0.982447</td>\n",
       "      <td>-1.229865</td>\n",
       "      <td>2.163130</td>\n",
       "      <td>0.691356</td>\n",
       "      <td>-1.114974</td>\n",
       "      <td>-0.611162</td>\n",
       "      <td>-0.131855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>0.101440</td>\n",
       "      <td>0.447461</td>\n",
       "      <td>0.425554</td>\n",
       "      <td>-0.463016</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>-0.671078</td>\n",
       "      <td>-0.214063</td>\n",
       "      <td>-0.636796</td>\n",
       "      <td>0.848351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.277018</td>\n",
       "      <td>-0.566664</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>1.033704</td>\n",
       "      <td>-0.115932</td>\n",
       "      <td>-0.340670</td>\n",
       "      <td>-1.060477</td>\n",
       "      <td>0.466924</td>\n",
       "      <td>-0.258491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>1.700543</td>\n",
       "      <td>-1.357993</td>\n",
       "      <td>1.653664</td>\n",
       "      <td>0.236097</td>\n",
       "      <td>0.874045</td>\n",
       "      <td>0.526645</td>\n",
       "      <td>1.113383</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>1.093142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>0.349494</td>\n",
       "      <td>0.528671</td>\n",
       "      <td>-0.279372</td>\n",
       "      <td>-0.597747</td>\n",
       "      <td>2.452261</td>\n",
       "      <td>0.780555</td>\n",
       "      <td>-1.197719</td>\n",
       "      <td>-0.509644</td>\n",
       "      <td>0.331627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>1.342995</td>\n",
       "      <td>-0.277189</td>\n",
       "      <td>2.220760</td>\n",
       "      <td>0.683786</td>\n",
       "      <td>0.404347</td>\n",
       "      <td>1.212728</td>\n",
       "      <td>1.445526</td>\n",
       "      <td>0.417885</td>\n",
       "      <td>-0.844295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -2.084836 -1.904383 -7.679614  4.631098 -0.114246  0.952415  9.226800   \n",
       "1    -0.044219 -0.938037 -0.639518  1.358543 -0.829323  1.034943 -1.095426   \n",
       "2     0.251319 -1.175339 -0.362331  1.208742  0.244125  0.614165 -1.533636   \n",
       "3     0.157709 -0.256910  1.068377  1.006826 -0.543655 -0.200589 -1.948781   \n",
       "4     0.398890  0.363869 -0.982447 -1.229865  2.163130  0.691356 -1.114974   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1163  0.101440  0.447461  0.425554 -0.463016  0.808511 -0.671078 -0.214063   \n",
       "1164 -0.277018 -0.566664  0.209700  1.033704 -0.115932 -0.340670 -1.060477   \n",
       "1165  1.700543 -1.357993  1.653664  0.236097  0.874045  0.526645  1.113383   \n",
       "1166  0.349494  0.528671 -0.279372 -0.597747  2.452261  0.780555 -1.197719   \n",
       "1167  1.342995 -0.277189  2.220760  0.683786  0.404347  1.212728  1.445526   \n",
       "\n",
       "             7         8  \n",
       "0     4.946266  3.855080  \n",
       "1     1.511421  0.209625  \n",
       "2     0.944224 -0.281345  \n",
       "3     0.180073  0.200504  \n",
       "4    -0.611162 -0.131855  \n",
       "...        ...       ...  \n",
       "1163 -0.636796  0.848351  \n",
       "1164  0.466924 -0.258491  \n",
       "1165  0.164551  1.093142  \n",
       "1166 -0.509644  0.331627  \n",
       "1167  0.417885 -0.844295  \n",
       "\n",
       "[1168 rows x 9 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_scaled_projected = pd.DataFrame(transformer.fit_transform(trainx_scaled))\n",
    "trainx_scaled_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.217221</td>\n",
       "      <td>0.109635</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>0.309669</td>\n",
       "      <td>-0.077961</td>\n",
       "      <td>-0.484826</td>\n",
       "      <td>-0.590635</td>\n",
       "      <td>-0.228753</td>\n",
       "      <td>-1.283319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.004990</td>\n",
       "      <td>0.904090</td>\n",
       "      <td>-2.338702</td>\n",
       "      <td>-1.436198</td>\n",
       "      <td>0.669173</td>\n",
       "      <td>-1.551760</td>\n",
       "      <td>-1.038953</td>\n",
       "      <td>-1.275229</td>\n",
       "      <td>-0.402319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.126228</td>\n",
       "      <td>-0.602707</td>\n",
       "      <td>-1.375399</td>\n",
       "      <td>0.479878</td>\n",
       "      <td>-0.072278</td>\n",
       "      <td>0.287839</td>\n",
       "      <td>-1.209882</td>\n",
       "      <td>0.496211</td>\n",
       "      <td>-0.898289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.175336</td>\n",
       "      <td>0.379408</td>\n",
       "      <td>-1.494564</td>\n",
       "      <td>-0.190441</td>\n",
       "      <td>0.355006</td>\n",
       "      <td>0.672971</td>\n",
       "      <td>-0.990754</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>1.160193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098267</td>\n",
       "      <td>-0.387930</td>\n",
       "      <td>-0.630646</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>-0.129846</td>\n",
       "      <td>0.535735</td>\n",
       "      <td>0.530788</td>\n",
       "      <td>0.399183</td>\n",
       "      <td>0.246728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1.420384</td>\n",
       "      <td>-1.444747</td>\n",
       "      <td>0.609356</td>\n",
       "      <td>-0.030920</td>\n",
       "      <td>1.736030</td>\n",
       "      <td>0.059661</td>\n",
       "      <td>0.801848</td>\n",
       "      <td>-0.084166</td>\n",
       "      <td>-1.102195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.353956</td>\n",
       "      <td>0.651689</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>-0.772536</td>\n",
       "      <td>-0.049334</td>\n",
       "      <td>-0.257829</td>\n",
       "      <td>1.063262</td>\n",
       "      <td>-0.335203</td>\n",
       "      <td>1.316166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-0.683256</td>\n",
       "      <td>1.333321</td>\n",
       "      <td>-2.015099</td>\n",
       "      <td>-2.216943</td>\n",
       "      <td>0.914813</td>\n",
       "      <td>-2.002454</td>\n",
       "      <td>-0.834970</td>\n",
       "      <td>-2.121601</td>\n",
       "      <td>-0.727955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.369125</td>\n",
       "      <td>0.043966</td>\n",
       "      <td>0.751574</td>\n",
       "      <td>0.068269</td>\n",
       "      <td>0.866530</td>\n",
       "      <td>1.049457</td>\n",
       "      <td>-0.296576</td>\n",
       "      <td>0.320927</td>\n",
       "      <td>-0.381043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.436852</td>\n",
       "      <td>-0.505498</td>\n",
       "      <td>0.382745</td>\n",
       "      <td>0.730087</td>\n",
       "      <td>-0.814584</td>\n",
       "      <td>1.334724</td>\n",
       "      <td>0.774577</td>\n",
       "      <td>1.005189</td>\n",
       "      <td>0.500094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.217221  0.109635  0.429832  0.309669 -0.077961 -0.484826 -0.590635   \n",
       "1   -1.004990  0.904090 -2.338702 -1.436198  0.669173 -1.551760 -1.038953   \n",
       "2   -0.126228 -0.602707 -1.375399  0.479878 -0.072278  0.287839 -1.209882   \n",
       "3   -0.175336  0.379408 -1.494564 -0.190441  0.355006  0.672971 -0.990754   \n",
       "4    0.098267 -0.387930 -0.630646  0.333914 -0.129846  0.535735  0.530788   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "287  1.420384 -1.444747  0.609356 -0.030920  1.736030  0.059661  0.801848   \n",
       "288 -0.353956  0.651689 -0.005949 -0.772536 -0.049334 -0.257829  1.063262   \n",
       "289 -0.683256  1.333321 -2.015099 -2.216943  0.914813 -2.002454 -0.834970   \n",
       "290  0.369125  0.043966  0.751574  0.068269  0.866530  1.049457 -0.296576   \n",
       "291  0.436852 -0.505498  0.382745  0.730087 -0.814584  1.334724  0.774577   \n",
       "\n",
       "            7         8  \n",
       "0   -0.228753 -1.283319  \n",
       "1   -1.275229 -0.402319  \n",
       "2    0.496211 -0.898289  \n",
       "3   -0.062600  1.160193  \n",
       "4    0.399183  0.246728  \n",
       "..        ...       ...  \n",
       "287 -0.084166 -1.102195  \n",
       "288 -0.335203  1.316166  \n",
       "289 -2.121601 -0.727955  \n",
       "290  0.320927 -0.381043  \n",
       "291  1.005189  0.500094  \n",
       "\n",
       "[292 rows x 9 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx_scaled_projected = pd.DataFrame(transformer.transform(testx_scaled))\n",
    "testx_scaled_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MICS_model(inp_size, drop_out, hidden_num = 4, hidden_size=32):\n",
    "    inputs = keras.layers.Input(shape=(inp_size), name=\"input\")\n",
    "    tf.keras.layers.GaussianNoise(0.05)(inputs)    \n",
    "    h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(inputs)\n",
    "    h = keras.layers.Dropout(drop_out)(h)\n",
    "    for hidden in range(hidden_num):\n",
    "        h = keras.layers.Dense(hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3))(h)\n",
    "        h = keras.layers.Dropout(drop_out)(h) \n",
    "\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(h)    \n",
    "    return keras.Model(inputs=[inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = len(trainx_scaled_projected.columns)\n",
    "inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6550 - accuracy: 0.7723 - val_loss: 0.4094 - val_accuracy: 0.8801\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.8373 - val_loss: 0.3687 - val_accuracy: 0.9075\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.5282 - accuracy: 0.8485 - val_loss: 0.4276 - val_accuracy: 0.8733\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.8656 - val_loss: 0.3529 - val_accuracy: 0.8904\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4478 - accuracy: 0.8707 - val_loss: 0.3470 - val_accuracy: 0.8836\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8707 - val_loss: 0.3077 - val_accuracy: 0.8904\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8673 - val_loss: 0.3188 - val_accuracy: 0.8938\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.8673 - val_loss: 0.3379 - val_accuracy: 0.8767\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3921 - accuracy: 0.8767 - val_loss: 0.3162 - val_accuracy: 0.8904\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8810 - val_loss: 0.3320 - val_accuracy: 0.8767\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8861 - val_loss: 0.3172 - val_accuracy: 0.8938\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8878 - val_loss: 0.3312 - val_accuracy: 0.8801\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8844 - val_loss: 0.3016 - val_accuracy: 0.8870\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8707 - val_loss: 0.3079 - val_accuracy: 0.8938\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8707 - val_loss: 0.3334 - val_accuracy: 0.8836\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8818 - val_loss: 0.3418 - val_accuracy: 0.8904\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.8904 - val_loss: 0.3069 - val_accuracy: 0.8870\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3862 - accuracy: 0.8793 - val_loss: 0.3183 - val_accuracy: 0.9041\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8682 - val_loss: 0.3176 - val_accuracy: 0.8836\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3904 - accuracy: 0.8818 - val_loss: 0.3247 - val_accuracy: 0.8836\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8896 - val_loss: 0.3133 - val_accuracy: 0.8801\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8690 - val_loss: 0.3176 - val_accuracy: 0.8938\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8741 - val_loss: 0.3169 - val_accuracy: 0.8973\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8878 - val_loss: 0.3095 - val_accuracy: 0.8938\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.8896 - val_loss: 0.3034 - val_accuracy: 0.8836\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8836 - val_loss: 0.3186 - val_accuracy: 0.8904\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8801 - val_loss: 0.3174 - val_accuracy: 0.8938\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8904 - val_loss: 0.3141 - val_accuracy: 0.8904\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8836 - val_loss: 0.2998 - val_accuracy: 0.8973\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8921 - val_loss: 0.3258 - val_accuracy: 0.8938\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8896 - val_loss: 0.3083 - val_accuracy: 0.8767\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8964 - val_loss: 0.3331 - val_accuracy: 0.8870\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8947 - val_loss: 0.3289 - val_accuracy: 0.8904\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8853 - val_loss: 0.3398 - val_accuracy: 0.8699\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8870 - val_loss: 0.3170 - val_accuracy: 0.8973\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3426 - accuracy: 0.8955 - val_loss: 0.3164 - val_accuracy: 0.8767\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8904 - val_loss: 0.3152 - val_accuracy: 0.8733\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.8861 - val_loss: 0.3305 - val_accuracy: 0.8870\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8853 - val_loss: 0.3278 - val_accuracy: 0.8836\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8818 - val_loss: 0.3231 - val_accuracy: 0.8801\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8836 - val_loss: 0.3209 - val_accuracy: 0.8699\n",
      "Epoch 42/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8716 - val_loss: 0.3398 - val_accuracy: 0.8836\n",
      "Epoch 43/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3549 - accuracy: 0.8861 - val_loss: 0.3114 - val_accuracy: 0.8938\n",
      "Epoch 44/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3461 - accuracy: 0.8878 - val_loss: 0.3191 - val_accuracy: 0.8973\n",
      "Epoch 45/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8759 - val_loss: 0.3260 - val_accuracy: 0.8836\n",
      "Epoch 46/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8870 - val_loss: 0.3309 - val_accuracy: 0.8904\n",
      "Epoch 47/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3543 - accuracy: 0.8861 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 48/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8818 - val_loss: 0.2967 - val_accuracy: 0.8870\n",
      "Epoch 49/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8716 - val_loss: 0.3409 - val_accuracy: 0.8767\n",
      "Epoch 50/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8853 - val_loss: 0.3018 - val_accuracy: 0.8870\n",
      "Epoch 51/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8759 - val_loss: 0.3052 - val_accuracy: 0.8973\n",
      "Epoch 52/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8938 - val_loss: 0.3020 - val_accuracy: 0.8836\n",
      "Epoch 53/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8844 - val_loss: 0.2877 - val_accuracy: 0.8870\n",
      "Epoch 54/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3902 - accuracy: 0.8861 - val_loss: 0.3504 - val_accuracy: 0.8767\n",
      "Epoch 55/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8853 - val_loss: 0.3257 - val_accuracy: 0.8938\n",
      "Epoch 56/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3781 - accuracy: 0.8861 - val_loss: 0.3368 - val_accuracy: 0.8733\n",
      "Epoch 57/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8759 - val_loss: 0.3250 - val_accuracy: 0.8938\n",
      "Epoch 58/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8861 - val_loss: 0.3203 - val_accuracy: 0.8870\n",
      "Epoch 59/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.8973 - val_loss: 0.3060 - val_accuracy: 0.8836\n",
      "Epoch 60/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.8921 - val_loss: 0.3043 - val_accuracy: 0.8836\n",
      "Epoch 61/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8870 - val_loss: 0.3119 - val_accuracy: 0.8836\n",
      "Epoch 62/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8810 - val_loss: 0.3367 - val_accuracy: 0.8733\n",
      "Epoch 63/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3684 - accuracy: 0.8896 - val_loss: 0.3109 - val_accuracy: 0.8938\n",
      "Epoch 64/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8930 - val_loss: 0.3234 - val_accuracy: 0.8767\n",
      "Epoch 65/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8947 - val_loss: 0.3471 - val_accuracy: 0.8767\n",
      "Epoch 66/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3616 - accuracy: 0.8793 - val_loss: 0.3184 - val_accuracy: 0.9007\n",
      "Epoch 67/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8904 - val_loss: 0.3360 - val_accuracy: 0.8836\n",
      "Epoch 68/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8878 - val_loss: 0.3094 - val_accuracy: 0.8904\n",
      "Epoch 69/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8913 - val_loss: 0.3430 - val_accuracy: 0.8904\n",
      "Epoch 70/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3534 - accuracy: 0.8776 - val_loss: 0.3314 - val_accuracy: 0.8836\n",
      "Epoch 71/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3475 - accuracy: 0.8896 - val_loss: 0.3216 - val_accuracy: 0.8904\n",
      "Epoch 72/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.8887 - val_loss: 0.3275 - val_accuracy: 0.8630\n",
      "Epoch 73/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8767 - val_loss: 0.2929 - val_accuracy: 0.8801\n",
      "Epoch 74/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8827 - val_loss: 0.3070 - val_accuracy: 0.8904\n",
      "Epoch 75/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3455 - accuracy: 0.8896 - val_loss: 0.3086 - val_accuracy: 0.8938\n",
      "Epoch 76/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8810 - val_loss: 0.3157 - val_accuracy: 0.8904\n",
      "Epoch 77/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8836 - val_loss: 0.3475 - val_accuracy: 0.8801\n",
      "Epoch 78/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8853 - val_loss: 0.2995 - val_accuracy: 0.8836\n",
      "Epoch 79/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3640 - accuracy: 0.8776 - val_loss: 0.3184 - val_accuracy: 0.8870\n",
      "Epoch 80/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3367 - accuracy: 0.8947 - val_loss: 0.3073 - val_accuracy: 0.8801\n",
      "Epoch 81/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8887 - val_loss: 0.3395 - val_accuracy: 0.8836\n",
      "Epoch 82/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8904 - val_loss: 0.3305 - val_accuracy: 0.8699\n",
      "Epoch 83/300\n",
      "23/37 [=================>............] - ETA: 0s - loss: 0.3305 - accuracy: 0.8981\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8844 - val_loss: 0.3077 - val_accuracy: 0.8767\n",
      "Epoch 84/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8904 - val_loss: 0.3170 - val_accuracy: 0.8904\n",
      "Epoch 85/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8827 - val_loss: 0.3050 - val_accuracy: 0.8904\n",
      "Epoch 86/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8870 - val_loss: 0.3125 - val_accuracy: 0.8767\n",
      "Epoch 87/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8913 - val_loss: 0.3191 - val_accuracy: 0.8733\n",
      "Epoch 88/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3441 - accuracy: 0.8853 - val_loss: 0.3287 - val_accuracy: 0.8870\n",
      "Epoch 89/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8930 - val_loss: 0.3066 - val_accuracy: 0.8973\n",
      "Epoch 90/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8887 - val_loss: 0.3129 - val_accuracy: 0.8801\n",
      "Epoch 91/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8964 - val_loss: 0.2951 - val_accuracy: 0.8904\n",
      "Epoch 92/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8947 - val_loss: 0.3131 - val_accuracy: 0.8836\n",
      "Epoch 93/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8861 - val_loss: 0.3140 - val_accuracy: 0.8733\n",
      "Epoch 94/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3383 - accuracy: 0.8921 - val_loss: 0.3220 - val_accuracy: 0.8904\n",
      "Epoch 95/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3267 - accuracy: 0.8955 - val_loss: 0.3035 - val_accuracy: 0.8938\n",
      "Epoch 96/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3372 - accuracy: 0.8887 - val_loss: 0.3101 - val_accuracy: 0.8904\n",
      "Epoch 97/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8844 - val_loss: 0.3098 - val_accuracy: 0.8836\n",
      "Epoch 98/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8964 - val_loss: 0.3052 - val_accuracy: 0.8938\n",
      "Epoch 99/300\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8844 - val_loss: 0.3202 - val_accuracy: 0.8938\n",
      "Epoch 100/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.8861 - val_loss: 0.3080 - val_accuracy: 0.8836\n",
      "Epoch 101/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8955 - val_loss: 0.3257 - val_accuracy: 0.8904\n",
      "Epoch 102/300\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.9067 - val_loss: 0.3140 - val_accuracy: 0.8938\n",
      "Epoch 103/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8955 - val_loss: 0.3060 - val_accuracy: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9075342416763306"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "inp_size = len(trainx_scaled_projected.columns)\n",
    "MICS_model = get_MICS_model(inp_size, drop_out = 0.25)\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50), \n",
    "        keras.callbacks.ReduceLROnPlateau(\"val_loss\", factor = 0.8, patience=30,\n",
    "                                         verbose = 2, mode = \"auto\", \n",
    "                                          min_lr = 1e-6)]\n",
    "MICS_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "history = MICS_model.fit(x = [trainx_scaled_projected], y = trainy.values,  \n",
    "                         validation_data = ([testx_scaled_projected], testy.values),\n",
    "                         epochs=300, batch_size = 32, callbacks=callback)\n",
    "training_val_accuracy = history.history[\"val_accuracy\"]\n",
    "best_row_index = np.argmax(training_val_accuracy)\n",
    "best_val_accuracy = training_val_accuracy[best_row_index]\n",
    "best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
